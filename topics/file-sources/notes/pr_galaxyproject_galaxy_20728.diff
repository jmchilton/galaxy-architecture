diff --git a/Makefile b/Makefile
index 89a9d0cdc37a..60f7fdf00af1 100644
--- a/Makefile
+++ b/Makefile
@@ -262,6 +262,12 @@ client-test-watch: client ## Watch and run all client unit tests on changes
 serve-selenium-notebooks: ## Serve testing notebooks for Jupyter
 	cd lib && export PYTHONPATH=`pwd`; jupyter notebook --notebook-dir=galaxy_test/selenium/jupyter
 
+files-sources-lint: ## Validate file sources configuration
+	$(IN_VENV) cd lib && PYTHONPATH=`pwd` python galaxy/files/validate/script.py
+
+files-sources-lint-verbose: ## Validate file sources configuration (verbose)
+	$(IN_VENV) cd lib && PYTHONPATH=`pwd` python galaxy/files/validate/script.py --verbose
+
 # Release Targets
 release-create-rc: ## Create a release-candidate branch or new release-candidate version
 	$(IN_VENV) ./scripts/release.sh -c
diff --git a/client/src/api/remoteFiles.ts b/client/src/api/remoteFiles.ts
index 5b014a75199f..1d8dc6eddb20 100644
--- a/client/src/api/remoteFiles.ts
+++ b/client/src/api/remoteFiles.ts
@@ -61,7 +61,7 @@ export interface BrowseRemoteFilesResult {
  * Get the list of files and directories from the server for the given file source URI.
  * @param uri The file source URI to browse.
  * @param isRecursive Whether to recursively retrieve all files inside subdirectories.
- * @param writeable Whether to return only entries that can be written to.
+ * @param isWriteIntent Whether to return only entries that can be written to.
  * @param limit The maximum number of entries to return.
  * @param offset The number of entries to skip before returning the rest.
  * @param query The query string to filter the entries.
@@ -71,7 +71,7 @@ export interface BrowseRemoteFilesResult {
 export async function browseRemoteFiles(
     uri: string,
     isRecursive = false,
-    writeable = false,
+    isWriteIntent = false,
     limit?: number,
     offset?: number,
     query?: string,
@@ -83,7 +83,7 @@ export async function browseRemoteFiles(
                 format: "uri",
                 target: uri,
                 recursive: isRecursive,
-                writeable,
+                write_intent: isWriteIntent,
                 limit,
                 offset,
                 query,
diff --git a/client/src/api/schema/schema.ts b/client/src/api/schema/schema.ts
index d75db5e81cfd..86a20af924a1 100644
--- a/client/src/api/schema/schema.ts
+++ b/client/src/api/schema/schema.ts
@@ -26124,8 +26124,13 @@ export interface operations {
                 recursive?: boolean | null;
                 /** @description (This only applies when `format` is `jstree`) The value can be either `folders` or `files` and it will disable the corresponding nodes of the tree. */
                 disable?: components["schemas"]["RemoteFilesDisableMode"] | null;
-                /** @description Whether the query is made with the intention of writing to the source. If set to True, only entries that can be written to will be returned. */
+                /**
+                 * @deprecated
+                 * @description Deprecated, please use `write_intent` instead.
+                 */
                 writeable?: boolean | null;
+                /** @description Whether the query is made with the intention of writing to the source. If set to True, only entries that can be written to will be returned. */
+                write_intent?: boolean | null;
                 /** @description Maximum number of entries to return. */
                 limit?: number | null;
                 /** @description Number of entries to skip. */
@@ -35754,8 +35759,13 @@ export interface operations {
                 recursive?: boolean | null;
                 /** @description (This only applies when `format` is `jstree`) The value can be either `folders` or `files` and it will disable the corresponding nodes of the tree. */
                 disable?: components["schemas"]["RemoteFilesDisableMode"] | null;
-                /** @description Whether the query is made with the intention of writing to the source. If set to True, only entries that can be written to will be returned. */
+                /**
+                 * @deprecated
+                 * @description Deprecated, please use `write_intent` instead.
+                 */
                 writeable?: boolean | null;
+                /** @description Whether the query is made with the intention of writing to the source. If set to True, only entries that can be written to will be returned. */
+                write_intent?: boolean | null;
                 /** @description Maximum number of entries to return. */
                 limit?: number | null;
                 /** @description Number of entries to skip. */
diff --git a/client/src/components/FilesDialog/FilesDialog.test.ts b/client/src/components/FilesDialog/FilesDialog.test.ts
index d600ef868f20..f5eb7d3bec8a 100644
--- a/client/src/components/FilesDialog/FilesDialog.test.ts
+++ b/client/src/components/FilesDialog/FilesDialog.test.ts
@@ -61,8 +61,12 @@ interface RowElement extends SelectionItem, Element {
     _rowVariant: SelectionState;
 }
 
-function paramsToKey(query: { target?: string | null; recursive?: string | null; writeable?: string | null }): string {
-    return `${query.target}?recursive=${query.recursive}&writeable=${query.writeable ?? "false"}`;
+function paramsToKey(query: {
+    target?: string | null;
+    recursive?: string | null;
+    write_intent?: string | null;
+}): string {
+    return `${query.target}?recursive=${query.recursive}&write_intent=${query.write_intent ?? "false"}`;
 }
 
 const mockedOkApiRoutesMap = new Map<string, RemoteFilesList>([
@@ -93,7 +97,7 @@ const initComponent = async (props: { multiple: boolean; mode?: string }, hasTem
             const responseKey = paramsToKey({
                 target: query.get("target"),
                 recursive: query.get("recursive"),
-                writeable: query.get("writeable"),
+                write_intent: query.get("write_intent"),
             });
             if (mockedErrorApiRoutesMap.has(responseKey)) {
                 return response("4XX").json({ err_msg: someErrorText, err_code: 400 }, { status: 400 });
diff --git a/lib/galaxy/app.py b/lib/galaxy/app.py
index 673cefd99478..db1a62abdadb 100644
--- a/lib/galaxy/app.py
+++ b/lib/galaxy/app.py
@@ -39,10 +39,8 @@
     ConfiguredFileSourcesConf,
     UserDefinedFileSources,
 )
-from galaxy.files.plugins import (
-    FileSourcePluginLoader,
-    FileSourcePluginsConfig,
-)
+from galaxy.files.models import FileSourcePluginsConfig
+from galaxy.files.plugins import FileSourcePluginLoader
 from galaxy.files.templates import ConfiguredFileSourceTemplates
 from galaxy.job_metrics import JobMetrics
 from galaxy.jobs.manager import JobManager
diff --git a/lib/galaxy/files/__init__.py b/lib/galaxy/files/__init__.py
index 3d7674a91758..bd41e42f9ae1 100644
--- a/lib/galaxy/files/__init__.py
+++ b/lib/galaxy/files/__init__.py
@@ -12,7 +12,6 @@
 from galaxy import exceptions
 from galaxy.files.sources import (
     BaseFilesSource,
-    FilesSourceProperties,
     PluginKind,
 )
 from galaxy.util.dictifiable import Dictifiable
@@ -45,7 +44,7 @@ class NoMatchingFileSource(Exception):
 
 
 class UserDefinedFileSources(Protocol):
-    """Entry-point for Galaxy to inject user-defined object stores.
+    """Entry-point for Galaxy to inject user-defined file sources.
 
     Supplied object of this class is used to write out concrete
     description of file sources when serializing all file sources
@@ -65,13 +64,14 @@ def user_file_sources_to_dicts(
         browsable_only: Optional[bool] = False,
         include_kind: Optional[set[PluginKind]] = None,
         exclude_kind: Optional[set[PluginKind]] = None,
-    ) -> list[FilesSourceProperties]:
+    ) -> list[dict[str, Any]]:
         """Write out user file sources as list of config dictionaries."""
         # config_dicts: List[FilesSourceProperties] = []
         # for file_source in self.user_file_sources():
         #     as_dict = file_source.to_dict(for_serialization=for_serialization, user_context=user_context)
         #     config_dicts.append(as_dict)
         # return config_dicts
+        return []
 
 
 class NullUserDefinedFileSources(UserDefinedFileSources):
@@ -89,7 +89,7 @@ def user_file_sources_to_dicts(
         browsable_only: Optional[bool] = False,
         include_kind: Optional[set[PluginKind]] = None,
         exclude_kind: Optional[set[PluginKind]] = None,
-    ) -> list[FilesSourceProperties]:
+    ) -> list[dict[str, Any]]:
         return []
 
 
@@ -156,7 +156,7 @@ def _ensure_loaded(plugin_type):
                 for file_source in file_sources:
                     if file_source.plugin_type == plugin_type:
                         return
-                stock_file_source_conf_dict.append({"type": plugin_type})
+                stock_file_source_conf_dict.append({"type": plugin_type, "id": f"stock_{plugin_type}"})
 
             _ensure_loaded("http")
             _ensure_loaded("base64")
@@ -257,8 +257,8 @@ def plugins_to_dict(
         browsable_only: Optional[bool] = False,
         include_kind: Optional[set[PluginKind]] = None,
         exclude_kind: Optional[set[PluginKind]] = None,
-    ) -> list[FilesSourceProperties]:
-        rval: list[FilesSourceProperties] = []
+    ) -> list[dict[str, Any]]:
+        rval: list[dict[str, Any]] = []
         for file_source in self._file_sources:
             if not file_source.user_has_access(user_context):
                 continue
diff --git a/lib/galaxy/files/models.py b/lib/galaxy/files/models.py
new file mode 100644
index 000000000000..ce1f7d72d5c4
--- /dev/null
+++ b/lib/galaxy/files/models.py
@@ -0,0 +1,452 @@
+"""Template-aware configuration models for file sources."""
+
+from typing import (
+    Annotated,
+    Any,
+    Generic,
+    Literal,
+    Optional,
+    TYPE_CHECKING,
+    TypeVar,
+    Union,
+)
+
+from pydantic import (
+    BaseModel,
+    ConfigDict,
+    Field,
+)
+
+from galaxy.files.sources._defaults import (
+    DEFAULT_SCHEME,
+    DEFAULT_WRITABLE,
+)
+from galaxy.util.config_parsers import (
+    IpAllowedListEntryT,
+    parse_allowlist_ips,
+)
+from galaxy.util.config_templates import (
+    EnvironmentDict,
+    partial_model,
+)
+from galaxy.util.template import fill_template
+
+if TYPE_CHECKING:
+    from galaxy.files import OptionalUserContext
+
+
+class StrictModel(BaseModel):
+    model_config = ConfigDict(extra="forbid")
+
+
+class FlexibleModel(BaseModel):
+    model_config = ConfigDict(extra="allow")
+
+
+class FileSourcePluginsConfig(BaseModel):
+    symlink_allowlist: list[str] = []
+    fetch_url_allowlist: list[IpAllowedListEntryT] = []
+    library_import_dir: Optional[str] = None
+    user_library_import_dir: Optional[str] = None
+    ftp_upload_dir: Optional[str] = None
+    ftp_upload_purge: bool = True
+    tmp_dir: Optional[str] = None
+    webdav_use_temp_files: Optional[bool] = None
+    listings_expiry_time: Optional[int] = None
+
+    @staticmethod
+    def from_app_config(config):
+        # Formalize what we read in from config to create a more clear interface
+        # for this component.
+        kwds = {}
+        kwds["symlink_allowlist"] = config.user_library_import_symlink_allowlist
+        kwds["fetch_url_allowlist"] = config.fetch_url_allowlist_ips
+        kwds["library_import_dir"] = config.library_import_dir
+        kwds["user_library_import_dir"] = config.user_library_import_dir
+        kwds["ftp_upload_dir"] = config.ftp_upload_dir
+        kwds["ftp_upload_purge"] = config.ftp_upload_purge
+        kwds["tmp_dir"] = config.file_source_temp_dir
+        kwds["webdav_use_temp_files"] = config.file_source_webdav_use_temp_files
+        kwds["listings_expiry_time"] = config.file_source_listings_expiry_time
+
+        return FileSourcePluginsConfig(**kwds)
+
+    def to_dict(self):
+        return {
+            "symlink_allowlist": self.symlink_allowlist,
+            "fetch_url_allowlist": [str(ip) for ip in self.fetch_url_allowlist],
+            "library_import_dir": self.library_import_dir,
+            "user_library_import_dir": self.user_library_import_dir,
+            "ftp_upload_dir": self.ftp_upload_dir,
+            "ftp_upload_purge": self.ftp_upload_purge,
+            "tmp_dir": self.tmp_dir,
+            "webdav_use_temp_files": self.webdav_use_temp_files,
+            "listings_expiry_time": self.listings_expiry_time,
+        }
+
+    @staticmethod
+    def from_dict(as_dict):
+        return FileSourcePluginsConfig(
+            symlink_allowlist=as_dict["symlink_allowlist"],
+            fetch_url_allowlist=parse_allowlist_ips(as_dict["fetch_url_allowlist"]),
+            library_import_dir=as_dict["library_import_dir"],
+            user_library_import_dir=as_dict["user_library_import_dir"],
+            ftp_upload_dir=as_dict["ftp_upload_dir"],
+            ftp_upload_purge=as_dict["ftp_upload_purge"],
+            # Always provided for new jobs, remove in 25.0
+            tmp_dir=as_dict.get("tmp_dir"),
+            webdav_use_temp_files=as_dict.get("webdav_use_temp_files"),
+            listings_expiry_time=as_dict.get("listings_expiry_time"),
+        )
+
+
+class UserData:
+    """User data exposed to file sources."""
+
+    def __init__(self, context: "OptionalUserContext" = None):
+        self.context = context
+
+    @property
+    def email(self) -> Optional[str]:
+        return self.context.email if self.context else None
+
+    @property
+    def username(self) -> Optional[str]:
+        return self.context.username if self.context else None
+
+    @property
+    def is_admin(self) -> bool:
+        return self.context.is_admin if self.context else False
+
+    @property
+    def is_anonymous(self) -> bool:
+        return self.context.anonymous if self.context else True
+
+
+class FileSourceSupports(StrictModel):
+    """Feature support flags for a file source plugin"""
+
+    pagination: Annotated[bool, Field(description="Whether this file source supports server-side pagination.")] = False
+    search: Annotated[bool, Field(description="Whether this file source supports server-side search.")] = False
+    sorting: Annotated[bool, Field(description="Whether this file source supports server-side sorting.")] = False
+
+
+class FilesSourceProperties(StrictModel):
+    """Initial set of properties used to initialize a file source.
+
+    File sources can extend this model to define any additional
+    filesource specific properties.
+    """
+
+    id: Annotated[
+        str,
+        Field(
+            ...,
+            description="The `FilesSource` plugin identifier",
+        ),
+    ]
+    type: Annotated[
+        str,
+        Field(
+            ...,
+            description="The type of the plugin.",
+        ),
+    ]
+    label: Annotated[
+        Optional[str],
+        Field(
+            ...,
+            description="The display label for this plugin.",
+        ),
+    ] = None
+    doc: Annotated[
+        Optional[str],
+        Field(
+            None,
+            title="Documentation",
+            description="Documentation or extended description for this plugin.",
+        ),
+    ] = None
+    browsable: Annotated[
+        bool,
+        Field(
+            ...,
+            title="Browsable",
+            description="Whether this file source plugin can list items.",
+        ),
+    ] = False
+    writable: Annotated[
+        bool,
+        Field(
+            ...,
+            title="Writeable",
+            description="Whether this files source plugin allows write access.",
+        ),
+    ] = DEFAULT_WRITABLE
+    requires_roles: Annotated[
+        Optional[str],
+        Field(
+            None,
+            title="Requires roles",
+            description=(
+                "Only users with the roles specified here can access this source."
+                " This is a boolean expression that can be evaluated by the server."
+                " It can be a simple role name or a complex expression."
+                " For example, 'role1 and (role2 or role3)' will allow access if the user has role1 and either role2 or role3."
+            ),
+        ),
+    ] = None
+    requires_groups: Annotated[
+        Optional[str],
+        Field(
+            None,
+            title="Requires groups",
+            description=(
+                "Only users belonging to the groups specified here can access this source."
+                " This is a boolean expression that can be evaluated by the server."
+                " It can be a simple group name or a complex expression."
+                " For example, 'group1 and (group2 or group3)' will allow access if the user belongs to group1 and either group2 or group3."
+            ),
+        ),
+    ] = None
+    disable_templating: Annotated[
+        Optional[bool],
+        Field(
+            False,
+            title="Disable Templating",
+            description=(
+                "Whether to disable templating for this file source. If set to True, "
+                "the file source will not support templating in paths or other properties."
+            ),
+        ),
+    ] = False
+    scheme: Annotated[
+        Optional[str],
+        Field(
+            DEFAULT_SCHEME,
+            title="Scheme",
+            description="The URI scheme used by this file source plugin.",
+        ),
+    ] = DEFAULT_SCHEME
+    uri_root: Annotated[
+        Optional[str],
+        Field(
+            None,
+            title="URI root",
+            description=(
+                "The URI root used by this type of plugin. This is used to identify the file source and "
+                "should be unique across all file sources."
+            ),
+        ),
+    ] = None
+    url: Annotated[
+        Optional[str],
+        Field(
+            None,
+            title="URL",
+            description="Optional URL that might be provided by some plugins to link to the remote source.",
+        ),
+    ] = None
+    supports: Annotated[
+        FileSourceSupports,
+        Field(
+            default_factory=FileSourceSupports,
+            description="Features supported by this file source.",
+        ),
+    ] = FileSourceSupports()
+    file_sources_config: Annotated[
+        FileSourcePluginsConfig,
+        Field(
+            ...,
+            description="Configuration for the file sources, used to validate and initialize the file source.",
+        ),
+    ]
+
+
+@partial_model()
+class PartialFilesSourceProperties(FilesSourceProperties):
+    """Partial model for FilesSourceProperties to allow partial updates."""
+
+    # We allow extra properties to be set in the model because each file source may have its own specific properties.
+    model_config = ConfigDict(extra="allow")
+
+
+class FilesSourceOptions(StrictModel):
+    """Options to control behavior of file source operations, such as realize_to, write_from and list."""
+
+    write_intent: Annotated[
+        bool,
+        Field(
+            False,
+            description=(
+                "Whether the query is made with the intention of writing to the source."
+                " If set to True, only entries (directories) that can be written to will be returned."
+                " This is used to filter out read-only locations within the file source when listing entries."
+            ),
+        ),
+    ] = False
+
+    # Property overrides for values initially configured through the constructor. For example
+    # the HTTPFilesSource passes in additional http_headers through these properties, which
+    # are merged with constructor defined http_headers. The interpretation of these properties
+    # are filesystem specific.
+    extra_props: Annotated[
+        Optional[PartialFilesSourceProperties],
+        Field(
+            description="Additional properties to override the initial properties defined in the constructor.",
+        ),
+    ] = None
+
+
+class EntryData(FlexibleModel):
+    """Provides data to create a new entry in a file source."""
+
+    name: str
+    # May contain additional properties depending on the file source
+
+
+class Entry(FlexibleModel):
+    """Represents the result of creating a new entry in a file source."""
+
+    name: str
+    uri: str
+    # May contain additional properties depending on the file source
+    external_link: Optional[str]
+
+
+class RemoteEntry(StrictModel):
+    """Represents a remote entry in a file source, either a directory or a file."""
+
+    name: str
+    uri: str
+    path: str
+
+
+class RemoteDirectory(RemoteEntry):
+    class_: Annotated[Literal["Directory"], Field(..., serialization_alias="class")] = "Directory"
+
+
+class RemoteFile(RemoteEntry):
+    class_: Annotated[Literal["File"], Field(..., serialization_alias="class")] = "File"
+    size: Annotated[int, Field(..., title="Size", description="The size of the file in bytes.")] = 0
+    ctime: Annotated[
+        Optional[str], Field(default="Unknown", title="Creation time", description="The creation time of the file.")
+    ]
+
+
+AnyRemoteEntry = Union[RemoteDirectory, RemoteFile]
+
+
+# Fields to skip during template expansion
+COMMON_FILE_SOURCE_PROP_NAMES = set(FilesSourceProperties.model_fields.keys())
+
+
+class BaseFileSourceTemplateConfiguration(FilesSourceProperties):
+    """Base class for template-aware file source configurations.
+
+    Subclasses should override fields that support templating to use
+    Union[ActualType, TemplateExpansion] for those fields.
+    """
+
+
+class BaseFileSourceConfiguration(FilesSourceProperties):
+    """Base class for resolved file source configurations.
+
+    This contains the actual resolved values after template evaluation.
+    """
+
+
+class FilesSourceTemplateContext:
+    """Context for filling templates in file source configurations.
+
+    This is used to provide additional context to file sources during operations.
+    It can include user data, environment variables, and other relevant information.
+    """
+
+    def __init__(
+        self,
+        user_data: Optional[UserData] = None,
+        environment: Optional[EnvironmentDict] = None,
+        file_sources_config: Optional[FileSourcePluginsConfig] = None,
+    ):
+        self.user_data = user_data or UserData()
+        self.environment = environment or {}
+        self.file_sources_config = file_sources_config or FileSourcePluginsConfig()
+
+    def to_dict(self) -> dict[str, Any]:
+        return {
+            "user": self.user_data.context if self.user_data.context else None,
+            "environ": self.environment,
+            "config": self.file_sources_config.to_dict(),
+        }
+
+
+TTemplateConfig = TypeVar("TTemplateConfig", bound=BaseFileSourceTemplateConfiguration)
+TResolvedConfig = TypeVar("TResolvedConfig", bound=BaseFileSourceConfiguration)
+
+
+def resolve_file_source_template(
+    template_config: BaseFileSourceTemplateConfiguration,
+    resolved_config_class: type[TResolvedConfig],
+    context: FilesSourceTemplateContext,
+) -> TResolvedConfig:
+    """Resolve templated file source configuration to actual values.
+
+    Args:
+        template_config: Configuration that may contain templated values
+        resolved_config_class: The target configuration class to instantiate
+        file_sources_config: File sources configuration
+        environment: Environment variables for template resolution
+        user_data: Current user context for template variable resolution
+
+    Returns:
+        Resolved configuration with all templates evaluated
+    """
+    template_variables = context.to_dict()
+
+    def expand_template_value(value: Any) -> Any:
+        """Recursively expand templated values."""
+        if isinstance(value, str) and "$" in value:
+            template_context = {
+                "user": template_variables.get("user"),
+                "environ": template_variables.get("environ", {}),
+                "config": template_variables.get("config"),
+            }
+            return fill_template(value, context=template_context, futurized=True)
+        elif isinstance(value, dict):
+            return {k: expand_template_value(v) for k, v in value.items()}
+        elif isinstance(value, list):
+            return [expand_template_value(item) for item in value]
+        else:
+            return value
+
+    # Convert to dict, expand templates, then convert back to the specific resolved config class
+    config_dict = template_config.model_dump(
+        exclude_unset=True, exclude_none=True, exclude=COMMON_FILE_SOURCE_PROP_NAMES
+    )
+    expanded_dict = {k: expand_template_value(v) for k, v in config_dict.items()}
+
+    # Add back the skipped fields that are still needed in the resolved config
+    for skip_field in COMMON_FILE_SOURCE_PROP_NAMES:
+        if hasattr(template_config, skip_field):
+            expanded_dict[skip_field] = getattr(template_config, skip_field)
+
+    return resolved_config_class(**expanded_dict)
+
+
+class FilesSourceRuntimeContext(Generic[TResolvedConfig]):
+    """Context for file source operations, providing user data and resolved configuration."""
+
+    def __init__(self, user_data: UserData, config: TResolvedConfig):
+        self._user_data = user_data
+        self._config = config
+
+    @property
+    def user_data(self) -> UserData:
+        """User data for the current context."""
+        return self._user_data
+
+    @property
+    def config(self) -> TResolvedConfig:
+        """Resolved configuration for the file source with all templates expanded."""
+        return self._config
diff --git a/lib/galaxy/files/plugins.py b/lib/galaxy/files/plugins.py
index 002a3af5e97a..ac2b4db52473 100644
--- a/lib/galaxy/files/plugins.py
+++ b/lib/galaxy/files/plugins.py
@@ -1,10 +1,9 @@
 from typing import (
     cast,
-    Optional,
     TYPE_CHECKING,
 )
 
-from galaxy.util.config_parsers import parse_allowlist_ips
+from galaxy.files.models import FileSourcePluginsConfig
 from galaxy.util.plugin_config import (
     load_plugins,
     PluginConfigSource,
@@ -15,87 +14,6 @@
     from galaxy.files.sources import BaseFilesSource
 
 
-class FileSourcePluginsConfig:
-    symlink_allowlist: list[str]
-    fetch_url_allowlist: list[str]
-    library_import_dir: Optional[str]
-    user_library_import_dir: Optional[str]
-    ftp_upload_dir: Optional[str]
-    ftp_upload_purge: bool
-    tmp_dir: Optional[str]
-    webdav_use_temp_files: Optional[bool]
-    listings_expiry_time: Optional[int]
-
-    def __init__(
-        self,
-        symlink_allowlist=None,
-        fetch_url_allowlist=None,
-        library_import_dir=None,
-        user_library_import_dir=None,
-        ftp_upload_dir=None,
-        ftp_upload_purge=True,
-        tmp_dir=None,
-        webdav_use_temp_files=None,
-        listings_expiry_time=None,
-    ):
-        symlink_allowlist = symlink_allowlist or []
-        fetch_url_allowlist = fetch_url_allowlist or []
-        self.symlink_allowlist = symlink_allowlist
-        self.fetch_url_allowlist = fetch_url_allowlist
-        self.library_import_dir = library_import_dir
-        self.user_library_import_dir = user_library_import_dir
-        self.ftp_upload_dir = ftp_upload_dir
-        self.ftp_upload_purge = ftp_upload_purge
-        self.tmp_dir = tmp_dir
-        self.webdav_use_temp_files = webdav_use_temp_files
-        self.listings_expiry_time = listings_expiry_time
-
-    @staticmethod
-    def from_app_config(config):
-        # Formalize what we read in from config to create a more clear interface
-        # for this component.
-        kwds = {}
-        kwds["symlink_allowlist"] = config.user_library_import_symlink_allowlist
-        kwds["fetch_url_allowlist"] = [str(ip) for ip in config.fetch_url_allowlist_ips]
-        kwds["library_import_dir"] = config.library_import_dir
-        kwds["user_library_import_dir"] = config.user_library_import_dir
-        kwds["ftp_upload_dir"] = config.ftp_upload_dir
-        kwds["ftp_upload_purge"] = config.ftp_upload_purge
-        kwds["tmp_dir"] = config.file_source_temp_dir
-        kwds["webdav_use_temp_files"] = config.file_source_webdav_use_temp_files
-        kwds["listings_expiry_time"] = config.file_source_listings_expiry_time
-
-        return FileSourcePluginsConfig(**kwds)
-
-    def to_dict(self):
-        return {
-            "symlink_allowlist": self.symlink_allowlist,
-            "fetch_url_allowlist": self.fetch_url_allowlist,
-            "library_import_dir": self.library_import_dir,
-            "user_library_import_dir": self.user_library_import_dir,
-            "ftp_upload_dir": self.ftp_upload_dir,
-            "ftp_upload_purge": self.ftp_upload_purge,
-            "tmp_dir": self.tmp_dir,
-            "webdav_use_temp_files": self.webdav_use_temp_files,
-            "listings_expiry_time": self.listings_expiry_time,
-        }
-
-    @staticmethod
-    def from_dict(as_dict):
-        return FileSourcePluginsConfig(
-            symlink_allowlist=as_dict["symlink_allowlist"],
-            fetch_url_allowlist=parse_allowlist_ips(as_dict["fetch_url_allowlist"]),
-            library_import_dir=as_dict["library_import_dir"],
-            user_library_import_dir=as_dict["user_library_import_dir"],
-            ftp_upload_dir=as_dict["ftp_upload_dir"],
-            ftp_upload_purge=as_dict["ftp_upload_purge"],
-            # Always provided for new jobs, remove in 25.0
-            tmp_dir=as_dict.get("tmp_dir"),
-            webdav_use_temp_files=as_dict.get("webdav_use_temp_files"),
-            listings_expiry_time=as_dict.get("listings_expiry_time"),
-        )
-
-
 class FileSourcePluginLoader:
 
     def __init__(self):
diff --git a/lib/galaxy/files/sources/__init__.py b/lib/galaxy/files/sources/__init__.py
index ad643c5148af..b72e1dc5161c 100644
--- a/lib/galaxy/files/sources/__init__.py
+++ b/lib/galaxy/files/sources/__init__.py
@@ -2,23 +2,13 @@
 import builtins
 import os
 import time
-from dataclasses import (
-    dataclass,
-    field,
-)
 from enum import Enum
 from typing import (
     Any,
     ClassVar,
+    Generic,
     Optional,
     TYPE_CHECKING,
-    Union,
-)
-
-from typing_extensions import (
-    Literal,
-    NotRequired,
-    TypedDict,
 )
 
 from galaxy.exceptions import (
@@ -26,15 +16,27 @@
     ItemAccessibilityException,
     RequestParameterInvalidException,
 )
-from galaxy.files.plugins import FileSourcePluginsConfig
+from galaxy.files.models import (
+    AnyRemoteEntry,
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    COMMON_FILE_SOURCE_PROP_NAMES,
+    Entry,
+    EntryData,
+    FilesSourceOptions,
+    FilesSourceProperties,
+    FilesSourceRuntimeContext,
+    FilesSourceTemplateContext,
+    resolve_file_source_template,
+    TResolvedConfig,
+    TTemplateConfig,
+    UserData,
+)
 from galaxy.util.bool_expressions import (
     BooleanExpressionEvaluator,
     TokenContainedEvaluator,
 )
-from galaxy.util.template import fill_template
 
-DEFAULT_SCHEME = "gxfiles"
-DEFAULT_WRITABLE = False
 DEFAULT_PAGE_LIMIT = 25
 
 if TYPE_CHECKING:
@@ -78,108 +80,19 @@ class PluginKind(str, Enum):
     """
 
 
-class FileSourceSupports(TypedDict):
-    """Feature support flags for a file source plugin"""
-
-    # Indicates whether the file source supports pagination for listing files
-    pagination: NotRequired[bool]
-    # Indicates whether the file source supports server-side search for listing files
-    search: NotRequired[bool]
-    # Indicates whether the file source supports server-side sorting for listing files
-    sorting: NotRequired[bool]
-
-
-class FilesSourceProperties(TypedDict):
-    """Initial set of properties used to initialize a filesource.
-
-    Filesources can extend this typed dict to define any additional
-    filesource specific properties.
-    """
-
-    file_sources_config: NotRequired[FileSourcePluginsConfig]
-    id: NotRequired[str]
-    label: NotRequired[str]
-    doc: NotRequired[Optional[str]]
-    scheme: NotRequired[str]
-    writable: NotRequired[bool]
-    requires_roles: NotRequired[Optional[str]]
-    requires_groups: NotRequired[Optional[str]]
-    disable_templating: NotRequired[Optional[bool]]
-    # API helper values
-    uri_root: NotRequired[str]
-    type: NotRequired[str]
-    browsable: NotRequired[bool]
-    url: NotRequired[Optional[str]]
-    supports: NotRequired[FileSourceSupports]
-
-
-@dataclass
-class FilesSourceOptions:
-    """Options to control behavior of file source operations, such as realize_to, write_from and list."""
-
-    # Indicates access to the FS operation with intent to write.
-    # Even if a file source is "writeable" some directories (or elements) may be restricted or read-only
-    # so those should be skipped while browsing with writeable=True.
-    writeable: Optional[bool] = False
-
-    # Property overrides for values initially configured through the constructor. For example
-    # the HTTPFilesSource passes in additional http_headers through these properties, which
-    # are merged with constructor defined http_headers. The interpretation of these properties
-    # are filesystem specific.
-    extra_props: Optional[FilesSourceProperties] = field(default_factory=lambda: FilesSourceProperties())
-
-
-class EntryData(TypedDict):
-    """Provides data to create a new entry in a file source."""
-
-    name: str
-    # May contain additional properties depending on the file source
-
-
-class Entry(TypedDict):
-    """Represents the result of creating a new entry in a file source."""
-
-    name: str
-    uri: str
-    # May contain additional properties depending on the file source
-    external_link: NotRequired[str]
-
-
-class RemoteEntry(TypedDict):
-    name: str
-    uri: str
-    path: str
-
-
-TDirectoryClass = TypedDict("TDirectoryClass", {"class": Literal["Directory"]})
-TFileClass = TypedDict("TFileClass", {"class": Literal["File"]})
-
-
-class RemoteDirectory(RemoteEntry, TDirectoryClass):
-    pass
-
-
-class RemoteFile(RemoteEntry, TFileClass):
-    size: int
-    ctime: str
-
-
-AnyRemoteEntry = Union[RemoteDirectory, RemoteFile]
-
-
 class SingleFileSource(metaclass=abc.ABCMeta):
     """
     Represents a protocol handler for a single remote file that can be read by or written to by Galaxy.
     A remote file source can typically handle a url like `https://galaxyproject.org/myfile.txt` or
     `drs://myserver/123456`. The filesource abstraction allows programmatic control over the specific source
-    to access, injection of credentials and access control. Filesources are typically listed and configured
+    to access, injection of credentials and access control. File sources are typically listed and configured
     through `file_sources_conf.yml` or programmatically, as required.
 
-    Filesources can be contextualized with a `user_context`, which contains information related to the current
+    File sources can be contextualized with a `user_context`, which contains information related to the current
     user attempting to access that filesource such as the username, preferences, roles etc., which can then
     be used by the filesource to make authorization decisions or inject credentials.
 
-    Filesources are loaded through Galaxy's plugin system in `galaxy.util.plugin_config`.
+    File sources are loaded through Galaxy's plugin system in `galaxy.util.plugin_config`.
     """
 
     @abc.abstractmethod
@@ -270,7 +183,7 @@ def to_relative_path(self, url: str) -> str:
         returned unchanged."""
 
     @abc.abstractmethod
-    def to_dict(self, for_serialization=False, user_context: "OptionalUserContext" = None) -> FilesSourceProperties:
+    def to_dict(self, for_serialization=False, user_context: "OptionalUserContext" = None) -> dict[str, Any]:
         """Return a dictified representation of this FileSource instance.
 
         If ``user_context`` is supplied, properties should be written so user
@@ -285,7 +198,7 @@ def prefer_links(self) -> bool:
 class SupportsBrowsing(metaclass=abc.ABCMeta):
     """An interface indicating that this filesource is browsable.
 
-    Browsable filesources will typically have a root uri from which to start browsing.
+    Browsable file sources will typically have a root uri from which to start browsing.
     e.g. In an s3 bucket, the root uri may be gxfiles://bucket1/
 
     They will also have a list method to list files in a specific path within the filesource.
@@ -311,7 +224,7 @@ def list(
 
 
 class FilesSource(SingleFileSource, SupportsBrowsing):
-    """Represents a combined interface for single or browsable filesources.
+    """Represents a combined interface for single or browsable file sources.
     The `get_browsable` method can be used to determine whether the filesource is browsable and
     implements the `SupportsBrowsing` interface.
     """
@@ -328,12 +241,57 @@ def file_source_type_is_browsable(target_type: type["BaseFilesSource"]) -> bool:
     return target_type.list != BaseFilesSource.list or target_type._list != BaseFilesSource._list
 
 
-class BaseFilesSource(FilesSource):
+class BaseFilesSource(FilesSource, Generic[TTemplateConfig, TResolvedConfig]):
+    """A base class for file sources that can resolve a template configuration to a specific configuration.
+
+    Implementations of this class should define 2 configuration models and assing them to the
+    `template_config_class` and `resolved_config_class` class variables.
+
+    The `template_config_class` should be a subclass of `BaseFileSourceTemplateConfiguration` and
+    the `resolved_config_class` should be a subclass of `BaseFileSourceConfiguration`.
+
+    Assuming a File Source named `MyFileSource`:
+    - `MyFileSourceTemplateConfiguration`: A template configuration class that defines the template variables.
+    - `MyFileSourceConfiguration`: A resolved configuration class that defines the final configuration after template resolution.
+    """
+
     plugin_kind: ClassVar[PluginKind] = PluginKind.rfs  # Remote File Source by default, override in subclasses
     supports_pagination: ClassVar[bool] = False
     supports_search: ClassVar[bool] = False
     supports_sorting: ClassVar[bool] = False
 
+    # Implementations must define these classes with their specific configuration models.
+    template_config_class: type[TTemplateConfig]
+    resolved_config_class: type[TResolvedConfig]
+
+    def __init__(self, template_config: TTemplateConfig):
+        self.template_config = template_config
+        self._parse_common_props(self.template_config)
+
+    @classmethod
+    def build_template_config(cls, **kwargs) -> TTemplateConfig:
+        """Build a template configuration instance from the provided keyword arguments."""
+        cls._ensure_config_models_defined()
+        return cls.template_config_class(**kwargs)
+
+    @classmethod
+    def _ensure_config_models_defined(cls):
+        try:
+            _ = cls.resolved_config_class
+        except AttributeError:
+            raise NotImplementedError(
+                f"Resolved configuration class not defined for plugin type '{cls.plugin_type}'. "
+                "Subclasses of BaseFilesSource must set 'resolved_config_class'."
+            )
+
+        try:
+            _ = cls.template_config_class
+        except AttributeError:
+            raise NotImplementedError(
+                f"Template configuration class not defined for plugin type '{cls.plugin_type}'. "
+                "Subclasses of BaseFilesSource must set 'template_config_class'."
+            )
+
     def get_browsable(self) -> bool:
         return file_source_type_is_browsable(type(self))
 
@@ -385,26 +343,20 @@ def uri_from_path(self, path: str) -> str:
         uri_root = self.get_uri_root()
         return uri_join(uri_root, path)
 
-    def _parse_common_config_opts(self, kwd: FilesSourceProperties):
-        self._file_sources_config = kwd.pop("file_sources_config")
-        self.id = kwd.pop("id")
-        self.label = kwd.pop("label", self.id)
-        self.doc = kwd.pop("doc", None)
-        self.scheme = kwd.pop("scheme", DEFAULT_SCHEME)
-        self.writable = kwd.pop("writable", DEFAULT_WRITABLE)
-        self.requires_roles = kwd.pop("requires_roles", None)
-        self.requires_groups = kwd.pop("requires_groups", None)
-        self.disable_templating = kwd.pop("disable_templating", False)
+    def _parse_common_props(self, config: FilesSourceProperties):
+        self._file_sources_config = config.file_sources_config
+        self.id = config.id
+        self.label = config.label or f"Unlabeled {self.plugin_kind.value.capitalize()} File Source [{self.id}]"
+        self.doc = config.doc
+        self.scheme = config.scheme
+        self.writable = config.writable
+        self.requires_roles = config.requires_roles
+        self.requires_groups = config.requires_groups
+        self.disable_templating = config.disable_templating
         self._validate_security_rules()
-        # If coming from to_dict, strip API helper values
-        kwd.pop("uri_root", None)
-        kwd.pop("type", None)
-        kwd.pop("browsable", None)
-        kwd.pop("supports", None)
-        return kwd
-
-    def to_dict(self, for_serialization=False, user_context: "OptionalUserContext" = None) -> FilesSourceProperties:
-        rval: FilesSourceProperties = {
+
+    def to_dict(self, for_serialization=False, user_context: "OptionalUserContext" = None) -> dict[str, Any]:
+        rval: dict[str, Any] = {
             "id": self.id,
             "type": self.plugin_type,
             "label": self.label,
@@ -426,10 +378,24 @@ def to_dict(self, for_serialization=False, user_context: "OptionalUserContext" =
         if self.get_url() is not None:
             rval["url"] = self.get_url()
         if for_serialization:
-            rval.update(self._serialization_props(user_context=user_context))
+            context = self._get_runtime_context(user_context=user_context)
+            serialized_config = self._serialize_config(context.config)
+            rval.update(serialized_config)
         return rval
 
-    def to_dict_time(self, ctime):
+    def _serialize_config(self, config: TResolvedConfig) -> dict[str, Any]:
+        """Serialize properties needed to recover plugin configuration.
+        Used in to_dict method if for_serialization is True.
+
+        Override this method in subclasses to customize serialization of the configuration.
+        """
+        return config.model_dump(
+            exclude_unset=True,
+            exclude_none=True,
+            exclude=COMMON_FILE_SOURCE_PROP_NAMES,
+        )
+
+    def to_dict_time(self, ctime) -> Optional[str]:
         if ctime is None:
             return None
         elif isinstance(ctime, (int, float)):
@@ -437,11 +403,50 @@ def to_dict_time(self, ctime):
         else:
             return ctime.strftime("%m/%d/%Y %I:%M:%S %p")
 
-    @abc.abstractmethod
-    def _serialization_props(self, user_context: "OptionalUserContext" = None) -> FilesSourceProperties:
-        """Serialize properties needed to recover plugin configuration.
-        Used in to_dict method if for_serialization is True.
+    def _get_runtime_context(
+        self,
+        opts: Optional[FilesSourceOptions] = None,
+        user_context: "OptionalUserContext" = None,
+    ) -> FilesSourceRuntimeContext:
         """
+        Get the runtime context for this file source, resolving the template configuration
+        with the provided user context and options.
+        """
+        user_data = UserData(context=user_context)
+
+        # Update template config with extra properties if provided
+        if opts and opts.extra_props:
+            extra_props = opts.extra_props.model_dump(exclude_unset=True)
+            self.template_config = self.template_config.model_copy(update=extra_props)
+
+        resolved_config = self._evaluate_template_config(user_data)
+        return FilesSourceRuntimeContext(user_data=user_data, config=resolved_config)
+
+    def _apply_defaults_to_template(
+        self, defaults: dict[str, Any], template_config: TTemplateConfig
+    ) -> TTemplateConfig:
+        """
+        Merge default values into the template config.
+
+        Values set in the template config take precedence over defaults.
+        Returns a new template config instance with all required fields set.
+        """
+        template_updates = template_config.model_dump(exclude_none=True, exclude_defaults=True)
+        defaults.update(template_updates)
+        return self.template_config_class(**defaults)
+
+    def _evaluate_template_config(self, user_data: Optional[UserData] = None) -> TResolvedConfig:
+        if self.disable_templating:
+            # Convert template config to resolved config without template evaluation
+            config_dict = self.template_config.model_dump(exclude_unset=True, exclude_none=True)
+            return self.resolved_config_class(**config_dict)
+
+        runtime_context = FilesSourceTemplateContext(
+            user_data=user_data,
+            environment=dict(os.environ),
+            file_sources_config=self._file_sources_config,
+        )
+        return resolve_file_source_template(self.template_config, self.resolved_config_class, runtime_context)
 
     def list(
         self,
@@ -467,14 +472,16 @@ def list(
             if offset is not None and offset < 0:
                 raise RequestParameterInvalidException("Offset must be greater than or equal to 0.")
 
-        return self._list(path, recursive, user_context, opts, limit, offset, query)
+        resolved_config = self._get_runtime_context(opts, user_context)
+        write_intent = opts.write_intent if opts else False
+        return self._list(resolved_config, path, recursive, write_intent, limit, offset, query)
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[TResolvedConfig],
         path="/",
         recursive=False,
-        user_context: "OptionalUserContext" = None,
-        opts: Optional[FilesSourceOptions] = None,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -490,14 +497,10 @@ def create_entry(
     ) -> Entry:
         self._ensure_writeable()
         self._check_user_access(user_context)
-        return self._create_entry(entry_data, user_context, opts)
+        context = self._get_runtime_context(opts, user_context)
+        return self._create_entry(entry_data, context)
 
-    def _create_entry(
-        self,
-        entry_data: EntryData,
-        user_context: "OptionalUserContext" = None,
-        opts: Optional[FilesSourceOptions] = None,
-    ) -> Entry:
+    def _create_entry(self, entry_data: EntryData, context: FilesSourceRuntimeContext[TResolvedConfig]) -> Entry:
         """Create a new entry (directory) in the file source.
 
         The file source must be writeable.
@@ -514,15 +517,15 @@ def write_from(
     ) -> str:
         self._ensure_writeable()
         self._check_user_access(user_context)
-        return self._write_from(target_path, native_path, user_context=user_context, opts=opts) or target_path
+        resolved_config = self._get_runtime_context(opts, user_context)
+        return self._write_from(target_path, native_path, resolved_config) or target_path
 
     @abc.abstractmethod
     def _write_from(
         self,
         target_path: str,
         native_path: str,
-        user_context: "OptionalUserContext" = None,
-        opts: Optional[FilesSourceOptions] = None,
+        context: FilesSourceRuntimeContext[TResolvedConfig],
     ) -> Optional[str]:
         pass
 
@@ -534,15 +537,15 @@ def realize_to(
         opts: Optional[FilesSourceOptions] = None,
     ):
         self._check_user_access(user_context)
-        self._realize_to(source_path, native_path, user_context, opts=opts)
+        resolved_config = self._get_runtime_context(opts, user_context)
+        self._realize_to(source_path, native_path, resolved_config)
 
     @abc.abstractmethod
     def _realize_to(
         self,
         source_path: str,
         native_path: str,
-        user_context: "OptionalUserContext" = None,
-        opts: Optional[FilesSourceOptions] = None,
+        context: FilesSourceRuntimeContext[TResolvedConfig],
     ):
         pass
 
@@ -559,27 +562,6 @@ def _check_user_access(self, user_context):
         if user_context is not None and not self.user_has_access(user_context):
             raise ItemAccessibilityException(f"User {user_context.username} has no access to file source.")
 
-    def _evaluate_prop(self, prop_val: Any, user_context: "OptionalUserContext"):
-        rval = prop_val
-
-        # just return if we've disabled templating for this plugin
-        if self.disable_templating:
-            return rval
-
-        if isinstance(prop_val, str) and "$" in prop_val:
-            template_context = dict(
-                user=user_context,
-                environ=os.environ,
-                config=self._file_sources_config,
-            )
-            rval = fill_template(prop_val, context=template_context, futurized=True)
-        elif isinstance(prop_val, dict):
-            rval = {key: self._evaluate_prop(childprop, user_context) for key, childprop in prop_val.items()}
-        elif isinstance(prop_val, list):
-            rval = [self._evaluate_prop(childprop, user_context) for childprop in prop_val]
-
-        return rval
-
     def _user_has_required_roles(self, user_context: "FileSourcesUserContext") -> bool:
         if self.requires_roles:
             return self._evaluate_security_rules(self.requires_roles, user_context.role_names)
@@ -621,3 +603,10 @@ def uri_join(*args: str) -> str:
 def slash_join(*args: str) -> str:
     # https://codereview.stackexchange.com/questions/175421/joining-strings-to-form-a-url
     return "/".join(arg.strip("/") for arg in args)
+
+
+class DefaultBaseFilesSource(BaseFilesSource[BaseFileSourceTemplateConfiguration, BaseFileSourceConfiguration]):
+    """A default implementation of BaseFilesSource that uses the base configuration models without any custom configuration."""
+
+    template_config_class = BaseFileSourceTemplateConfiguration
+    resolved_config_class = BaseFileSourceConfiguration
diff --git a/lib/galaxy/files/sources/_defaults.py b/lib/galaxy/files/sources/_defaults.py
new file mode 100644
index 000000000000..1b6a966d0f30
--- /dev/null
+++ b/lib/galaxy/files/sources/_defaults.py
@@ -0,0 +1,2 @@
+DEFAULT_WRITABLE = False
+DEFAULT_SCHEME = "gxfiles"
diff --git a/lib/galaxy/files/sources/_pyfilesystem2.py b/lib/galaxy/files/sources/_pyfilesystem2.py
index 6bce4349497e..7751afb1c71c 100644
--- a/lib/galaxy/files/sources/_pyfilesystem2.py
+++ b/lib/galaxy/files/sources/_pyfilesystem2.py
@@ -10,19 +10,22 @@
 import fs
 import fs.errors
 from fs.base import FS
-from typing_extensions import Unpack
 
 from galaxy.exceptions import (
     AuthenticationRequired,
     MessageException,
 )
-from galaxy.files import OptionalUserContext
-from . import (
+from galaxy.files.models import (
     AnyRemoteEntry,
+    FilesSourceRuntimeContext,
+    RemoteDirectory,
+    RemoteFile,
+    TResolvedConfig,
+    TTemplateConfig,
+)
+from . import (
     BaseFilesSource,
     DEFAULT_PAGE_LIMIT,
-    FilesSourceOptions,
-    FilesSourceProperties,
 )
 
 log = logging.getLogger(__name__)
@@ -30,29 +33,35 @@
 PACKAGE_MESSAGE = "FilesSource plugin is missing required Python PyFilesystem2 plugin package [%s]"
 
 
-class PyFilesystem2FilesSource(BaseFilesSource):
+class PyFilesystem2FilesSource(BaseFilesSource[TTemplateConfig, TResolvedConfig]):
     required_module: ClassVar[Optional[type[FS]]]
     required_package: ClassVar[str]
     supports_pagination = True
     supports_search = True
     allow_key_error_on_empty_directories = False  # work around a bug in webdav
 
-    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
+    def __init__(self, template_config: TTemplateConfig):
         if self.required_module is None:
-            raise Exception(PACKAGE_MESSAGE % self.required_package)
-        props = self._parse_common_config_opts(kwd)
-        self._props = props
+            raise self.required_package_exception
+        super().__init__(template_config)
+
+    @property
+    def required_package_exception(self) -> Exception:
+        return Exception(PACKAGE_MESSAGE % self.required_package)
 
     @abc.abstractmethod
-    def _open_fs(self, user_context: OptionalUserContext = None, opts: Optional[FilesSourceOptions] = None) -> FS:
-        """Subclasses must instantiate a PyFilesystem2 handle for this file system."""
+    def _open_fs(self, context: FilesSourceRuntimeContext[TResolvedConfig]) -> FS:
+        """Subclasses must instantiate a PyFilesystem2 handle for this file system.
+
+        All the required properties should be already set in the config.
+        """
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[TResolvedConfig],
         path="/",
         recursive=False,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -60,7 +69,7 @@ def _list(
     ) -> tuple[list[AnyRemoteEntry], int]:
         """Return dictionary of 'Directory's and 'File's."""
         try:
-            with self._open_fs(user_context=user_context, opts=opts) as h:
+            with self._open_fs(context) as h:
                 if recursive:
                     recursive_result: list[AnyRemoteEntry] = []
                     try:
@@ -102,49 +111,30 @@ def _query_to_filter(self, query: Optional[str]) -> Optional[list[str]]:
             return None
         return [f"*{query}*"]
 
-    def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
-    ):
+    def _realize_to(self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[TResolvedConfig]):
         with open(native_path, "wb") as write_file:
-            self._open_fs(user_context=user_context, opts=opts).download(source_path, write_file)
+            self._open_fs(context).download(source_path, write_file)
 
-    def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
-    ):
+    def _write_from(self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[TResolvedConfig]):
         with open(native_path, "rb") as read_file:
-            openfs = self._open_fs(user_context=user_context, opts=opts)
+            openfs = self._open_fs(context)
             dirname = fs.path.dirname(target_path)
             if not openfs.isdir(dirname):
                 openfs.makedirs(dirname)
             openfs.upload(target_path, read_file)
 
     def _resource_info_to_dict(self, dir_path, resource_info) -> AnyRemoteEntry:
-        name = resource_info.name
+        name = str(resource_info.name)
         path = os.path.join(dir_path, name)
         uri = self.uri_from_path(path)
         if resource_info.is_dir:
-            return {"class": "Directory", "name": name, "uri": uri, "path": path}
+            return RemoteDirectory(name=name, uri=uri, path=path)
         else:
             created = resource_info.created
-            return {
-                "class": "File",
-                "name": name,
-                "size": resource_info.size,
-                "ctime": self.to_dict_time(created),
-                "uri": uri,
-                "path": path,
-            }
-
-    def _serialization_props(self, user_context: OptionalUserContext = None):
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        return effective_props
+            return RemoteFile(
+                name=name,
+                size=resource_info.size,
+                ctime=self.to_dict_time(created),
+                uri=uri,
+                path=path,
+            )
diff --git a/lib/galaxy/files/sources/_rdm.py b/lib/galaxy/files/sources/_rdm.py
index 1da6d0aec182..335f7299a157 100644
--- a/lib/galaxy/files/sources/_rdm.py
+++ b/lib/galaxy/files/sources/_rdm.py
@@ -1,24 +1,33 @@
 import logging
 from typing import (
+    Any,
     NamedTuple,
     Optional,
+    Union,
 )
 
-from typing_extensions import Unpack
-
-from galaxy.files import OptionalUserContext
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+    RemoteDirectory,
+    RemoteFile,
+)
 from galaxy.files.sources import (
     BaseFilesSource,
-    FilesSourceProperties,
     PluginKind,
-    RemoteDirectory,
-    RemoteFile,
 )
+from galaxy.util.config_templates import TemplateExpansion
 
 log = logging.getLogger(__name__)
 
 
-class RDMFilesSourceProperties(FilesSourceProperties):
+class RDMFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    token: Union[str, TemplateExpansion]
+    public_name: Union[str, TemplateExpansion]
+
+
+class RDMFileSourceConfiguration(BaseFileSourceConfiguration):
     token: str
     public_name: str
 
@@ -66,8 +75,8 @@ def to_plugin_uri(self, container_id: str, filename: Optional[str] = None) -> st
 
     def get_file_containers(
         self,
-        writeable: bool,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
+        write_intent: bool,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -76,15 +85,14 @@ def get_file_containers(
         """Returns the list of file containers in the repository and the total count containers.
 
         If writeable is True, only containers that the user can write to will be returned.
-        The user_context might be required to authenticate the user in the repository.
         """
         raise NotImplementedError()
 
     def get_files_in_container(
         self,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         container_id: str,
         writeable: bool,
-        user_context: OptionalUserContext = None,
         query: Optional[str] = None,
     ) -> list[RemoteFile]:
         """Returns the list of files of a file container.
@@ -93,7 +101,9 @@ def get_files_in_container(
         """
         raise NotImplementedError()
 
-    def create_draft_file_container(self, title: str, public_name: str, user_context: OptionalUserContext = None):
+    def create_draft_file_container(
+        self, title: str, public_name: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
+    ) -> dict[str, Any]:
         """Creates a draft file container in the repository with basic metadata.
 
         The metadata is usually just the title of the container and the user that created it.
@@ -105,14 +115,13 @@ def upload_file_to_draft_container(
         container_id: str,
         filename: str,
         file_path: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ) -> None:
         """Uploads a file with the provided filename (from file_path) to a draft container with the given container_id.
 
         The draft container must have been created in advance with the `create_draft_file_container` method.
 
         The file must exist in the file system at the given file_path.
-        The user_context might be required to authenticate the user in the repository.
         """
         raise NotImplementedError()
 
@@ -121,18 +130,16 @@ def download_file_from_container(
         container_id: str,
         file_identifier: str,
         file_path: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ) -> None:
         """Downloads a file with the provided filename from the container with the given container_id.
 
         The file will be downloaded to the file system at the given file_path.
-        The user_context might be required to authenticate the user in the repository if the
-        file is not publicly available.
         """
         raise NotImplementedError()
 
 
-class RDMFilesSource(BaseFilesSource):
+class RDMFilesSource(BaseFilesSource[RDMFileSourceTemplateConfiguration, RDMFileSourceConfiguration]):
     """Base class for Research Data Management (RDM) file sources.
 
     This class is not intended to be used directly, but rather to be subclassed
@@ -148,20 +155,21 @@ class RDMFilesSource(BaseFilesSource):
 
     plugin_kind = PluginKind.rdm
 
-    def __init__(self, **kwd: Unpack[RDMFilesSourceProperties]):
-        props = self._parse_common_config_opts(kwd)
-        self.url = props.get("url")
-        if not self.url:
+    template_config_class = RDMFileSourceTemplateConfiguration
+    resolved_config_class = RDMFileSourceConfiguration
+
+    def __init__(self, template_config: RDMFileSourceTemplateConfiguration):
+        super().__init__(template_config)
+        if not self.template_config.url:
             raise Exception("URL for RDM repository must be provided in configuration")
-        self._props = props
-        self._repository_interactor = self.get_repository_interactor(self.url)
+        self._repository_interactor = self.get_repository_interactor(self.template_config.url)
 
     @property
     def repository(self) -> RDMRepositoryInteractor:
         return self._repository_interactor
 
     def get_url(self) -> Optional[str]:
-        return self.url
+        return self.template_config.url
 
     def get_repository_interactor(self, repository_url: str) -> RDMRepositoryInteractor:
         """Returns an interactor compatible with the given repository URL.
@@ -180,16 +188,8 @@ def parse_path(self, source_path: str, container_id_only: bool = False) -> Conta
     def get_container_id_from_path(self, source_path: str) -> str:
         raise NotImplementedError()
 
-    def _serialization_props(self, user_context: OptionalUserContext = None):
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        return effective_props
-
-    def get_authorization_token(self, user_context: OptionalUserContext) -> Optional[str]:
-        effective_props = self._serialization_props(user_context)
-        return effective_props.get("token")
+    def get_authorization_token(self, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]) -> Optional[str]:
+        return context.config.token
 
-    def get_public_name(self, user_context: OptionalUserContext) -> Optional[str]:
-        effective_props = self._serialization_props(user_context)
-        return effective_props.get("public_name")
+    def get_public_name(self, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]) -> str:
+        return context.config.public_name or "Anonymous Galaxy User"
diff --git a/lib/galaxy/files/sources/anvil.py b/lib/galaxy/files/sources/anvil.py
index a98c58001919..0dee14253971 100644
--- a/lib/galaxy/files/sources/anvil.py
+++ b/lib/galaxy/files/sources/anvil.py
@@ -7,22 +7,51 @@
     Union,
 )
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class AnVILFilesSource(PyFilesystem2FilesSource):
+class AnVILFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    namespace: Union[str, TemplateExpansion]
+    workspace: Union[str, TemplateExpansion]
+    api_url: Union[str, TemplateExpansion, None] = None
+    on_anvil: Union[bool, TemplateExpansion, None] = False
+    drs_url: Union[str, TemplateExpansion, None] = None
+
+
+class AnVILFileSourceConfiguration(BaseFileSourceConfiguration):
+    namespace: str
+    workspace: str
+    api_url: Optional[str] = None
+    on_anvil: Optional[bool] = False
+    drs_url: Optional[str] = None
+
+
+class AnVILFilesSource(PyFilesystem2FilesSource[AnVILFileSourceTemplateConfiguration, AnVILFileSourceConfiguration]):
     plugin_type = "anvil"
     required_module = AnVILFS
     required_package = "fs.anvilfs"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        handle = AnVILFS(**{**props, **extra_props})
+    template_config_class = AnVILFileSourceTemplateConfiguration
+    resolved_config_class = AnVILFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[AnVILFileSourceConfiguration]):
+        if AnVILFS is None:
+            raise self.required_package_exception
+
+        config = context.config
+        handle = AnVILFS(
+            namespace=config.namespace,
+            workspace=config.workspace,
+            api_url=config.api_url,
+            on_anvil=config.on_anvil,
+            drs_url=config.drs_url,
+        )
         return handle
 
 
diff --git a/lib/galaxy/files/sources/azure.py b/lib/galaxy/files/sources/azure.py
index 125bbc291919..25fc06fbaca8 100644
--- a/lib/galaxy/files/sources/azure.py
+++ b/lib/galaxy/files/sources/azure.py
@@ -1,44 +1,67 @@
-from typing import Union
+from typing_extensions import Literal
 
 try:
-    from fs.azblob import (
-        BlobFS,
-        BlobFSV2,
-    )
+    from fs.azblob.blob_fs import BlobFS
+    from fs.azblob.blob_fs_v2 import BlobFSV2
 except ImportError:
     BlobFS = None
+    BlobFSV2 = None
 
-from typing import Optional
+from typing import (
+    Optional,
+    Union,
+)
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
+AzureNamespaceType = Literal["hierarchical", "flat"]
+
+
+class AzureFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    account_name: Union[str, TemplateExpansion]
+    container_name: Union[str, TemplateExpansion]
+    account_key: Union[str, TemplateExpansion]
+    namespace_type: Optional[AzureNamespaceType] = "hierarchical"
+
+
+class AzureFileSourceConfiguration(BaseFileSourceConfiguration):
+    account_name: str
+    container_name: str
+    account_key: str
+    namespace_type: Optional[AzureNamespaceType] = "hierarchical"
 
-class AzureFileSource(PyFilesystem2FilesSource):
+
+class AzureFileSource(PyFilesystem2FilesSource[AzureFileSourceTemplateConfiguration, AzureFileSourceConfiguration]):
     plugin_type = "azure"
     required_module = BlobFS
     required_package = "fs-azureblob"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        all_props = {**props, **extra_props}
-        namespace_type = all_props.get("namespace_type", "hierarchical")
-        if namespace_type not in ["hierarchical", "flat"]:
-            raise Exception("Misconfigured azure file source")
-        account_name = all_props["account_name"]
-        account_key = all_props["account_key"]
-        container = all_props["container_name"]
-        handle: Union[BlobFSV2, BlobFS]
-        if namespace_type == "flat":
-            handle = BlobFS(account_name, container, account_key)
-        else:
-            handle = BlobFSV2(account_name, container, account_key)
+    template_config_class = AzureFileSourceTemplateConfiguration
+    resolved_config_class = AzureFileSourceConfiguration
 
-        return handle
+    def _open_fs(self, context: FilesSourceRuntimeContext[AzureFileSourceConfiguration]):
+        config = context.config
+        if BlobFS is None or BlobFSV2 is None:
+            raise self.required_package_exception
+
+        if config.namespace_type == "flat":
+            return BlobFS(
+                account_name=config.account_name,
+                container=config.container_name,
+                account_key=config.account_key,
+            )
+        else:
+            return BlobFSV2(
+                account_name=config.account_name,
+                container=config.container_name,
+                account_key=config.account_key,
+            )
 
 
 __all__ = ("AzureFileSource",)
diff --git a/lib/galaxy/files/sources/base64.py b/lib/galaxy/files/sources/base64.py
index 968ae499ac5f..1977f111afe2 100644
--- a/lib/galaxy/files/sources/base64.py
+++ b/lib/galaxy/files/sources/base64.py
@@ -1,52 +1,42 @@
 import base64
 import logging
-from typing import Optional
 
-from typing_extensions import Unpack
-
-from galaxy.files import OptionalUserContext
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
 from . import (
-    BaseFilesSource,
-    FilesSourceOptions,
-    FilesSourceProperties,
+    DefaultBaseFilesSource,
     PluginKind,
 )
 
 log = logging.getLogger(__name__)
 
 
-class Base64FilesSource(BaseFilesSource):
+class Base64FilesSource(DefaultBaseFilesSource):
     plugin_type = "base64"
     plugin_kind = PluginKind.stock
 
-    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
-        kwds: FilesSourceProperties = dict(
+    def __init__(self, template_config: BaseFileSourceTemplateConfiguration):
+        defaults = dict(
             id="_base64",
             label="Base64 encoded string",
             doc="Base64 string handler",
             writable=False,
         )
-        kwds.update(kwd)
-        props = self._parse_common_config_opts(kwds)
-        self._props = props
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[BaseFileSourceConfiguration]
     ):
         with open(native_path, "wb") as temp:
             temp.write(base64.b64decode(source_path[len("base64://") :]))
             temp.flush()
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[BaseFileSourceConfiguration]
     ):
         raise NotImplementedError()
 
@@ -56,11 +46,5 @@ def score_url_match(self, url: str):
         else:
             return 0
 
-    def _serialization_props(self, user_context: OptionalUserContext = None):
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        return effective_props
-
 
 __all__ = ("Base64FilesSource",)
diff --git a/lib/galaxy/files/sources/basespace.py b/lib/galaxy/files/sources/basespace.py
index 3229955857d6..7c20fd0a7756 100644
--- a/lib/galaxy/files/sources/basespace.py
+++ b/lib/galaxy/files/sources/basespace.py
@@ -8,23 +8,53 @@
     Union,
 )
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class BaseSpaceFilesSource(PyFilesystem2FilesSource):
+class BaseSpaceFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    dir_path: Union[str, TemplateExpansion, None] = "/"
+    client_id: Union[str, TemplateExpansion, None] = None
+    client_secret: Union[str, TemplateExpansion, None] = None
+    access_token: Union[str, TemplateExpansion, None] = None
+    basespace_server: Union[str, TemplateExpansion, None] = None
+
+
+class BaseSpaceFileSourceConfiguration(BaseFileSourceConfiguration):
+    dir_path: Optional[str] = "/"
+    client_id: Optional[str] = None
+    client_secret: Optional[str] = None
+    access_token: Optional[str] = None
+    basespace_server: Optional[str] = None
+
+
+class BaseSpaceFilesSource(
+    PyFilesystem2FilesSource[BaseSpaceFileSourceTemplateConfiguration, BaseSpaceFileSourceConfiguration]
+):
     plugin_type = "basespace"
     required_module = BASESPACEFS
     required_package = "fs-basespace"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        handle = BASESPACEFS(**{**props, **extra_props})
-        return handle
+    template_config_class = BaseSpaceFileSourceTemplateConfiguration
+    resolved_config_class = BaseSpaceFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[BaseSpaceFileSourceConfiguration]):
+        if BASESPACEFS is None:
+            raise self.required_package_exception
+
+        config = context.config
+        return BASESPACEFS(
+            dir_path=config.dir_path,
+            client_id=config.client_id,
+            client_secret=config.client_secret,
+            access_token=config.access_token,
+            basespace_server=config.basespace_server,
+        )
 
 
 __all__ = ("BaseSpaceFilesSource",)
diff --git a/lib/galaxy/files/sources/dataverse.py b/lib/galaxy/files/sources/dataverse.py
index d1bd786f3229..1e1dc41684b3 100644
--- a/lib/galaxy/files/sources/dataverse.py
+++ b/lib/galaxy/files/sources/dataverse.py
@@ -9,27 +9,24 @@
 from urllib.error import HTTPError
 from urllib.parse import quote
 
-from typing_extensions import (
-    TypedDict,
-    Unpack,
-)
+from typing_extensions import TypedDict
 
 from galaxy.exceptions import AuthenticationRequired
-from galaxy.files import OptionalUserContext
-from galaxy.files.sources import (
+from galaxy.files.models import (
     AnyRemoteEntry,
-    DEFAULT_PAGE_LIMIT,
-    DEFAULT_SCHEME,
     Entry,
     EntryData,
-    FilesSourceOptions,
+    FilesSourceRuntimeContext,
     RemoteDirectory,
     RemoteFile,
 )
+from galaxy.files.sources import DEFAULT_PAGE_LIMIT
+from galaxy.files.sources._defaults import DEFAULT_SCHEME
 from galaxy.files.sources._rdm import (
     ContainerAndFileIdentifier,
+    RDMFileSourceConfiguration,
+    RDMFileSourceTemplateConfiguration,
     RDMFilesSource,
-    RDMFilesSourceProperties,
     RDMRepositoryInteractor,
 )
 from galaxy.util import (
@@ -70,8 +67,8 @@ class DataverseRDMFilesSource(RDMFilesSource):
     supports_pagination = True
     supports_search = True
 
-    def __init__(self, **kwd: Unpack[RDMFilesSourceProperties]):
-        super().__init__(**kwd)
+    def __init__(self, template_config: RDMFileSourceTemplateConfiguration):
+        super().__init__(template_config)
         self._scheme_regex = re.compile(rf"^{self.get_scheme()}?://{self.id}|^{DEFAULT_SCHEME}://{self.id}")
         self.repository: DataverseRepositoryInteractor
 
@@ -128,58 +125,47 @@ def get_container_id_from_path(self, source_path: str) -> str:
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         path="/",
-        recursive=True,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        recursive=False,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
         sort_by: Optional[str] = None,
     ) -> tuple[list[AnyRemoteEntry], int]:
         """This method lists the datasets or files from dataverse."""
-        writeable = opts and opts.writeable or False
         is_root_path = path == "/"
         if is_root_path:
             datasets, total_hits = self.repository.get_file_containers(
-                writeable, user_context, limit=limit, offset=offset, query=query
+                context, write_intent, limit=limit, offset=offset, query=query
             )
             return cast(list[AnyRemoteEntry], datasets), total_hits
         dataset_id = self.get_container_id_from_path(path)
-        files = self.repository.get_files_in_container(dataset_id, writeable, user_context, query)
+        files = self.repository.get_files_in_container(context, dataset_id, write_intent, query)
         return cast(list[AnyRemoteEntry], files), len(files)
 
     def _create_entry(
-        self,
-        entry_data: EntryData,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, entry_data: EntryData, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ) -> Entry:
         """Creates a draft dataset in the repository."""
-        public_name = self.get_public_name(user_context) or "Anonymous Galaxy User"
-        title = entry_data.get("name") or "No title"
-        dataset = self.repository.create_draft_file_container(title, public_name, user_context)
+        public_name = self.get_public_name(context)
+        title = entry_data.name or "No title"
+        dataset = self.repository.create_draft_file_container(title, public_name, context)
         datasetId = str(dataset.get("persistentId"))
-        return {
-            "uri": self.repository.to_plugin_uri(datasetId),
-            "name": title,
-            "external_link": self.repository.public_dataset_url(datasetId),
-        }
+        return Entry(
+            name=title,
+            uri=self.repository.to_plugin_uri(datasetId),
+            external_link=self.repository.public_dataset_url(datasetId),
+        )
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ):
         """Used when downloading files from dataverse."""
-        # TODO: user_context is always None here when called from a data fetch. (same problem as in invenio.py)
-        # This prevents downloading files that require authentication even if the user provided a token.
-
         dataset_id, file_id = self.parse_path(source_path)
         try:
-            self.repository.download_file_from_container(dataset_id, file_id, native_path, user_context=user_context)
+            self.repository.download_file_from_container(dataset_id, file_id, native_path, context)
         except NotFoundException:
             filename = file_id.split("/")[-1]
             is_zip_file = self._is_zip_archive(filename)
@@ -189,21 +175,17 @@ def _realize_to(
                 # Only the contents are stored, not the zip itself.
                 # So, if a zip is not found, we suppose we are trying to reimport an archived history
                 # and make an API call to Dataverse to download the dataset as a zip.
-                self.repository._download_dataset_as_zip(dataset_id, native_path, user_context)
+                self.repository._download_dataset_as_zip(dataset_id, native_path, context)
 
     def _is_zip_archive(self, file_name: str) -> bool:
         return file_name.endswith(".zip")
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ):
         """Used when uploading files to dataverse."""
         dataset_id, file_id = self.parse_path(target_path)
-        self.repository.upload_file_to_draft_container(dataset_id, file_id, native_path, user_context=user_context)
+        self.repository.upload_file_to_draft_container(dataset_id, file_id, native_path, context)
 
 
 class DataverseRepositoryInteractor(RDMRepositoryInteractor):
@@ -247,8 +229,8 @@ def _is_api_url(self, url: str) -> bool:
 
     def get_file_containers(
         self,
-        writeable: bool,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
+        write_intent: bool,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -263,22 +245,22 @@ def get_file_containers(
             "q": f"title:{query}" if query else "*",
             "sort": sort_by or "date",
         }
-        if writeable:
+        if write_intent:
             params["fq"] = "publicationStatus:Draft"
-        response_data = self._get_response(user_context, request_url, params)
+        response_data = self._get_response(context, request_url, params)
         total_hits = response_data["data"]["total_count"]
         return self._get_datasets_from_response(response_data["data"]), total_hits
 
     def get_files_in_container(
         self,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         container_id: str,
         writeable: bool,
-        user_context: OptionalUserContext = None,
         query: Optional[str] = None,
     ) -> list[RemoteFile]:
         """This method lists the files in a dataverse dataset."""
         request_url = self.files_of_dataset_url(dataset_id=container_id)
-        response_data = self._get_response(user_context, request_url)
+        response_data = self._get_response(context, request_url)
         files = self._get_files_from_response(container_id, response_data["data"])
         files = self._filter_files_by_name(files, query)
         return files
@@ -286,24 +268,25 @@ def get_files_in_container(
     def _filter_files_by_name(self, files: list[RemoteFile], query: Optional[str] = None) -> list[RemoteFile]:
         if not query:
             return files
-        return [file for file in files if query in file["name"]]
+        return [file for file in files if query in file.name]
 
     def create_draft_file_container(
-        self, title: str, public_name: str, user_context: OptionalUserContext = None
-    ) -> RemoteDirectory:
+        self, title: str, public_name: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
+    ) -> dict[str, Any]:
         """Creates a draft dataset in the repository. Dataverse datasets are contained in collections. Collections can be contained in collections.
         We create a collection inside the root collection and then a dataset inside that collection.
         """
         # Prepare and create the collection
-        collection_payload = self._prepare_collection_data(title, public_name, user_context)
-        collection = self._create_collection(":root", collection_payload, user_context)
+        user_email = context.user_data.email or "enteryourmail@placeholder.com"
+        collection_payload = self._prepare_collection_data(title, public_name, user_email)
+        collection = self._create_collection(":root", collection_payload, context)
         if not collection or "data" not in collection or "alias" not in collection["data"]:
             raise Exception("Could not create collection in Dataverse or response has an unexpected format.")
         collection_alias = collection["data"]["alias"]
 
         # Prepare and create the dataset
-        dataset_payload = self._prepare_dataset_data(title, public_name, user_context)
-        dataset = self._create_dataset(collection_alias, dataset_payload, user_context)
+        dataset_payload = self._prepare_dataset_data(title, public_name, user_email)
+        dataset = self._create_dataset(collection_alias, dataset_payload, context)
         if not dataset or "data" not in dataset:
             raise Exception("Could not create dataset in Dataverse or response has an unexpected format.")
 
@@ -311,17 +294,17 @@ def create_draft_file_container(
         return dataset["data"]
 
     def _create_collection(
-        self, parent_alias: str, collection_payload: str, user_context: OptionalUserContext = None
+        self, parent_alias: str, collection_payload: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ) -> dict:
-        headers = self._get_request_headers(user_context, auth_required=True)
+        headers = self._get_request_headers(context, auth_required=True)
         response = requests.post(self.create_collection_url(parent_alias), data=collection_payload, headers=headers)
         self._ensure_response_has_expected_status_code(response, 201)
         return response.json()
 
     def _create_dataset(
-        self, parent_alias: str, dataset_payload: str, user_context: OptionalUserContext = None
+        self, parent_alias: str, dataset_payload: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ) -> dict:
-        headers = self._get_request_headers(user_context, auth_required=True)
+        headers = self._get_request_headers(context, auth_required=True)
         response = requests.post(self.create_dataset_url(parent_alias), data=dataset_payload, headers=headers)
         self._ensure_response_has_expected_status_code(response, 201)
         return response.json()
@@ -331,10 +314,10 @@ def upload_file_to_draft_container(
         dataset_id: str,
         filename: str,
         file_path: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ):
         """Uploads a file to a draft dataset in the repository."""
-        headers = self._get_request_headers(user_context, auth_required=True)
+        headers = self._get_request_headers(context, auth_required=True)
 
         with open(file_path, "rb") as file:
             # TODO: For some reason tar.gz files are not uploaded successfully to Dataverse.
@@ -348,26 +331,28 @@ def download_file_from_container(
         container_id: str,
         file_identifier: str,
         file_path: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ):
         download_file_content_url = self.file_access_url(file_identifier)
-        self._download_file(file_path, download_file_content_url, user_context)
+        self._download_file(file_path, download_file_content_url, context)
 
-    def _download_dataset_as_zip(self, dataset_id: str, file_path: str, user_context: OptionalUserContext = None):
+    def _download_dataset_as_zip(
+        self, dataset_id: str, file_path: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
+    ):
         download_dataset_url = self.download_dataset_as_zip_url(dataset_id)
-        self._download_file(file_path, download_dataset_url, user_context)
+        self._download_file(file_path, download_dataset_url, context)
 
     def _download_file(
         self,
         file_path: str,
         download_file_content_url: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ):
         headers = {}
 
         if self._is_api_url(download_file_content_url):
             # pass the token as a header only when using the API
-            headers = self._get_request_headers(user_context)
+            headers = self._get_request_headers(context)
         try:
             req = urllib.request.Request(download_file_content_url, headers=headers)
             with urllib.request.urlopen(req, timeout=DEFAULT_SOCKET_TIMEOUT) as page:
@@ -387,12 +372,11 @@ def _get_datasets_from_response(self, response: dict) -> list[RemoteDirectory]:
         for dataset in response["items"]:
             uri = self.to_plugin_uri(dataset_id=dataset["global_id"])
             rval.append(
-                {
-                    "class": "Directory",
-                    "name": dataset.get("name") or "No title",
-                    "uri": uri,
-                    "path": self.plugin.to_relative_path(uri),
-                }
+                RemoteDirectory(
+                    name=dataset.get("name") or "No title",
+                    uri=uri,
+                    path=self.plugin.to_relative_path(uri),
+                )
             )
         return rval
 
@@ -402,31 +386,32 @@ def _get_files_from_response(self, dataset_id: str, response: dict) -> list[Remo
             dataFile = entry.get("dataFile")
             uri = self.to_plugin_uri(dataset_id, dataFile.get("persistentId"))
             rval.append(
-                {
-                    "class": "File",
-                    "name": dataFile.get("filename"),
-                    "size": dataFile.get("filesize"),
-                    "ctime": dataFile.get("creationDate"),
-                    "uri": uri,
-                    "path": self.plugin.to_relative_path(uri),
-                }
+                RemoteFile(
+                    name=dataFile.get("filename"),
+                    size=dataFile.get("filesize"),
+                    ctime=dataFile.get("creationDate"),
+                    uri=uri,
+                    path=self.plugin.to_relative_path(uri),
+                )
             )
         return rval
 
     def _get_response(
         self,
-        user_context: OptionalUserContext,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         request_url: str,
         params: Optional[dict[str, Any]] = None,
         auth_required: bool = False,
     ) -> dict:
-        headers = self._get_request_headers(user_context, auth_required)
+        headers = self._get_request_headers(context, auth_required)
         response = requests.get(request_url, params=params, headers=headers)
         self._ensure_response_has_expected_status_code(response, 200)
         return response.json()
 
-    def _get_request_headers(self, user_context: OptionalUserContext, auth_required: bool = False):
-        token = self.plugin.get_authorization_token(user_context)
+    def _get_request_headers(
+        self, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration], auth_required: bool = False
+    ):
+        token = self.plugin.get_authorization_token(context)
         headers = {"X-Dataverse-Key": f"{token}"} if token else {}
         if auth_required and token is None:
             self._raise_auth_required()
@@ -454,38 +439,24 @@ def _get_response_error_message(self, response):
             error_message += f"\n{json.dumps(error)}"
         return error_message
 
-    def _get_user_email(self, user_context: OptionalUserContext = None) -> str:
-        return user_context.email if user_context and user_context.email else "enteryourmail@placeholder.com"
-
     def _create_valid_alias(self, public_name: str, title: str) -> str:
         return re.sub(
             r"[^a-zA-Z0-9-_]", "", public_name.lower().replace(" ", "-") + "_" + title.lower().replace(" ", "-")
         )
 
-    def _prepare_collection_data(
-        self,
-        title: str,
-        public_name: str,
-        user_context: OptionalUserContext = None,
-    ) -> str:
+    def _prepare_collection_data(self, title: str, public_name: str, user_email: str) -> str:
         return json.dumps(
             {
                 "name": title,
                 "alias": self._create_valid_alias(public_name, title),
                 "dataverseContacts": [
-                    {"contactEmail": self._get_user_email(user_context)},
+                    {"contactEmail": user_email},
                 ],
             }
         )
 
-    def _prepare_dataset_data(
-        self,
-        title: str,
-        public_name: str,
-        user_context: OptionalUserContext = None,
-    ) -> str:
+    def _prepare_dataset_data(self, title: str, public_name: str, user_email: str) -> str:
         """Prepares the dataset data with all required metadata fields."""
-        user_email = self._get_user_email(user_context)
         author_name = public_name
         dataset_data = {
             "datasetVersion": {
diff --git a/lib/galaxy/files/sources/dropbox.py b/lib/galaxy/files/sources/dropbox.py
index e8bad36d2812..c8869642395e 100644
--- a/lib/galaxy/files/sources/dropbox.py
+++ b/lib/galaxy/files/sources/dropbox.py
@@ -1,41 +1,63 @@
 try:
-    from fs.dropboxfs import DropboxFS
+    from fs.dropboxfs.dropboxfs import DropboxFS
 except ImportError:
     DropboxFS = None
 
+
 from typing import (
-    Optional,
+    Annotated,
     Union,
 )
 
+from pydantic import (
+    AliasChoices,
+    Field,
+)
+
 from galaxy.exceptions import (
     AuthenticationRequired,
     MessageException,
 )
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
+AccessTokenField = Field(
+    ...,
+    title="Access Token",
+    description="The access token for Dropbox. You can generate one from your Dropbox app settings.",
+    validation_alias=AliasChoices("oauth2_access_token", "accessToken", "access_token"),
+)
+
 
-class DropboxFilesSource(PyFilesystem2FilesSource):
+class DropboxFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    access_token: Annotated[Union[str, TemplateExpansion], AccessTokenField]
+
+
+class DropboxFilesSourceConfiguration(BaseFileSourceConfiguration):
+    access_token: Annotated[str, AccessTokenField]
+
+
+class DropboxFilesSource(
+    PyFilesystem2FilesSource[DropboxFileSourceTemplateConfiguration, DropboxFilesSourceConfiguration]
+):
     plugin_type = "dropbox"
     required_module = DropboxFS
     required_package = "fs.dropboxfs"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        # accessToken has been renamed to access_token in fs.dropboxfs 1.0
-        if "accessToken" in props:
-            props["access_token"] = props.pop("accessToken")
-        if "oauth2_access_token" in props:
-            props["access_token"] = props.pop("oauth2_access_token")
+    template_config_class = DropboxFileSourceTemplateConfiguration
+    resolved_config_class = DropboxFilesSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[DropboxFilesSourceConfiguration]):
+        if DropboxFS is None:
+            raise self.required_package_exception
 
         try:
-            handle = DropboxFS(**{**props, **extra_props})
-            return handle
+            return DropboxFS(access_token=context.config.access_token)
         except Exception as e:
             # This plugin might raise dropbox.dropbox_client.BadInputException
             # which is not a subclass of fs.errors.FSError
diff --git a/lib/galaxy/files/sources/drs.py b/lib/galaxy/files/sources/drs.py
index 72e35f2b70bf..eb3277da903a 100644
--- a/lib/galaxy/files/sources/drs.py
+++ b/lib/galaxy/files/sources/drs.py
@@ -1,17 +1,15 @@
 import logging
 import re
-from typing import (
-    cast,
-    Optional,
-)
-
-from typing_extensions import Unpack
+from typing import Union
 
-from galaxy.files import OptionalUserContext
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
+from galaxy.util.config_templates import TemplateExpansion
 from . import (
     BaseFilesSource,
-    FilesSourceOptions,
-    FilesSourceProperties,
     PluginKind,
 )
 from .util import fetch_drs_to_file
@@ -19,59 +17,58 @@
 log = logging.getLogger(__name__)
 
 
-class DRSFilesSourceProperties(FilesSourceProperties, total=False):
-    url_regex: str
-    force_http: bool
-    http_headers: dict[str, str]
+class DRSFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    # `url_regex` is not templated because it needs to be set at initialization with no RuntimeContext available.
+    url_regex: str = r"^drs://"
+    force_http: Union[bool, TemplateExpansion] = False
+    http_headers: Union[dict[str, str], TemplateExpansion] = {}
+
 
+class DRSFileSourceConfiguration(BaseFileSourceConfiguration):
+    url_regex: str = r"^drs://"
+    force_http: bool = False
+    http_headers: dict[str, str] = {}
 
-class DRSFilesSource(BaseFilesSource):
+
+class DRSFilesSource(BaseFilesSource[DRSFileSourceTemplateConfiguration, DRSFileSourceConfiguration]):
     plugin_type = "drs"
     plugin_kind = PluginKind.drs
 
-    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
-        kwds: FilesSourceProperties = dict(
+    template_config_class = DRSFileSourceTemplateConfiguration
+    resolved_config_class = DRSFileSourceConfiguration
+
+    def __init__(self, template_config: DRSFileSourceTemplateConfiguration):
+        defaults = dict(
             id="_drs",
             label="DRS file",
             doc="DRS file handler",
             writable=False,
         )
-        kwds.update(kwd)
-        props: DRSFilesSourceProperties = cast(DRSFilesSourceProperties, self._parse_common_config_opts(kwds))
-        self._url_regex_str = props.pop("url_regex", r"^drs://")
-        assert self._url_regex_str
-        self._url_regex = re.compile(self._url_regex_str)
-        self._force_http = props.pop("force_http", False)
-        self._props = props
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
+        assert self.template_config.url_regex, "DRSFilesSource requires a url_regex to be set in the configuration"
+        self._url_regex = re.compile(self.template_config.url_regex)
 
     @property
     def _allowlist(self):
         return self._file_sources_config.fetch_url_allowlist
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[DRSFileSourceConfiguration]
     ):
-        props = self._serialization_props(user_context)
-        headers = props.pop("http_headers", {}) or {}
+        user_context = context.user_data.context if context.user_data.context else None
+        config = context.config
         fetch_drs_to_file(
             source_path,
             native_path,
-            user_context,
+            user_context=user_context,
             fetch_url_allowlist=self._allowlist,
-            headers=headers,
-            force_http=self._force_http,
+            headers=config.http_headers,
+            force_http=config.force_http,
         )
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[DRSFileSourceConfiguration]
     ):
         raise NotImplementedError()
 
@@ -81,13 +78,5 @@ def score_url_match(self, url: str):
         else:
             return 0
 
-    def _serialization_props(self, user_context: OptionalUserContext = None) -> DRSFilesSourceProperties:
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        effective_props["url_regex"] = self._url_regex_str
-        effective_props["force_http"] = self._force_http
-        return cast(DRSFilesSourceProperties, effective_props)
-
 
 __all__ = ("DRSFilesSource",)
diff --git a/lib/galaxy/files/sources/elabftw.py b/lib/galaxy/files/sources/elabftw.py
index ea0134d3881c..bef77b7568e5 100644
--- a/lib/galaxy/files/sources/elabftw.py
+++ b/lib/galaxy/files/sources/elabftw.py
@@ -58,13 +58,13 @@
 from textwrap import dedent
 from time import time
 from typing import (
-    Any,
     cast,
     Generic,
     get_type_hints,
     Literal,
     Optional,
     TypeVar,
+    Union,
 )
 from urllib.parse import (
     ParseResult,
@@ -77,22 +77,24 @@
 from typing_extensions import (
     NotRequired,
     TypedDict,
-    Unpack,
 )
 
 from galaxy import exceptions as galaxy_exceptions
-from galaxy.files import OptionalUserContext
-from galaxy.files.sources import (
+from galaxy.files.models import (
     AnyRemoteEntry,
-    BaseFilesSource,
-    DEFAULT_SCHEME,
-    FilesSourceOptions,
-    FilesSourceProperties,
-    PluginKind,
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
     RemoteDirectory,
     RemoteFile,
 )
+from galaxy.files.sources import (
+    BaseFilesSource,
+    PluginKind,
+)
+from galaxy.files.sources._defaults import DEFAULT_SCHEME
 from galaxy.util import requests
+from galaxy.util.config_templates import TemplateExpansion
 
 __all__ = ("eLabFTWFilesSource",)
 
@@ -148,17 +150,22 @@ def _get_part(self, part: Literal["entity_type", "entity_id", "attachment_id"])
         """
         Get the entity type, entity id or attachment id for the wrapped entry.
         """
-        path = self.entry["path"]
+        path = self.entry.path
         entity_type, entity_id, attachment_id = split_path(path)
         return locals()[part]
 
 
-class eLabFTWFilesSourceProperties(FilesSourceProperties, total=False):  # noqa
+class eLabFTWFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    endpoint: Union[str, TemplateExpansion]
+    api_key: Union[str, TemplateExpansion]
+
+
+class eLabFTWFileSourceConfiguration(BaseFileSourceConfiguration):
     endpoint: str
     api_key: str
 
 
-class eLabFTWFilesSource(BaseFilesSource):  # noqa
+class eLabFTWFilesSource(BaseFilesSource[eLabFTWFileSourceTemplateConfiguration, eLabFTWFileSourceConfiguration]):
 
     plugin_type = "elabftw"
     plugin_kind = PluginKind.rfs
@@ -168,17 +175,11 @@ class eLabFTWFilesSource(BaseFilesSource):  # noqa
     supports_search = True
     supports_sorting = True
 
-    def __init__(self, *args, **kwargs: Unpack[eLabFTWFilesSourceProperties]):
-        """Initialize the eLabFTW files source with an API key and an endpoint URL."""
-        super().__init__()
-        props = self._parse_common_config_opts(kwargs)
-        self._props = props
-
-        self._endpoint = kwargs["endpoint"]  # meant to be accessed only from `_get_endpoint()`
-        self._api_key = kwargs["api_key"]  # meant to be accessed only from `_create_session()`
+    template_config_class = eLabFTWFileSourceTemplateConfiguration
+    resolved_config_class = eLabFTWFileSourceConfiguration
 
-    def get_prefix(self, user_context: OptionalUserContext = None) -> Optional[str]:
-        endpoint: ParseResult = self._get_endpoint(user_context=user_context)
+    def get_prefix(self) -> Optional[str]:
+        endpoint: ParseResult = self._get_endpoint()
         return self.id if self.scheme not in {"elabftw", DEFAULT_SCHEME} else (endpoint.netloc or None)
         # it would make better sense to return
         # `self.id if self.scheme == USER_FILE_SOURCES_SCHEME else (endpoint.netloc or None)`, where
@@ -208,97 +209,50 @@ def to_relative_path(self, url: str) -> str:
             path = f"/{path}"
         return path
 
-    def _create_session(
-        self,
-        options: Optional[FilesSourceOptions] = None,
-        user_context: OptionalUserContext = None,
-    ) -> RequestsSession:
+    def _create_session(self, config: eLabFTWFileSourceConfiguration) -> RequestsSession:
         """
-        Create a Galaxy ``requests`` session, overriding initial settings via a :class:`FileSourceOptions` object.
+        Create a Galaxy ``requests`` session.
         """
         return requests.Session(
-            headers=self._get_session_headers(options=options, user_context=user_context),  # type: ignore[call-arg]
+            headers=self._get_session_headers(config),  # type: ignore[call-arg]
         )
 
-    def _create_session_async(
-        self,
-        options: Optional[FilesSourceOptions] = None,
-        user_context: OptionalUserContext = None,
-    ) -> aiohttp.ClientSession:
+    def _create_session_async(self, config: eLabFTWFileSourceConfiguration) -> aiohttp.ClientSession:
         """
-        Create an ``aiohttp`` session, overriding initial settings via a :class:`FileSourceOptions` object.
+        Create an ``aiohttp`` session.
         """
         connector = aiohttp.TCPConnector(limit=MAX_CONCURRENT_REQUESTS)
         return aiohttp.ClientSession(
             connector=connector,
             raise_for_status=True,
-            headers=self._get_session_headers(options=options, user_context=user_context),
+            headers=self._get_session_headers(config),
         )
 
-    def _get_session_headers(
-        self,
-        options: Optional[FilesSourceOptions] = None,
-        user_context: OptionalUserContext = None,
-    ) -> dict:
+    def _get_session_headers(self, config: eLabFTWFileSourceConfiguration) -> dict:
         """
         Construct a dictionary of HTTP client session headers.
 
-        Optionally, override initial settings via a :class:`FileSourceOptions` object and/or a
-        :class:`FileSourcesUserContext` object.
-
         Meant to be used only by `_create_session()` and `_create_session_async()`.
         """
-        props = {}
-        props.update(self._props)
-        props.update(options.extra_props if options and options.extra_props else {})
-        props.update(
-            {key: value for key, value in self._serialization_props(user_context).items() if value is not None}
-        )
+
         headers = {
-            "Authorization": props.get("api_key", self._api_key),
+            "Authorization": config.api_key,
             "Accept": "application/json",
         }
         return headers
 
-    def _get_endpoint(
-        self,
-        options: Optional[FilesSourceOptions] = None,
-        user_context: OptionalUserContext = None,
-    ) -> ParseResult:
+    def _get_endpoint(self) -> ParseResult:
         """
-        Retrieve the endpoint from the constructor, or override it via a :class:`FileSourceOptions` object.
+        Retrieve the endpoint from the constructor.
         """
-        props = {}
-        props.update(self._props)
-        props.update(options.extra_props if options and options.extra_props else {})
-        props.update(
-            {key: value for key, value in self._serialization_props(user_context).items() if value is not None}
-        )
-        endpoint = props.get("endpoint", self._endpoint)
-        # given that `options.extra_props` is of `eLabFTWFilesSourceProperties` type, it should be a string
-        endpoint = cast(str, endpoint)
-
-        return urlparse(endpoint)
-
-    def _serialization_props(self, user_context: OptionalUserContext = None) -> eLabFTWFilesSourceProperties:
-        effective_props: dict[str, Any] = {}
-
-        for key, val in self._props.items():
-            if key in {"api_key", "endpoint"} and user_context is None:
-                # prevent exception while expanding `${user.user_vault.read_secret('preferences/elabftw/api_key')}` or
-                # `${user.preferences['elabftw|endpoint']}` without `user_context`
-                effective_props[key] = None
-                continue
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-
-        return cast(eLabFTWFilesSourceProperties, effective_props)
+        return urlparse(self.template_config.endpoint)
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[eLabFTWFileSourceConfiguration],
         path="/",
         recursive=False,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -324,10 +278,9 @@ def _list(
             asyncio.set_event_loop(event_loop)
             return event_loop.run_until_complete(
                 self._list_async(
+                    context=context,
                     path=path,
                     recursive=recursive,
-                    user_context=user_context,
-                    opts=opts,
                     limit=limit,
                     offset=offset,
                     query=query,
@@ -339,10 +292,9 @@ def _list(
 
     async def _list_async(
         self,
+        context: FilesSourceRuntimeContext[eLabFTWFileSourceConfiguration],
         path="/",
         recursive=False,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -363,10 +315,6 @@ async def _list_async(
         :param recursive: List recursively, including all entity types for the root, all entities for each entity type
                           and all attachments for each entity.
         :type recursive: bool
-        :param user_context: Alter behavior using information from a user context (e.g. override the API key).
-        :type user_context: OptionalUserContext
-        :param opts: Alter behavior using information from a file source options object (e.g. ignore locked resources).
-        :type opts: Optional[FilesSourceOptions]
         :param limit: Show at most this amount of results, defaults to unlimited.
         :type limit: Optional[int]
         :param offset: Filter out this amount of results from the beginning of the sequence, defaults to zero.
@@ -383,8 +331,8 @@ async def _list_async(
         :raises InvalidPath: Path constraints described in the docstring of :class:`InvalidPath` are not satisfied.
         :raises ResourceNotFound: If the path refers to a non-existing experiment, resource, or attachment.
         """
-        session = self._create_session_async(options=opts, user_context=user_context)
-        endpoint = self._get_endpoint(options=opts, user_context=user_context)
+        session = self._create_session_async(context.config)
+        endpoint = self._get_endpoint()
 
         entity_type, entity_id, attachment_id = split_path(path)
 
@@ -415,7 +363,6 @@ async def collect_async_iterator(async_iter: AsyncIterator) -> list:
                             self._yield_entity_types(
                                 endpoint,
                                 session,
-                                user_context=user_context,
                             )
                         )
                     )
@@ -456,7 +403,6 @@ async def collect_async_iterator(async_iter: AsyncIterator) -> list:
                                     else None
                                 ),
                                 writable=self.writable,
-                                user_context=user_context,
                             )
                         )
                     )
@@ -487,7 +433,6 @@ async def collect_async_iterator(async_iter: AsyncIterator) -> list:
                                 cast(str, wrapped_entity.entity_id) if retrieve_entities else cast(str, entity_id),
                                 endpoint,
                                 session,
-                                user_context=user_context,
                             )
                         )
                     )
@@ -543,18 +488,16 @@ async def collect_async_iterator(async_iter: AsyncIterator) -> list:
             wrapped_entries,
             key=lambda x: (
                 (
-                    x.entry.get(sort_by, constructors[sort_by]())  # fall back to the default object for this key type
+                    getattr(x.entry, sort_by, constructors[sort_by]())  # fall back to the default object for this key type
                     if sort_by is not None else None  # fmt: skip
                 ),
-                x.entry["uri"],  # ensure deterministic ordering (URIs are unique)
+                x.entry.uri,  # ensure deterministic ordering (URIs are unique)
             ),
         )
 
         # filter out remaining items locally; by `query`, `offset` and `limit`
         if query is not None:
-            wrapped_entries = [
-                wrapped_entry for wrapped_entry in wrapped_entries if query in wrapped_entry.entry.get("name", "")
-            ]
+            wrapped_entries = [wrapped_entry for wrapped_entry in wrapped_entries if query in wrapped_entry.entry.name]
         if offset is not None:
             wrapped_entries = wrapped_entries[offset - (retrieve_entities_server_side_offset or 0) :]
         if limit is not None:
@@ -569,7 +512,6 @@ async def _yield_entity_types(
         self,
         endpoint: ParseResult,
         session: aiohttp.ClientSession,
-        user_context: OptionalUserContext = None,
     ) -> AsyncIterator[eLabFTWRemoteEntryWrapper[RemoteDirectory]]:
         """
         List the root directory, i.e. "/".
@@ -598,22 +540,16 @@ async def _yield_entity_types(
 
         experiments = eLabFTWRemoteEntryWrapper(
             RemoteDirectory(
-                **{
-                    "name": "Experiments",
-                    "uri": f"{self.get_scheme()}://{self.get_prefix(user_context=user_context)}/experiments",
-                    "path": "/experiments",
-                    "class": "Directory",
-                }
+                name="Experiments",
+                uri=f"{self.get_scheme()}://{self.get_prefix()}/experiments",
+                path="/experiments",
             )
         )
         resources = eLabFTWRemoteEntryWrapper(
             RemoteDirectory(
-                **{
-                    "name": "Resources",
-                    "uri": f"{self.get_scheme()}://{self.get_prefix(user_context=user_context)}/resources",
-                    "path": "/resources",
-                    "class": "Directory",
-                }
+                name="Resources",
+                uri=f"{self.get_scheme()}://{self.get_prefix()}/resources",
+                path="/resources",
             )
         )
 
@@ -630,7 +566,6 @@ async def _yield_entities(
         query: Optional[str] = None,
         order: Optional[str] = None,
         writable: bool = False,
-        user_context: OptionalUserContext = None,
     ) -> AsyncIterator[eLabFTWRemoteEntryWrapper[RemoteDirectory]]:
         """List an entity type, i.e. either "/experiments" or "/resources"."""
         url = urljoin(
@@ -697,10 +632,7 @@ def validate_and_register_entity(item, mapping: dict[int, dict]) -> Literal[True
                     RemoteDirectory(
                         **{
                             "name": entity["title"],
-                            "uri": (
-                                f"{self.get_scheme()}://{self.get_prefix(user_context=user_context)}"
-                                f"/{entity_type}/{entity['id']}"
-                            ),
+                            "uri": f"{self.get_scheme()}://{self.get_prefix()}/{entity_type}/{entity['id']}",
                             "path": f"/{entity_type}/{entity['id']}",
                             "class": "Directory",
                         }
@@ -719,7 +651,6 @@ async def _yield_attachments(
         entity_id: str,
         endpoint: ParseResult,
         session: aiohttp.ClientSession,
-        user_context: OptionalUserContext = None,
     ) -> AsyncIterator[eLabFTWRemoteEntryWrapper[RemoteFile]]:
         """List attachments of a specific entity, e.g. "/resources/48"."""
         url = urljoin(
@@ -754,10 +685,7 @@ async def _yield_attachments(
                 RemoteFile(
                     **{
                         "name": upload["real_name"],
-                        "uri": (
-                            f"{self.get_scheme()}://{self.get_prefix(user_context=user_context)}"
-                            f"/{entity_type}/{entity_id}/{upload['id']}"
-                        ),
+                        "uri": f"{self.get_scheme()}://{self.get_prefix()}/{entity_type}/{entity_id}/{upload['id']}",
                         "path": f"/{entity_type}/{entity_id}/{upload['id']}",
                         "class": "File",
                         "size": upload["filesize"],
@@ -768,11 +696,7 @@ async def _yield_attachments(
             )
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[eLabFTWFileSourceConfiguration]
     ) -> str:
         """
         Attach the file located at ``native_path`` on the filesystem to an eLabFTW resource or experiment with URI
@@ -783,10 +707,6 @@ def _write_from(
         :type target_path: str
         :param native_path: The local file to upload, e.g. ``/tmp/myfile.txt``
         :type native_path: str
-        :param user_context: A user context, defaults to ``None``
-        :type user_context: OptionalUserContext
-        :param opts: A set of options to exercise additional control over this method. Defaults to ``None``
-        :type opts: Optional[FilesSourceOptions], optional
         :return: Path *assigned by eLabFTW* to the uploaded file.
         :rtype: str
 
@@ -800,8 +720,8 @@ def _write_from(
                              three components.
         :raises EntityExpected: When attempting to attach the file to the root "/" or an entity type.
         """
-        session = self._create_session(options=opts, user_context=user_context)
-        endpoint = self._get_endpoint(options=opts, user_context=user_context)
+        session = self._create_session(context.config)
+        endpoint = self._get_endpoint()
 
         target_path_obj = Path(target_path)
         attachment_name = target_path_obj.name
@@ -852,11 +772,7 @@ def _write_from(
         return f"/{entity_type}/{entity_id}/{attachment_id}"
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[eLabFTWFileSourceConfiguration]
     ):
         """
         Save the file attachment from an eLabFTW resource or experiment located at ``source_path`` to ``native_path``.
@@ -865,16 +781,13 @@ def _realize_to(
         :type source_path: str
         :param native_path: The path on the filesystem to save the file to, e.g. ``/tmp/myfile.txt``
         :type native_path: str
-        :param user_context: A user context, defaults to ``None``
-        :type user_context: OptionalUserContext
-        :param opts: A set of options to exercise additional control over this method. Defaults to ``None``
 
         :raises requests.RequestException: When there is a connection error.
         :raises ValidationError: If the HTTP response from the eLabFTW server is invalid.
         :raises AttachmentExpected: When referencing an entity type, an entity or the root rather than an attachment.
         """
-        session = self._create_session(options=opts, user_context=user_context)
-        endpoint = self._get_endpoint(options=opts, user_context=user_context)
+        session = self._create_session(context.config)
+        endpoint = self._get_endpoint()
 
         entity_type, entity_id, attachment_id = split_path(source_path)
         if not all((entity_type, entity_id, attachment_id)):
diff --git a/lib/galaxy/files/sources/ftp.py b/lib/galaxy/files/sources/ftp.py
index 05658e925ac0..00d0cd5a2a4a 100644
--- a/lib/galaxy/files/sources/ftp.py
+++ b/lib/galaxy/files/sources/ftp.py
@@ -1,90 +1,93 @@
 import urllib.parse
-
-from galaxy.files import OptionalUserContext
+from typing import Union
 
 try:
     from fs.ftpfs import FTPFS
 except ImportError:
     FTPFS = None  # type: ignore[misc,assignment]
 
-from typing import (
-    cast,
-    Optional,
-)
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class FTPFilesSourceProperties(FilesSourceProperties, total=False):
-    host: str
-    port: int
-    user: str
-    passwd: str
+class FTPFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    host: Union[str, TemplateExpansion] = ""
+    port: Union[int, TemplateExpansion] = 21
+    user: Union[str, TemplateExpansion] = "anonymous"
+    passwd: Union[str, TemplateExpansion] = ""
+    acct: Union[str, TemplateExpansion] = ""
+    timeout: Union[int, TemplateExpansion] = 10
+    proxy: Union[str, TemplateExpansion, None] = None
+    tls: Union[bool, TemplateExpansion] = False
+
 
+class FTPFileSourceConfiguration(BaseFileSourceConfiguration):
+    host: str = ""
+    port: int = 21
+    user: str = "anonymous"
+    passwd: str = ""
+    acct: str = ""
+    timeout: int = 10
+    proxy: Union[str, None] = None
+    tls: bool = False
 
-class FtpFilesSource(PyFilesystem2FilesSource):
+
+class FtpFilesSource(PyFilesystem2FilesSource[FTPFileSourceTemplateConfiguration, FTPFileSourceConfiguration]):
     plugin_type = "ftp"
     required_module = FTPFS
     required_package = "fs.ftpfs"
 
-    def _open_fs(self, user_context: OptionalUserContext = None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: FTPFilesSourceProperties = cast(FTPFilesSourceProperties, opts.extra_props or {} if opts else {})
-        handle = FTPFS(**{**props, **extra_props})
-        return handle
+    template_config_class = FTPFileSourceTemplateConfiguration
+    resolved_config_class = FTPFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[FTPFileSourceConfiguration]):
+        if FTPFS is None:
+            raise self.required_package_exception
+
+        config = context.config
+        return FTPFS(
+            host=config.host,
+            port=config.port,
+            user=config.user,
+            passwd=config.passwd,
+            timeout=config.timeout,
+            acct=config.acct,
+            tls=config.tls,
+            proxy=config.proxy,
+        )
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[FTPFileSourceConfiguration]
     ):
-        extra_props: FTPFilesSourceProperties
-        if opts and opts.extra_props:
-            extra_props = cast(FTPFilesSourceProperties, opts.extra_props)
-        else:
-            opts = FilesSourceOptions()
-            extra_props = {}
-        path, opts.extra_props = self._get_props_and_rel_path(extra_props, source_path)
-        super()._realize_to(path, native_path, user_context=user_context, opts=opts)
+        path = self._parse_url_and_get_path(source_path, context.config)
+        super()._realize_to(path, native_path, context)
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[FTPFileSourceConfiguration]
     ):
-        extra_props: FTPFilesSourceProperties
-        if opts and opts.extra_props:
-            extra_props = cast(FTPFilesSourceProperties, opts.extra_props)
-        else:
-            opts = FilesSourceOptions()
-            extra_props = {}
-        path, opts.extra_props = self._get_props_and_rel_path(extra_props, target_path)
-        super()._write_from(path, native_path, user_context=user_context, opts=opts)
-
-    def _get_props_and_rel_path(
-        self, extra_props: FTPFilesSourceProperties, url: str
-    ) -> tuple[str, FTPFilesSourceProperties]:
-        host = self._props.get("host")
-        port = self._props.get("port")
-        user = self._props.get("user")
-        passwd = self._props.get("passwd")
+        path = self._parse_url_and_get_path(target_path, context.config)
+        super()._write_from(path, native_path, context)
+
+    def _parse_url_and_get_path(self, url: str, config: FTPFileSourceConfiguration) -> str:
+        host = config.host
+        port = config.port
+        user = config.user
+        passwd = config.passwd
         rel_path = url
         if url.startswith(f"ftp://{host or ''}"):
             props = self._extract_url_props(url)
-            extra_props["host"] = host or props["host"]
-            extra_props["port"] = port or props["port"]
-            extra_props["user"] = user or props["user"]
-            extra_props["passwd"] = passwd or props["passwd"]
+            config.host = host or props["host"]
+            config.port = port or props["port"]
+            config.user = user or props["user"]
+            config.passwd = passwd or props["passwd"]
             rel_path = props["path"] or url
-        return rel_path, extra_props
+        return rel_path
 
     def _extract_url_props(self, url: str):
         result = urllib.parse.urlparse(url)
@@ -97,8 +100,9 @@ def _extract_url_props(self, url: str):
         }
 
     def score_url_match(self, url: str):
-        host = self._props.get("host")
-        port = self._props.get("port")
+        # We need to use template_config here because this is called before the template is expanded.
+        host = self.template_config.host
+        port = self.template_config.port
         if host and port and url.startswith(f"ftp://{host}:{port}"):
             return len(f"ftp://{host}:{port}")
         # For security, we need to ensure that a partial match doesn't work e.g. ftp://{host}something/myfiles
diff --git a/lib/galaxy/files/sources/galaxy.py b/lib/galaxy/files/sources/galaxy.py
index 141e92202a22..1e3548cd8b87 100644
--- a/lib/galaxy/files/sources/galaxy.py
+++ b/lib/galaxy/files/sources/galaxy.py
@@ -1,15 +1,11 @@
 """Static Galaxy file sources - ftp and libraries."""
 
-from typing import (
-    Optional,
-)
-
-from typing_extensions import Unpack
+from typing import Optional
 
 from galaxy.files.sources import PluginKind
 from .posix import (
     PosixFilesSource,
-    PosixFilesSourceProperties,
+    PosixTemplateConfiguration,
 )
 
 
@@ -17,19 +13,19 @@ class UserFtpFilesSource(PosixFilesSource):
     plugin_type = "gxftp"
     plugin_kind = PluginKind.stock
 
-    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
-        posix_kwds: PosixFilesSourceProperties = dict(
+    def __init__(self, template_config: PosixTemplateConfiguration):
+        defaults = dict(
             id="_ftp",
             root="${user.ftp_dir}",
             label="FTP Directory",
             doc="Galaxy User's FTP Directory",
             writable=True,
         )
-        posix_kwds.update(kwd)
-        if "delete_on_realize" not in posix_kwds:
-            file_sources_config = kwd["file_sources_config"]
-            posix_kwds["delete_on_realize"] = file_sources_config.ftp_upload_purge
-        super().__init__(**posix_kwds)
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
+        if not self.template_config.delete_on_realize:
+            # If delete_on_realize is not set, use the default from the file sources config.
+            self.template_config.delete_on_realize = self.template_config.file_sources_config.ftp_upload_purge
 
     def get_prefix(self) -> Optional[str]:
         return None
@@ -42,15 +38,15 @@ class LibraryImportFilesSource(PosixFilesSource):
     plugin_type = "gximport"
     plugin_kind = PluginKind.stock
 
-    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
-        posix_kwds: PosixFilesSourceProperties = dict(
+    def __init__(self, template_config: PosixTemplateConfiguration):
+        defaults = dict(
             id="_import",
             root="${config.library_import_dir}",
             label="Library Import Directory",
             doc="Galaxy's library import directory",
         )
-        posix_kwds.update(kwd)
-        super().__init__(**posix_kwds)
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
 
     def get_prefix(self) -> Optional[str]:
         return None
@@ -63,15 +59,15 @@ class UserLibraryImportFilesSource(PosixFilesSource):
     plugin_type = "gxuserimport"
     plugin_kind = PluginKind.stock
 
-    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
-        posix_kwds: PosixFilesSourceProperties = dict(
+    def __init__(self, template_config: PosixTemplateConfiguration):
+        defaults = dict(
             id="_userimport",
             root="${config.user_library_import_dir}/${user.email}",
             label="Library User Import Directory",
             doc="Galaxy's user library import directory",
         )
-        posix_kwds.update(kwd)
-        super().__init__(**posix_kwds)
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
 
     def get_prefix(self) -> Optional[str]:
         return None
diff --git a/lib/galaxy/files/sources/googlecloudstorage.py b/lib/galaxy/files/sources/googlecloudstorage.py
index 9eaaa0c79adf..2f0c64d56a52 100644
--- a/lib/galaxy/files/sources/googlecloudstorage.py
+++ b/lib/galaxy/files/sources/googlecloudstorage.py
@@ -6,43 +6,75 @@
     GCSFS = None
 
 from typing import (
-    cast,
     Optional,
+    Union,
 )
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class GoogleCloudStorageFilesSourceProperties(FilesSourceProperties, total=False):
+class GoogleCloudStorageFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    bucket_name: Union[str, TemplateExpansion]
+    root_path: Union[str, TemplateExpansion, None] = None
+    project: Union[str, TemplateExpansion, None] = None
+    anonymous: Union[bool, TemplateExpansion, None] = True
+    token: Union[str, TemplateExpansion, None] = None
+    token_uri: Union[str, TemplateExpansion, None] = None
+    client_id: Union[str, TemplateExpansion, None] = None
+    client_secret: Union[str, TemplateExpansion, None] = None
+    refresh_token: Union[str, TemplateExpansion, None] = None
+
+
+class GoogleCloudStorageFileSourceConfiguration(BaseFileSourceConfiguration):
     bucket_name: str
-    root_path: str
-    project: str
-    anonymous: bool
+    root_path: Optional[str] = None
+    project: Optional[str] = None
+    anonymous: Optional[bool] = True
+    token: Optional[str] = None
+    token_uri: Optional[str] = None
+    client_id: Optional[str] = None
+    client_secret: Optional[str] = None
+    refresh_token: Optional[str] = None
 
 
-class GoogleCloudStorageFilesSource(PyFilesystem2FilesSource):
+class GoogleCloudStorageFilesSource(
+    PyFilesystem2FilesSource[
+        GoogleCloudStorageFileSourceTemplateConfiguration, GoogleCloudStorageFileSourceConfiguration
+    ]
+):
     plugin_type = "googlecloudstorage"
     required_module = GCSFS
     required_package = "fs-gcsfs"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: GoogleCloudStorageFilesSourceProperties = cast(
-            GoogleCloudStorageFilesSourceProperties, opts.extra_props or {} if opts else {}
-        )
-        bucket_name = props.pop("bucket_name", None)
-        root_path = props.pop("root_path", None)
-        project = props.pop("project", None)
-        args = {}
-        if props.get("anonymous"):
-            args["client"] = Client.create_anonymous_client()
-        elif props.get("token"):
-            args["client"] = Client(project=project, credentials=Credentials(**props))
-        handle = GCSFS(bucket_name, root_path=root_path, retry=0, **{**args, **extra_props})
+    template_config_class = GoogleCloudStorageFileSourceTemplateConfiguration
+    resolved_config_class = GoogleCloudStorageFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[GoogleCloudStorageFileSourceConfiguration]):
+        if GCSFS is None:
+            raise self.required_package_exception
+
+        config = context.config
+        if config.anonymous:
+            client = Client.create_anonymous_client()
+        elif config.token:
+            client = Client(
+                project=config.project,
+                credentials=Credentials(
+                    token=config.token,
+                    token_uri=config.token_uri,
+                    client_id=config.client_id,
+                    client_secret=config.client_secret,
+                    refresh_token=config.refresh_token,
+                ),
+            )
+
+        handle = GCSFS(bucket_name=config.bucket_name, root_path=config.root_path or "", retry=0, client=client)
         return handle
 
 
diff --git a/lib/galaxy/files/sources/googledrive.py b/lib/galaxy/files/sources/googledrive.py
index 4f32803ea70b..234c7d0e701f 100644
--- a/lib/galaxy/files/sources/googledrive.py
+++ b/lib/galaxy/files/sources/googledrive.py
@@ -1,25 +1,56 @@
 try:
-    from fs.googledrivefs import GoogleDriveFS
+    from fs.googledrivefs.googledrivefs import GoogleDriveFS
     from google.oauth2.credentials import Credentials
 except ImportError:
     GoogleDriveFS = None
 
-from typing import Optional
 
-from . import FilesSourceOptions
+from typing import (
+    Annotated,
+    Union,
+)
+
+from pydantic import (
+    AliasChoices,
+    Field,
+)
+
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
+AccessTokenField = Field(
+    ...,
+    validation_alias=AliasChoices("oauth2_access_token", "accessToken", "access_token"),
+)
+
+
+class GoogleDriveFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    access_token: Annotated[Union[str, TemplateExpansion], AccessTokenField]
 
-class GoogleDriveFilesSource(PyFilesystem2FilesSource):
+
+class GoogleDriveFilesSourceConfiguration(BaseFileSourceConfiguration):
+    access_token: Annotated[str, AccessTokenField]
+
+
+class GoogleDriveFilesSource(
+    PyFilesystem2FilesSource[GoogleDriveFileSourceTemplateConfiguration, GoogleDriveFilesSourceConfiguration]
+):
     plugin_type = "googledrive"
     required_module = GoogleDriveFS
     required_package = "fs.googledrivefs"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        if access_token := props.pop("oauth2_access_token"):
-            props["token"] = access_token
-        credentials = Credentials(**props)
+    template_config_class = GoogleDriveFileSourceTemplateConfiguration
+    resolved_config_class = GoogleDriveFilesSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[GoogleDriveFilesSourceConfiguration]):
+        if GoogleDriveFS is None:
+            raise self.required_package_exception
+        credentials = Credentials(token=context.config.access_token)
         handle = GoogleDriveFS(credentials)
         return handle
 
diff --git a/lib/galaxy/files/sources/http.py b/lib/galaxy/files/sources/http.py
index fb564cb7b74d..30462ad11150 100644
--- a/lib/galaxy/files/sources/http.py
+++ b/lib/galaxy/files/sources/http.py
@@ -1,14 +1,13 @@
 import logging
 import re
 import urllib.request
-from typing import (
-    cast,
-    Optional,
-)
-
-from typing_extensions import Unpack
+from typing import Union
 
-from galaxy.files import OptionalUserContext
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
 from galaxy.files.uris import validate_non_local
 from galaxy.util import (
     DEFAULT_SOCKET_TIMEOUT,
@@ -16,83 +15,72 @@
     stream_to_open_named_file,
 )
 from galaxy.util.config_parsers import IpAllowedListEntryT
+from galaxy.util.config_templates import TemplateExpansion
 from . import (
     BaseFilesSource,
-    FilesSourceOptions,
-    FilesSourceProperties,
     PluginKind,
 )
 
 log = logging.getLogger(__name__)
 
 
-class HTTPFilesSourceProperties(FilesSourceProperties, total=False):
-    url_regex: str
-    http_headers: dict[str, str]
-    fetch_url_allowlist: list[IpAllowedListEntryT]
+class HTTPFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    # `url_regex` is not templated because it needs to be set at initialization with no RuntimeContext available.
+    url_regex: str = r"^https?://|^ftp://"
+    http_headers: Union[dict[str, str], TemplateExpansion] = {}
+    fetch_url_allowlist: Union[list[IpAllowedListEntryT], TemplateExpansion] = []
+
 
+class HTTPFileSourceConfiguration(BaseFileSourceConfiguration):
+    url_regex: str = r"^https?://|^ftp://"
+    http_headers: dict[str, str] = {}
+    fetch_url_allowlist: list[IpAllowedListEntryT] = []
 
-class HTTPFilesSource(BaseFilesSource):
+
+class HTTPFilesSource(BaseFilesSource[HTTPFileSourceTemplateConfiguration, HTTPFileSourceConfiguration]):
     plugin_type = "http"
     plugin_kind = PluginKind.stock
 
-    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
-        kwds: FilesSourceProperties = dict(
+    template_config_class = HTTPFileSourceTemplateConfiguration
+    resolved_config_class = HTTPFileSourceConfiguration
+
+    def __init__(self, template_config: HTTPFileSourceTemplateConfiguration):
+        defaults = dict(
             id="_http",
             label="HTTP File",
             doc="Default HTTP file handler",
             writable=False,
         )
-        kwds.update(kwd)
-        props: HTTPFilesSourceProperties = cast(HTTPFilesSourceProperties, self._parse_common_config_opts(kwds))
-        self._url_regex_str = props.pop("url_regex", r"^https?://|^ftp://")
-        assert self._url_regex_str
-        self._url_regex = re.compile(self._url_regex_str)
-        self._props = props
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
+        assert self.template_config.url_regex, "HTTPFilesSource requires a url_regex to be set in the configuration"
+        self._compiled_url_regex = re.compile(self.template_config.url_regex)
 
     @property
     def _allowlist(self):
         return self._file_sources_config.fetch_url_allowlist
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[HTTPFileSourceConfiguration]
     ):
-        props = self._serialization_props(user_context)
-        extra_props: HTTPFilesSourceProperties = cast(HTTPFilesSourceProperties, opts.extra_props or {} if opts else {})
-        headers = props.pop("http_headers", {}) or {}
-        headers.update(extra_props.get("http_headers") or {})
-        req = urllib.request.Request(source_path, headers=headers)
+        config = context.config
+        req = urllib.request.Request(source_path, headers=config.http_headers)
 
         with urllib.request.urlopen(req, timeout=DEFAULT_SOCKET_TIMEOUT) as page:
             # Verify url post-redirects is still allowlisted
-            validate_non_local(page.geturl(), self._allowlist or extra_props.get("fetch_url_allowlist") or [])
+            validate_non_local(page.geturl(), self._allowlist or config.fetch_url_allowlist)
             f = open(native_path, "wb")  # fd will be .close()ed in stream_to_open_named_file
             return stream_to_open_named_file(
                 page, f.fileno(), native_path, source_encoding=get_charset_from_http_headers(page.headers)
             )
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[HTTPFileSourceConfiguration]
     ):
         raise NotImplementedError()
 
-    def _serialization_props(self, user_context: OptionalUserContext = None) -> HTTPFilesSourceProperties:
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        effective_props["url_regex"] = self._url_regex_str
-        return cast(HTTPFilesSourceProperties, effective_props)
-
     def score_url_match(self, url: str):
-        if match := self._url_regex.match(url):
+        if match := self._compiled_url_regex.match(url):
             return match.span()[1]
         else:
             return 0
diff --git a/lib/galaxy/files/sources/invenio.py b/lib/galaxy/files/sources/invenio.py
index 1b75dccc044b..09b719c43a9b 100644
--- a/lib/galaxy/files/sources/invenio.py
+++ b/lib/galaxy/files/sources/invenio.py
@@ -7,33 +7,33 @@
     cast,
     Optional,
 )
+from urllib.error import HTTPError
 from urllib.parse import quote
 
 from typing_extensions import (
     Literal,
     TypedDict,
-    Unpack,
 )
 
 from galaxy.exceptions import (
     AuthenticationRequired,
     MessageException,
 )
-from galaxy.files import OptionalUserContext
-from galaxy.files.sources import (
+from galaxy.files.models import (
     AnyRemoteEntry,
-    DEFAULT_PAGE_LIMIT,
-    DEFAULT_SCHEME,
     Entry,
     EntryData,
-    FilesSourceOptions,
+    FilesSourceRuntimeContext,
     RemoteDirectory,
     RemoteFile,
 )
+from galaxy.files.sources import DEFAULT_PAGE_LIMIT
+from galaxy.files.sources._defaults import DEFAULT_SCHEME
 from galaxy.files.sources._rdm import (
     ContainerAndFileIdentifier,
+    RDMFileSourceConfiguration,
+    RDMFileSourceTemplateConfiguration,
     RDMFilesSource,
-    RDMFilesSourceProperties,
     RDMRepositoryInteractor,
 )
 from galaxy.util import (
@@ -128,8 +128,8 @@ class InvenioRDMFilesSource(RDMFilesSource):
     supports_search = True
     rdm_scheme = "invenio"
 
-    def __init__(self, **kwd: Unpack[RDMFilesSourceProperties]):
-        super().__init__(**kwd)
+    def __init__(self, template_config: RDMFileSourceTemplateConfiguration):
+        super().__init__(template_config)
         self._scheme_regex = re.compile(rf"^{self.get_scheme()}?://{self.id}|^{DEFAULT_SCHEME}://{self.id}")
         self.repository: InvenioRepositoryInteractor
 
@@ -184,34 +184,30 @@ def get_container_id_from_path(self, source_path: str) -> str:
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         path="/",
-        recursive=True,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        recursive=False,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
         sort_by: Optional[str] = None,
     ) -> tuple[list[AnyRemoteEntry], int]:
-        writeable = opts and opts.writeable or False
         is_root_path = path == "/"
         if is_root_path:
             records, total_hits = self.repository.get_file_containers(
-                writeable, user_context, limit=limit, offset=offset, query=query
+                context, write_intent, limit=limit, offset=offset, query=query
             )
             return cast(list[AnyRemoteEntry], records), total_hits
         record_id = self.get_container_id_from_path(path)
-        files = self.repository.get_files_in_container(record_id, writeable, user_context)
+        files = self.repository.get_files_in_container(context, record_id, write_intent, query)
         return cast(list[AnyRemoteEntry], files), len(files)
 
     def _create_entry(
-        self,
-        entry_data: EntryData,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, entry_data: EntryData, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ) -> Entry:
-        public_name = self.get_public_name(user_context) or "No name"
-        record = self.repository.create_draft_file_container(entry_data["name"], public_name, user_context=user_context)
+        public_name = self.get_public_name(context)
+        record = self.repository.create_draft_file_container(entry_data.name, public_name, context)
         record_id = record.get("id")
         record_id = str(record_id) if record_id else None
         if not record_id:
@@ -226,33 +222,23 @@ def _create_entry(
         external_link = links.get("self_html")
         if not external_link or not isinstance(external_link, str):
             raise Exception("Failed to get record link.")
-        return {
-            "uri": uri,
-            "name": name,
-            "external_link": external_link,
-        }
+        return Entry(
+            name=name,
+            uri=uri,
+            external_link=external_link,
+        )
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ):
-        # TODO: user_context is always None here when called from a data fetch.
-        # This prevents downloading files that require authentication even if the user provided a token.
         record_id, filename = self.parse_path(source_path)
-        self.repository.download_file_from_container(record_id, filename, native_path, user_context=user_context)
+        self.repository.download_file_from_container(record_id, filename, native_path, context)
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
     ):
         record_id, filename = self.parse_path(target_path)
-        self.repository.upload_file_to_draft_container(record_id, filename, native_path, user_context=user_context)
+        self.repository.upload_file_to_draft_container(record_id, filename, native_path, context)
 
 
 class InvenioRepositoryInteractor(RDMRepositoryInteractor):
@@ -271,8 +257,8 @@ def to_plugin_uri(self, record_id: str, filename: Optional[str] = None) -> str:
 
     def get_file_containers(
         self,
-        writeable: bool,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
+        write_intent: bool,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -281,7 +267,7 @@ def get_file_containers(
         """Gets the records in the repository and returns the total count of records."""
         params: dict[str, Any] = {}
         request_url = self.records_url
-        if writeable:
+        if write_intent:
             # Only draft records owned by the user can be written to.
             params["is_published"] = "false"
             request_url = self.user_records_url
@@ -291,7 +277,7 @@ def get_file_containers(
         if query:
             params["q"] = query
             params["sort"] = "bestmatch"
-        response_data = self._get_response(user_context, request_url, params=params)
+        response_data = self._get_response(context, request_url, params=params)
         total_hits = response_data["hits"]["total"]
         return self._get_records_from_response(response_data), total_hits
 
@@ -304,19 +290,19 @@ def _to_size_page(self, limit: Optional[int], offset: Optional[int]) -> tuple[Op
 
     def get_files_in_container(
         self,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         container_id: str,
         writeable: bool,
-        user_context: OptionalUserContext = None,
         query: Optional[str] = None,
     ) -> list[RemoteFile]:
         conditionally_draft = "/draft" if writeable else ""
         request_url = f"{self.records_url}/{container_id}{conditionally_draft}/files"
-        response_data = self._get_response(user_context, request_url)
+        response_data = self._get_response(context, request_url)
         return self._get_record_files_from_response(container_id, response_data)
 
     def create_draft_file_container(
-        self, title: str, public_name: str, user_context: OptionalUserContext = None
-    ) -> RemoteDirectory:
+        self, title: str, public_name: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
+    ) -> dict[str, Any]:
         today = datetime.date.today().isoformat()
         creator = self._get_creator_from_public_name(public_name)
         create_record_request = {
@@ -331,7 +317,7 @@ def create_draft_file_container(
             },
         }
 
-        headers = self._get_request_headers(user_context, auth_required=True)
+        headers = self._get_request_headers(context, auth_required=True)
         response = requests.post(self.records_url, json=create_record_request, headers=headers)
         self._ensure_response_has_expected_status_code(response, 201)
         record = response.json()
@@ -343,11 +329,11 @@ def upload_file_to_draft_container(
         record_id: str,
         filename: str,
         file_path: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ):
-        record = self._get_draft_record(record_id, user_context=user_context)
+        record = self._get_draft_record(record_id, context)
         upload_file_url = record["links"]["files"]
-        headers = self._get_request_headers(user_context, auth_required=True)
+        headers = self._get_request_headers(context, auth_required=True)
 
         # Add file metadata entry
         response = requests.post(upload_file_url, json=[{"key": filename}], headers=headers)
@@ -371,13 +357,13 @@ def download_file_from_container(
         container_id: str,
         file_identifier: str,
         file_path: str,
-        user_context: OptionalUserContext = None,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
     ):
-        download_file_content_url = self._get_download_file_url(container_id, file_identifier, user_context)
+        download_file_content_url = self._get_download_file_url(container_id, file_identifier, context)
         headers = {}
         if self._is_api_url(download_file_content_url):
             # pass the token as a header only when using the API
-            headers = self._get_request_headers(user_context)
+            headers = self._get_request_headers(context)
         try:
             req = urllib.request.Request(download_file_content_url, headers=headers)
             with urllib.request.urlopen(req, timeout=DEFAULT_SOCKET_TIMEOUT) as page:
@@ -385,22 +371,24 @@ def download_file_from_container(
                 return stream_to_open_named_file(
                     page, f.fileno(), file_path, source_encoding=get_charset_from_http_headers(page.headers)
                 )
-        except urllib.error.HTTPError as e:
+        except HTTPError as e:
             # TODO: We can only download files from published records for now
             if e.code in [401, 403, 404]:
                 raise Exception(
                     f"Cannot download file '{file_identifier}' from record '{container_id}'. Please make sure the record exists and it is public."
                 )
 
-    def _get_download_file_url(self, record_id: str, filename: str, user_context: OptionalUserContext = None):
+    def _get_download_file_url(
+        self, record_id: str, filename: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]
+    ):
         """Get the URL to download a file from a record.
 
         This method is used to download files from both published and draft records that are accessible by the user.
         """
         file_details_url = self._get_file_details_url(record_id, filename)
-        if self._is_published_record(record_id, user_context):
+        if self._is_published_record(record_id, context):
             return self._file_url_to_download_url(file_details_url)
-        if self._is_draft_record(record_id, user_context):
+        if self._is_draft_record(record_id, context):
             draft_download_url = f"{self._to_draft_url(file_details_url)}/content"
             return draft_download_url
         raise MessageException(
@@ -413,15 +401,15 @@ def _is_api_url(self, url: str) -> bool:
     def _to_draft_url(self, url: str) -> str:
         return url.replace("/files/", "/draft/files/")
 
-    def _is_draft_record(self, record_id: str, user_context: OptionalUserContext = None):
+    def _is_draft_record(self, record_id: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]):
         request_url = self._get_draft_record_url(record_id)
-        headers = self._get_request_headers(user_context)
+        headers = self._get_request_headers(context)
         response = requests.head(request_url, headers=headers)
         return response.status_code == 200
 
-    def _is_published_record(self, record_id: str, user_context: OptionalUserContext = None):
+    def _is_published_record(self, record_id: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]):
         request_url = self._get_record_url(record_id)
-        headers = self._get_request_headers(user_context)
+        headers = self._get_request_headers(context)
         response = requests.head(request_url, headers=headers)
         return response.status_code == 200
 
@@ -440,9 +428,9 @@ def _file_url_to_download_url(self, file_url: str) -> str:
         # So this is the most reliable way to download files for now.
         return f"{file_url.replace('/api', '')}?download=1"
 
-    def _get_draft_record(self, record_id: str, user_context: OptionalUserContext = None):
+    def _get_draft_record(self, record_id: str, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration]):
         request_url = self._get_draft_record_url(record_id)
-        draft_record = self._get_response(user_context, request_url)
+        draft_record = self._get_response(context, request_url)
         return draft_record
 
     def _get_records_from_response(self, response: dict) -> list[RemoteDirectory]:
@@ -453,12 +441,11 @@ def _get_records_from_response(self, response: dict) -> list[RemoteDirectory]:
             path = self.plugin.to_relative_path(uri)
             name = self._get_record_title(record)
             rval.append(
-                {
-                    "class": "Directory",
-                    "name": name,
-                    "uri": uri,
-                    "path": path,
-                }
+                RemoteDirectory(
+                    name=name,
+                    uri=uri,
+                    path=path,
+                )
             )
         return rval
 
@@ -479,14 +466,13 @@ def _get_record_files_from_response(self, record_id: str, response: dict) -> lis
                 uri = self.to_plugin_uri(record_id=record_id, filename=entry["key"])
                 path = self.plugin.to_relative_path(uri)
                 rval.append(
-                    {
-                        "class": "File",
-                        "name": entry["key"],
-                        "size": entry["size"],
-                        "ctime": entry["created"],
-                        "uri": uri,
-                        "path": path,
-                    }
+                    RemoteFile(
+                        name=entry["key"],
+                        size=entry["size"],
+                        ctime=entry["created"],
+                        uri=uri,
+                        path=path,
+                    )
                 )
         return rval
 
@@ -513,18 +499,20 @@ def _get_creator_from_public_name(self, public_name: Optional[str] = None) -> Cr
 
     def _get_response(
         self,
-        user_context: OptionalUserContext,
+        context: FilesSourceRuntimeContext[RDMFileSourceConfiguration],
         request_url: str,
         params: Optional[dict[str, Any]] = None,
         auth_required: bool = False,
     ) -> dict:
-        headers = self._get_request_headers(user_context, auth_required)
+        headers = self._get_request_headers(context, auth_required)
         response = requests.get(request_url, params=params, headers=headers)
         self._ensure_response_has_expected_status_code(response, 200)
         return response.json()
 
-    def _get_request_headers(self, user_context: OptionalUserContext, auth_required: bool = False):
-        token = self.plugin.get_authorization_token(user_context)
+    def _get_request_headers(
+        self, context: FilesSourceRuntimeContext[RDMFileSourceConfiguration], auth_required: bool = False
+    ):
+        token = self.plugin.get_authorization_token(context)
         headers = {"Authorization": f"Bearer {token}"} if token else {}
         if auth_required and token is None:
             self._raise_auth_required()
diff --git a/lib/galaxy/files/sources/onedata.py b/lib/galaxy/files/sources/onedata.py
index 87f455c68dd9..e5171f9d75ea 100644
--- a/lib/galaxy/files/sources/onedata.py
+++ b/lib/galaxy/files/sources/onedata.py
@@ -3,39 +3,60 @@
 except ImportError:
     OnedataRESTFS = None
 
-from typing import Optional
 
+from typing import Union
+
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
 from galaxy.util import mapped_chars
-from . import FilesSourceOptions
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-def remove_prefix(prefix, string):
+def remove_prefix(prefix: str, string: str) -> str:
     if string.startswith(prefix):
         string = string[len(prefix) :]
     return string
 
 
-class OnedataFilesSource(PyFilesystem2FilesSource):
+class OnedataFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    access_token: Union[str, TemplateExpansion]
+    onezone_domain: Union[str, TemplateExpansion]
+    disable_tls_certificate_validation: Union[bool, TemplateExpansion] = False
+
+
+class OnedataFileSourceConfiguration(BaseFileSourceConfiguration):
+    access_token: str
+    onezone_domain: str
+    disable_tls_certificate_validation: bool = False
+
+
+class OnedataFilesSource(
+    PyFilesystem2FilesSource[OnedataFileSourceTemplateConfiguration, OnedataFileSourceConfiguration]
+):
     plugin_type = "onedata"
     required_module = OnedataRESTFS
     required_package = "fs.onedatarestfs"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
+    template_config_class = OnedataFileSourceTemplateConfiguration
+    resolved_config_class = OnedataFileSourceConfiguration
 
-        access_token = props.pop("access_token", "") or ""
-        onezone_domain = props.pop("onezone_domain", "") or ""
-        onezone_domain = remove_prefix("http://", remove_prefix("https://", onezone_domain))
-        disable_tls_certificate_validation = props.pop("disable_tls_certificate_validation", False) or False
+    def _open_fs(self, context: FilesSourceRuntimeContext[OnedataFileSourceConfiguration]):
+        if OnedataRESTFS is None:
+            raise self.required_package_exception
 
+        config = context.config
+        onezone_domain = remove_prefix("http://", remove_prefix("https://", config.onezone_domain))
         alt_space_fqn_separators = [mapped_chars["@"]] if "@" in mapped_chars else None
 
         handle = OnedataRESTFS(
-            onezone_domain,
-            access_token,
+            onezone_host=onezone_domain,
+            token=config.access_token,
             alt_space_fqn_separators=alt_space_fqn_separators,
-            verify_ssl=not disable_tls_certificate_validation,
+            verify_ssl=not config.disable_tls_certificate_validation,
         )
         return handle
 
diff --git a/lib/galaxy/files/sources/posix.py b/lib/galaxy/files/sources/posix.py
index 79840e9e7d2a..df9e2e265a61 100644
--- a/lib/galaxy/files/sources/posix.py
+++ b/lib/galaxy/files/sources/posix.py
@@ -2,24 +2,27 @@
 import os
 import shutil
 from typing import (
+    Any,
     Optional,
+    Union,
 )
 
-from typing_extensions import Unpack
-
 from galaxy import exceptions
-from galaxy.files import OptionalUserContext
+from galaxy.files.models import (
+    AnyRemoteEntry,
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+    RemoteDirectory,
+    RemoteFile,
+)
+from galaxy.util.config_templates import TemplateExpansion
 from galaxy.util.path import (
     safe_contains,
     safe_path,
     safe_walk,
 )
-from . import (
-    AnyRemoteEntry,
-    BaseFilesSource,
-    FilesSourceOptions,
-    FilesSourceProperties,
-)
+from . import BaseFilesSource
 
 DEFAULT_ENFORCE_SYMLINK_SECURITY = True
 DEFAULT_DELETE_ON_REALIZE = False
@@ -27,101 +30,101 @@
 DEFAULT_PREFER_LINKS = False
 
 
-class PosixFilesSourceProperties(FilesSourceProperties, total=False):
-    root: str
-    enforce_symlink_security: bool
-    delete_on_realize: bool
-    allow_subdir_creation: bool
-    prefer_links: bool
+class PosixTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    """Posix template configuration with templating support."""
+
+    root: Union[str, TemplateExpansion, None] = None
+    # These are not using TemplateExpansion because they are not user-configurable.
+    enforce_symlink_security: bool = DEFAULT_ENFORCE_SYMLINK_SECURITY
+    delete_on_realize: bool = DEFAULT_DELETE_ON_REALIZE
+    allow_subdir_creation: bool = DEFAULT_ALLOW_SUBDIR_CREATION
+    prefer_links: bool = DEFAULT_PREFER_LINKS
+
+
+class PosixConfiguration(BaseFileSourceConfiguration):
+    """Posix resolved configuration with proper types."""
 
+    root: Optional[str] = None
+    enforce_symlink_security: bool = DEFAULT_ENFORCE_SYMLINK_SECURITY
+    delete_on_realize: bool = DEFAULT_DELETE_ON_REALIZE
+    allow_subdir_creation: bool = DEFAULT_ALLOW_SUBDIR_CREATION
+    prefer_links: bool = DEFAULT_PREFER_LINKS
 
-class PosixFilesSource(BaseFilesSource):
+
+class PosixFilesSource(BaseFilesSource[PosixTemplateConfiguration, PosixConfiguration]):
     plugin_type = "posix"
 
-    # If this were a PyFilesystem2FilesSource all that would be needed would be,
+    template_config_class = PosixTemplateConfiguration
+    resolved_config_class = PosixConfiguration
+
+    # If this were a PyFilesystem2FilesSource it would be much simpler,
     # but we couldn't enforce security our way I suspect.
-    # def _open_fs(self):
-    #    from fs.osfs import OSFS
-    #    handle = OSFS(**self._props)
-    #    return handle
-
-    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
-        props = self._parse_common_config_opts(kwd)
-        self.root = props.get("root")
-        if not self.root:
-            self.writable = False
-        self.enforce_symlink_security = props.get("enforce_symlink_security", DEFAULT_ENFORCE_SYMLINK_SECURITY)
-        self.delete_on_realize = props.get("delete_on_realize", DEFAULT_DELETE_ON_REALIZE)
-        self.allow_subdir_creation = props.get("allow_subdir_creation", DEFAULT_ALLOW_SUBDIR_CREATION)
-        self._prefer_links = props.get("prefer_links", DEFAULT_PREFER_LINKS)
+
+    def __init__(self, template_config: PosixTemplateConfiguration):
+        super().__init__(template_config)
+        if not self.template_config.root:
+            self.template_config.writable = False
 
     def prefer_links(self) -> bool:
-        return self._prefer_links
+        return self.template_config.prefer_links
+
+    @property
+    def root(self) -> Optional[str]:
+        """Return the root directory for backward compatibility."""
+        return self.template_config.root
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[PosixConfiguration],
         path="/",
-        recursive=True,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        recursive=False,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
         sort_by: Optional[str] = None,
     ) -> tuple[list[AnyRemoteEntry], int]:
-        if not self.root:
+        if not context.config.root:
             raise exceptions.ItemAccessibilityException("Listing files at file:// URLs has been disabled.")
-        dir_path = self._to_native_path(path, user_context=user_context)
-        if not self._safe_directory(dir_path):
+        dir_path = self._to_native_path(path, context.config)
+        if not self._safe_directory(dir_path, context.config):
             raise exceptions.ObjectNotFound(f"The specified directory does not exist [{dir_path}].")
         if recursive:
             res: list[AnyRemoteEntry] = []
-            effective_root = self._effective_root(user_context)
+            effective_root = self._effective_root(context.config)
             for p, dirs, files in safe_walk(dir_path, allowlist=self._allowlist):
                 rel_dir = os.path.relpath(p, effective_root)
-                to_dict = functools.partial(self._resource_info_to_dict, rel_dir, user_context=user_context)
+                to_dict = functools.partial(self._resource_info_to_dict, rel_dir, config=context.config)
                 res.extend(map(to_dict, dirs))
                 res.extend(map(to_dict, files))
             return res, len(res)
         else:
-            res = os.listdir(dir_path)
-            to_dict = functools.partial(self._resource_info_to_dict, path, user_context=user_context)
-            return list(map(to_dict, res)), len(res)
+            entry_names = os.listdir(dir_path)
+            to_dict = functools.partial(self._resource_info_to_dict, path, config=context.config)
+            return list(map(to_dict, entry_names)), len(entry_names)
 
-    def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
-    ):
-        if not self.root and (not user_context or not user_context.is_admin):
+    def _realize_to(self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[PosixConfiguration]):
+        if not context.config.root and not context.user_data.is_admin:
             raise exceptions.ItemAccessibilityException("Writing to file:// URLs has been disabled.")
 
-        effective_root = self._effective_root(user_context)
-        source_native_path = self._to_native_path(source_path, user_context=user_context)
-        if self.enforce_symlink_security:
+        effective_root = self._effective_root(context.config)
+        source_native_path = self._to_native_path(source_path, context.config)
+        if context.config.enforce_symlink_security:
             if not safe_contains(effective_root, source_native_path, allowlist=self._allowlist):
                 raise Exception("Operation not allowed.")
         else:
             source_native_path = os.path.normpath(source_native_path)
             assert source_native_path.startswith(os.path.normpath(effective_root))
 
-        if not self.delete_on_realize:
+        if not context.config.delete_on_realize:
             shutil.copyfile(source_native_path, native_path)
         else:
             shutil.move(source_native_path, native_path)
 
-    def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
-    ):
-        effective_root = self._effective_root(user_context)
-        target_native_path = self._to_native_path(target_path, user_context=user_context)
-        if self.enforce_symlink_security:
+    def _write_from(self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[PosixConfiguration]):
+        effective_root = self._effective_root(context.config)
+        target_native_path = self._to_native_path(target_path, context.config)
+        if context.config.enforce_symlink_security:
             if not safe_contains(effective_root, target_native_path, allowlist=self._allowlist):
                 raise Exception("Operation not allowed.")
         else:
@@ -130,7 +133,7 @@ def _write_from(
 
         target_native_path_parent, target_native_path_name = os.path.split(target_native_path)
         if not os.path.exists(target_native_path_parent):
-            if self.allow_subdir_creation:
+            if context.config.allow_subdir_creation:
                 os.makedirs(target_native_path_parent)
             else:
                 raise Exception("Parent directory does not exist.")
@@ -141,34 +144,33 @@ def _write_from(
         shutil.copyfile(native_path, target_native_path_part)
         os.rename(target_native_path_part, target_native_path)
 
-    def _to_native_path(self, source_path: str, user_context: OptionalUserContext = None):
+    def _to_native_path(self, source_path: str, config: PosixConfiguration):
         source_path = os.path.normpath(source_path)
         if source_path.startswith("/"):
             source_path = source_path[1:]
-        return os.path.join(self._effective_root(user_context), source_path)
+        return os.path.join(self._effective_root(config), source_path)
 
-    def _effective_root(self, user_context: OptionalUserContext = None):
-        return self._evaluate_prop(self.root or "/", user_context=user_context)
+    def _effective_root(self, config: PosixConfiguration) -> str:
+        return config.root or "/"
 
-    def _resource_info_to_dict(self, dir: str, name: str, user_context: OptionalUserContext = None) -> AnyRemoteEntry:
+    def _resource_info_to_dict(self, dir: str, name: str, config: PosixConfiguration) -> AnyRemoteEntry:
         rel_path = os.path.normpath(os.path.join(dir, name))
-        full_path = self._to_native_path(rel_path, user_context=user_context)
+        full_path = self._to_native_path(rel_path, config)
         uri = self.uri_from_path(rel_path)
         if os.path.isdir(full_path):
-            return {"class": "Directory", "name": name, "uri": uri, "path": rel_path}
+            return RemoteDirectory(name=name, uri=uri, path=rel_path)
         else:
-            statinfo = os.lstat(full_path)
-            return {
-                "class": "File",
-                "name": name,
-                "size": statinfo.st_size,
-                "ctime": self.to_dict_time(statinfo.st_ctime),
-                "uri": uri,
-                "path": rel_path,
-            }
-
-    def _safe_directory(self, directory):
-        if self.enforce_symlink_security:
+            file_stat_info = os.lstat(full_path)
+            return RemoteFile(
+                name=name,
+                size=file_stat_info.st_size,
+                ctime=self.to_dict_time(file_stat_info.st_ctime),
+                uri=uri,
+                path=rel_path,
+            )
+
+    def _safe_directory(self, directory: str, config: PosixConfiguration) -> bool:
+        if config.enforce_symlink_security:
             if not safe_path(directory, allowlist=self._allowlist):
                 raise exceptions.ConfigDoesNotAllowException(
                     f"directory ({directory}) is a symlink to a location not on the allowlist"
@@ -178,37 +180,36 @@ def _safe_directory(self, directory):
             return False
         return True
 
-    def _serialization_props(self, user_context: OptionalUserContext = None) -> PosixFilesSourceProperties:
-        return {
-            # abspath needed because will be used by external Python from
-            # a job working directory
-            "root": os.path.abspath(self._effective_root(user_context)),
-            "enforce_symlink_security": self.enforce_symlink_security,
-            "delete_on_realize": self.delete_on_realize,
-            "allow_subdir_creation": self.allow_subdir_creation,
-            "prefer_links": self._prefer_links,
-        }
+    def _serialize_config(self, config: PosixConfiguration) -> dict[str, Any]:
+        # abspath needed because will be used by external Python from
+        # a job working directory
+        abs_root = os.path.abspath(self._effective_root(config))
+        serialized_config = super()._serialize_config(config)
+        serialized_config.update({"root": abs_root})
+        return serialized_config
 
     @property
     def _allowlist(self):
         return self._file_sources_config.symlink_allowlist
 
     def score_url_match(self, url: str):
+        # We need to use template_config here because this is called before the template is expanded.
+        root = self.template_config.root
         # For security, we need to ensure that a partial match doesn't work. e.g. file://{root}something/myfiles
-        if self.root and (
-            url.startswith(f"{self.get_uri_root()}://{self.root}/") or url == f"self.get_uri_root()://{self.root}"
-        ):
-            return len(f"self.get_uri_root()://{self.root}")
-        elif self.root and (url.startswith(f"file://{self.root}/") or url == f"file://{self.root}"):
-            return len(f"file://{self.root}")
-        elif not self.root and url.startswith("file://"):
+        if root and (url.startswith(f"{self.get_uri_root()}://{root}/") or url == f"self.get_uri_root()://{root}"):
+            return len(f"self.get_uri_root()://{root}")
+        elif root and (url.startswith(f"file://{root}/") or url == f"file://{root}"):
+            return len(f"file://{root}")
+        elif not root and url.startswith("file://"):
             return len("file://")
         else:
             return super().score_url_match(url)
 
     def to_relative_path(self, url: str) -> str:
-        if url.startswith(f"file://{self.root}"):
-            return url[len(f"file://{self.root}") :]
+        # We need to use template_config.root here because this is called before the template is expanded.
+        root = self.template_config.root
+        if url.startswith(f"file://{root}"):
+            return url[len(f"file://{root}") :]
         elif url.startswith("file://"):
             return url[7:]
         else:
diff --git a/lib/galaxy/files/sources/remotezip.py b/lib/galaxy/files/sources/remotezip.py
index b64ef4d1d586..e6a6b627528e 100644
--- a/lib/galaxy/files/sources/remotezip.py
+++ b/lib/galaxy/files/sources/remotezip.py
@@ -5,9 +5,6 @@
     fields,
 )
 from struct import unpack
-from typing import (
-    Optional,
-)
 from urllib.parse import (
     parse_qs,
     unquote,
@@ -15,14 +12,15 @@
 )
 
 import requests
-from typing_extensions import Unpack
 
-from galaxy.files import OptionalUserContext
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
 from galaxy.files.uris import validate_uri_access
 from . import (
-    BaseFilesSource,
-    FilesSourceOptions,
-    FilesSourceProperties,
+    DefaultBaseFilesSource,
     PluginKind,
 )
 
@@ -51,48 +49,39 @@ class FileExtractParameters:
 """
 
 
-class RemoteZipFilesSource(BaseFilesSource):
+class RemoteZipFilesSource(DefaultBaseFilesSource):
     plugin_type = "remoteZip"
     plugin_kind = PluginKind.stock
 
-    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
-        kwds: FilesSourceProperties = {
-            "id": "extract",
-            "label": "Remote ZIP extractor",
-            "doc": DOC_TEMPLATE,
-            "writable": False,
-            "browsable": False,
-        }
-        kwds.update(kwd)
-        props = self._parse_common_config_opts(kwds)
-        self._props = props
+    def __init__(self, template_config: BaseFileSourceTemplateConfiguration):
+        defaults = dict(
+            id="extract",
+            label="Remote ZIP extractor",
+            doc=DOC_TEMPLATE,
+            writable=False,
+            browsable=False,
+        )
+        template_config = self._apply_defaults_to_template(defaults, template_config)
+        super().__init__(template_config)
 
     @property
     def _allowlist(self):
         return self._file_sources_config.fetch_url_allowlist
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[BaseFileSourceConfiguration]
     ):
         params = extract_query_parameters(source_path)
         file_extract_params = validate_params(params)
         validate_uri_access(
             file_extract_params.source,
-            user_context.is_admin if user_context else False,
+            context.user_data.is_admin,
             self._allowlist or [],
         )
         stream_and_decompress(file_extract_params, native_path)
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[BaseFileSourceConfiguration]
     ):
         raise NotImplementedError()
 
@@ -102,12 +91,6 @@ def score_url_match(self, url: str):
         else:
             return 0
 
-    def _serialization_props(self, user_context: OptionalUserContext = None):
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        return effective_props
-
 
 def extract_query_parameters(url: str) -> dict[str, str]:
     parsed_url = urlparse(url)
diff --git a/lib/galaxy/files/sources/rspace.py b/lib/galaxy/files/sources/rspace.py
index 1f788ca8271b..85458497579a 100644
--- a/lib/galaxy/files/sources/rspace.py
+++ b/lib/galaxy/files/sources/rspace.py
@@ -41,14 +41,15 @@
     Union,
 )
 
-from galaxy.files import OptionalUserContext
-from . import (
+from galaxy.files.models import (
     AnyRemoteEntry,
-    FilesSourceOptions,
-    FilesSourceProperties,
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
     RemoteDirectory,
     RemoteFile,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 try:
@@ -122,28 +123,37 @@ def upload(self, path: str, file: BinaryIO, chunk_size: Optional[int] = None, **
             self.upload_global_id = self._upload_response["globalId"]
 
 
-class RSpaceFilesSource(PyFilesystem2FilesSource):
+class RSpaceFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    endpoint: Union[str, TemplateExpansion]
+    api_key: Union[str, TemplateExpansion]
+
+
+class RSpaceFileSourceConfiguration(BaseFileSourceConfiguration):
+    endpoint: str
+    api_key: str
+
+
+class RSpaceFilesSource(PyFilesystem2FilesSource[RSpaceFileSourceTemplateConfiguration, RSpaceFileSourceConfiguration]):
     plugin_type = "rspace"
     required_module = RSpaceGalleryFilesystem
     required_package = "rspace_client.eln.fs"
 
+    template_config_class = RSpaceFileSourceTemplateConfiguration
+    resolved_config_class = RSpaceFileSourceConfiguration
+
     _upload_global_id: str
     # The RSpace global id of the most recently uploaded file.
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None) -> RSpaceGalleryFilesystem:
+    def _open_fs(self, context: FilesSourceRuntimeContext[RSpaceFileSourceConfiguration]):
         """
         Instantiate a PyFilesystem2 FS object for RSpace's Gallery.
         """
-        props = self._serialization_props(user_context)
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        all_props: dict = {**props, **extra_props}
-
-        endpoint = all_props["endpoint"]
-        api_key = all_props["api_key"]
+        if RSpaceGalleryFilesystem is None:
+            raise self.required_package_exception
 
         # patch the `upload()` method to keep track of the global id of the most recently uploaded file, change the
         # target path and fake the name of the file
-        gallery_fs = PatchedRSpaceGalleryFilesystem(endpoint, api_key)
+        gallery_fs = PatchedRSpaceGalleryFilesystem(context.config.endpoint, context.config.api_key)
         gallery_fs_upload_method = gallery_fs.upload
 
         def upload(self_, path: str, file: BinaryIO, chunk_size: Optional[int] = None, **options: Any) -> None:
@@ -160,17 +170,13 @@ def upload(self_, path: str, file: BinaryIO, chunk_size: Optional[int] = None, *
         return gallery_fs
 
     def _write_from(
-        self,
-        target_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, target_path: str, native_path: str, context: FilesSourceRuntimeContext[RSpaceFileSourceConfiguration]
     ) -> str:
         """
         Save a file to the RSpace Gallery.
         """
         target_directory = os.path.dirname(target_path)
-        super()._write_from(target_path, native_path, user_context=user_context, opts=opts)
+        super()._write_from(target_path, native_path, context)
         return os.path.join(target_directory, self._upload_global_id)
 
     def _resource_info_to_dict(self, dir_path, resource_info) -> AnyRemoteEntry:
@@ -194,16 +200,10 @@ def _resource_info_to_dict(self, dir_path, resource_info) -> AnyRemoteEntry:
 
         entry: AnyRemoteEntry
         if resource_info.is_dir:
-            dict_.update(
-                {
-                    "class": "Directory",
-                }
-            )
-            entry = RemoteDirectory(**cast(RemoteDirectory, dict_))
+            entry = RemoteDirectory(name=dict_["name"], uri=dict_["uri"], path=dict_["path"])
         else:
             dict_.update(
                 {
-                    "class": "File",
                     "size": resource_info.size,
                     "ctime": self.to_dict_time(
                         datetime.datetime.fromisoformat(resource_info.get("rspace", "created")).astimezone(
@@ -212,6 +212,12 @@ def _resource_info_to_dict(self, dir_path, resource_info) -> AnyRemoteEntry:
                     ),
                 }
             )
-            entry = RemoteFile(**cast(RemoteFile, dict_))
+            entry = RemoteFile(
+                name=dict_["name"],
+                size=dict_["size"],
+                ctime=dict_["ctime"],
+                uri=dict_["uri"],
+                path=dict_["path"],
+            )
 
         return entry
diff --git a/lib/galaxy/files/sources/s3fs.py b/lib/galaxy/files/sources/s3fs.py
index 57d1f7d9f268..d1718e4b4526 100644
--- a/lib/galaxy/files/sources/s3fs.py
+++ b/lib/galaxy/files/sources/s3fs.py
@@ -2,22 +2,20 @@
 import logging
 import os
 from typing import (
-    cast,
     Optional,
-)
-
-from typing_extensions import (
-    NotRequired,
-    Unpack,
+    Union,
 )
 
 from galaxy import exceptions
-from galaxy.files import OptionalUserContext
-from . import (
+from galaxy.files.models import (
     AnyRemoteEntry,
-    FilesSourceOptions,
-    FilesSourceProperties,
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+    RemoteDirectory,
+    RemoteFile,
 )
+from galaxy.util.config_templates import TemplateExpansion
 
 try:
     import s3fs
@@ -28,60 +26,61 @@
 
 DEFAULT_ENFORCE_SYMLINK_SECURITY = True
 DEFAULT_DELETE_ON_REALIZE = False
+FS_PLUGIN_TYPE = "s3fs"
 
 log = logging.getLogger(__name__)
 
 
-class S3FsFilesSourceProperties(FilesSourceProperties, total=False):
-    bucket: str
-    endpoint_url: int
-    user: str
-    passwd: str
-    listings_expiry_time: NotRequired[Optional[int]]
-    client_kwargs: dict  # internally computed. Should not be specified in config file
+class S3FSFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    anon: Union[bool, TemplateExpansion] = False
+    endpoint_url: Union[str, TemplateExpansion, None] = None
+    bucket: Union[str, TemplateExpansion, None] = None
+    secret: Union[str, TemplateExpansion, None] = None
+    key: Union[str, TemplateExpansion, None] = None
+
 
+class S3FSFileSourceConfiguration(BaseFileSourceConfiguration):
+    anon: bool = False
+    endpoint_url: Optional[str] = None
+    bucket: Optional[str] = None
+    secret: Optional[str] = None
+    key: Optional[str] = None
 
-class S3FsFilesSource(BaseFilesSource):
-    plugin_type = "s3fs"
 
-    def __init__(self, **kwd: Unpack[S3FsFilesSourceProperties]):
+class S3FsFilesSource(BaseFilesSource[S3FSFileSourceTemplateConfiguration, S3FSFileSourceConfiguration]):
+    plugin_type = FS_PLUGIN_TYPE
+
+    template_config_class = S3FSFileSourceTemplateConfiguration
+    resolved_config_class = S3FSFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[S3FSFileSourceConfiguration]):
         if s3fs is None:
-            raise Exception("Package s3fs unavailable but required for this file source plugin.")
-        props: S3FsFilesSourceProperties = cast(S3FsFilesSourceProperties, self._parse_common_config_opts(kwd))
-        file_sources_config = self._file_sources_config
-        if (
-            props.get("listings_expiry_time") is None
-            and file_sources_config
-            and file_sources_config.listings_expiry_time
-        ):
-            if file_sources_config.listings_expiry_time:
-                props["listings_expiry_time"] = file_sources_config.listings_expiry_time
-        # There is a possibility that the bucket name could be parameterized: e.g.
-        # bucket: ${user.preferences['generic_s3|bucket']}
-        # that's ok, because we evaluate the bucket name again later. The bucket property here will only
-        # be used by `score_url_match`. In the case of a parameterized bucket name, we will always return
-        # a score of 4 as the url will only match the s3:// part.
-        self._bucket = props.get("bucket", "")
-        self._endpoint_url = props.pop("endpoint_url", None)
-        self._props = props
-        if self._endpoint_url:
-            self._props.update({"client_kwargs": {"endpoint_url": self._endpoint_url}})
+            raise Exception("Missing s3fs package, please install it to use S3 file sources.")
+
+        config = context.config
+        client_kwargs = {"endpoint_url": config.endpoint_url} if config.endpoint_url else None
+        fs = s3fs.S3FileSystem(
+            anon=config.anon,
+            key=config.key,
+            secret=config.secret,
+            client_kwargs=client_kwargs,
+            listings_expiry_time=config.file_sources_config.listings_expiry_time,
+        )
+        return fs
 
     def _list(
         self,
+        context: FilesSourceRuntimeContext[S3FSFileSourceConfiguration],
         path="/",
-        recursive=True,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        recursive=False,
+        write_intent: bool = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
         sort_by: Optional[str] = None,
     ) -> tuple[list[AnyRemoteEntry], int]:
-        _props = self._serialization_props(user_context)
-        # we need to pop the 'bucket' here, because the argument is not recognised in a downstream function
-        _bucket_name = _props.pop("bucket", "")
-        fs = self._open_fs(props=_props, opts=opts)
+        _bucket_name = context.config.bucket or ""
+        fs = self._open_fs(context)
         if recursive:
             res: list[AnyRemoteEntry] = []
             bucket_path = self._bucket_path(_bucket_name, path)
@@ -100,28 +99,16 @@ def _list(
             return list(map(to_dict, res)), len(res)
 
     def _realize_to(
-        self,
-        source_path: str,
-        native_path: str,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
+        self, source_path: str, native_path: str, context: FilesSourceRuntimeContext[S3FSFileSourceConfiguration]
     ):
-        _props = self._serialization_props(user_context)
-        _bucket_name = _props.pop("bucket", "")
-        fs = self._open_fs(props=_props, opts=opts)
+        _bucket_name = context.config.bucket or ""
+        fs = self._open_fs(context)
         bucket_path = self._bucket_path(_bucket_name, source_path)
         fs.download(bucket_path, native_path)
 
-    def _write_from(
-        self,
-        target_path,
-        native_path,
-        user_context: OptionalUserContext = None,
-        opts: Optional[FilesSourceOptions] = None,
-    ):
-        _props = self._serialization_props(user_context)
-        _bucket_name = _props.pop("bucket", "")
-        fs = self._open_fs(props=_props, opts=opts)
+    def _write_from(self, target_path, native_path, context: FilesSourceRuntimeContext[S3FSFileSourceConfiguration]):
+        _bucket_name = context.config.bucket or ""
+        fs = self._open_fs(context)
         bucket_path = self._bucket_path(_bucket_name, target_path)
         fs.upload(native_path, bucket_path)
 
@@ -132,39 +119,28 @@ def _bucket_path(self, bucket_name: str, path: str):
             path = f"/{path}"
         return f"{bucket_name}{path}"
 
-    def _open_fs(self, props: FilesSourceProperties, opts: Optional[FilesSourceOptions] = None):
-        extra_props = opts.extra_props or {} if opts else {}
-        fs = s3fs.S3FileSystem(**{**props, **extra_props})
-        return fs
-
     def _resource_info_to_dict(self, dir_path: str, resource_info) -> AnyRemoteEntry:
-        name = os.path.basename(resource_info["name"])
+        name = str(os.path.basename(resource_info["name"]))
         path = os.path.join(dir_path, name)
         uri = self.uri_from_path(path)
         if resource_info["type"] == "directory":
-            return {"class": "Directory", "name": name, "uri": uri, "path": path}
+            return RemoteDirectory(name=name, uri=uri, path=path)
         else:
-            return {
-                "class": "File",
-                "name": name,
-                "size": resource_info["size"],
-                # should this be mtime...
-                "ctime": self.to_dict_time(resource_info["LastModified"]),
-                "uri": uri,
-                "path": path,
-            }
-
-    def _serialization_props(self, user_context: OptionalUserContext = None) -> S3FsFilesSourceProperties:
-        effective_props = {}
-        for key, val in self._props.items():
-            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
-        return cast(S3FsFilesSourceProperties, effective_props)
+            return RemoteFile(
+                name=name,
+                size=resource_info["size"],
+                ctime=self.to_dict_time(resource_info["LastModified"]),
+                uri=uri,
+                path=path,
+            )
 
     def score_url_match(self, url: str):
+        # We need to use template_config here because this is called before the template is expanded.
+        bucket_name = self.template_config.bucket
         # For security, we need to ensure that a partial match doesn't work. e.g. s3://{bucket}something/myfiles
-        if self._bucket and (url.startswith(f"s3://{self._bucket}/") or url == f"s3://{self._bucket}"):
-            return len(f"s3://{self._bucket}")
-        elif not self._bucket and url.startswith("s3://"):
+        if bucket_name and (url.startswith(f"s3://{bucket_name}/") or url == f"s3://{bucket_name}"):
+            return len(f"s3://{bucket_name}")
+        elif not bucket_name and url.startswith("s3://"):
             return len("s3://")
         else:
             return super().score_url_match(url)
diff --git a/lib/galaxy/files/sources/ssh.py b/lib/galaxy/files/sources/ssh.py
index 7be93f7ec2c4..59790dfe4a09 100644
--- a/lib/galaxy/files/sources/ssh.py
+++ b/lib/galaxy/files/sources/ssh.py
@@ -1,5 +1,5 @@
 try:
-    from fs.sshfs import SSHFS
+    from fs.sshfs.sshfs import SSHFS
 except ImportError:
     SSHFS = None
 
@@ -8,25 +8,64 @@
     Union,
 )
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class SshFilesSource(PyFilesystem2FilesSource):
+class SshFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    host: Union[str, TemplateExpansion]
+    user: Optional[Union[str, TemplateExpansion]] = None
+    passwd: Optional[Union[str, TemplateExpansion]] = None
+    pkey: Optional[Union[str, TemplateExpansion]] = None
+    timeout: Union[int, TemplateExpansion] = 10
+    port: Union[int, TemplateExpansion] = 22
+    compress: Union[bool, TemplateExpansion] = False
+    config_path: Union[str, TemplateExpansion] = "~/.ssh/config"
+    path: Union[str, TemplateExpansion]
+
+
+class SshFileSourceConfiguration(BaseFileSourceConfiguration):
+    host: str
+    user: Optional[str] = None
+    passwd: Optional[str] = None
+    pkey: Optional[str] = None
+    timeout: int = 10
+    port: int = 22
+    compress: bool = False
+    config_path: str = "~/.ssh/config"
+    path: str
+
+
+class SshFilesSource(PyFilesystem2FilesSource[SshFileSourceTemplateConfiguration, SshFileSourceConfiguration]):
     plugin_type = "ssh"
     required_module = SSHFS
     required_package = "fs.sshfs"
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        path = props.pop("path")
-        handle = SSHFS(**{**props, **extra_props})
-        if path:
-            handle = handle.opendir(path)
+    template_config_class = SshFileSourceTemplateConfiguration
+    resolved_config_class = SshFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[SshFileSourceConfiguration]):
+        if SSHFS is None:
+            raise self.required_package_exception
+
+        config = context.config
+        handle = SSHFS(
+            host=config.host,
+            user=config.user,
+            passwd=config.passwd,
+            pkey=config.pkey,
+            port=config.port,
+            timeout=config.timeout,
+            compress=config.compress,
+            config_path=config.config_path,
+        )
+        if config.path:
+            return handle.opendir(config.path)
         return handle
 
 
diff --git a/lib/galaxy/files/sources/temp.py b/lib/galaxy/files/sources/temp.py
index 441c9e82c49f..ca7d6693a445 100644
--- a/lib/galaxy/files/sources/temp.py
+++ b/lib/galaxy/files/sources/temp.py
@@ -1,12 +1,25 @@
-from typing import Optional
+from typing import Union
 
 from fs.osfs import OSFS
 
-from . import FilesSourceOptions
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
+)
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class TempFilesSource(PyFilesystem2FilesSource):
+class TempFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    root_path: Union[str, TemplateExpansion]
+
+
+class TempFileSourceConfiguration(BaseFileSourceConfiguration):
+    root_path: str
+
+
+class TempFilesSource(PyFilesystem2FilesSource[TempFileSourceTemplateConfiguration, TempFileSourceConfiguration]):
     """A FilesSource plugin for temporary file systems.
 
     Used for testing and other temporary file system needs.
@@ -16,14 +29,16 @@ class TempFilesSource(PyFilesystem2FilesSource):
 
     plugin_type = "temp"
     required_module = OSFS
+    required_package = "fs.osfs"
+
+    template_config_class = TempFileSourceTemplateConfiguration
+    resolved_config_class = TempFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[TempFileSourceConfiguration]):
+        if OSFS is None:
+            raise self.required_package_exception
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = self._serialization_props(user_context)
-        extra_props = opts.extra_props or {} if opts else {}
-        # We use OSFS here because using TempFS or MemoryFS would wipe out the files
-        # every time we instantiate a new handle, which happens on every request.
-        handle = OSFS(**{**props, **extra_props})
-        return handle
+        return OSFS(root_path=context.config.root_path)
 
     def get_scheme(self) -> str:
         return "temp"
diff --git a/lib/galaxy/files/sources/util.py b/lib/galaxy/files/sources/util.py
index 1faeb2e5726d..7de0a49cf74d 100644
--- a/lib/galaxy/files/sources/util.py
+++ b/lib/galaxy/files/sources/util.py
@@ -1,15 +1,15 @@
 import time
-from typing import (
-    Optional,
-)
+from typing import Optional
 
 from galaxy import exceptions
 from galaxy.files import (
     ConfiguredFileSources,
     FileSourcesUserContext,
 )
-from galaxy.files.sources import FilesSourceOptions
-from galaxy.files.sources.http import HTTPFilesSourceProperties
+from galaxy.files.models import (
+    FilesSourceOptions,
+    PartialFilesSourceProperties,
+)
 from galaxy.files.uris import stream_url_to_file
 from galaxy.util import (
     DEFAULT_SOCKET_TIMEOUT,
@@ -106,13 +106,12 @@ def fetch_drs_to_file(
         access_url, access_headers = _get_access_info(get_url, access_method, headers=headers)
         opts = FilesSourceOptions()
         if access_method["type"] == "https":
-            extra_props: HTTPFilesSourceProperties = {
+            extra_props = {
                 "http_headers": access_headers or {},
                 "fetch_url_allowlist": fetch_url_allowlist or [],
             }
-            opts.extra_props = extra_props
-        else:
-            opts.extra_props = {}
+            opts.extra_props = PartialFilesSourceProperties(**extra_props)
+
         try:
             file_sources = (
                 user_context.file_sources
diff --git a/lib/galaxy/files/sources/webdav.py b/lib/galaxy/files/sources/webdav.py
index 9aec17fdbdc4..218ce5515399 100644
--- a/lib/galaxy/files/sources/webdav.py
+++ b/lib/galaxy/files/sources/webdav.py
@@ -5,51 +5,94 @@
 
 import tempfile
 from typing import (
-    cast,
+    Annotated,
     Optional,
     Union,
 )
 
-from typing_extensions import NotRequired
+from pydantic import (
+    Field,
+    field_validator,
+)
 
-from . import (
-    FilesSourceOptions,
-    FilesSourceProperties,
+from galaxy.files.models import (
+    BaseFileSourceConfiguration,
+    BaseFileSourceTemplateConfiguration,
+    FilesSourceRuntimeContext,
 )
+from galaxy.util.config_templates import TemplateExpansion
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
-class WebDavFilesSourceProperties(FilesSourceProperties, total=False):
-    use_temp_files: NotRequired[Optional[bool]]
-    temp_path: NotRequired[Optional[str]]
+class WebDavFileSourceTemplateConfiguration(BaseFileSourceTemplateConfiguration):
+    url: Union[str, TemplateExpansion, None] = None
+    root: Optional[Union[str, TemplateExpansion]] = None
+    login: Optional[Union[str, TemplateExpansion]] = None
+    password: Optional[Union[str, TemplateExpansion]] = None
+    temp_path: Optional[Union[str, TemplateExpansion]] = None
+    use_temp_files: Union[bool, TemplateExpansion] = True
+
+
+class WebDavFileSourceConfiguration(BaseFileSourceConfiguration):
+    # Override url field to make it required for WebDAV - we keep a default but validate it's provided
+    url: Annotated[
+        str,
+        Field(
+            None,
+            title="WebDAV URL",
+            description="The URL of the WebDAV server. This is required for WebDAV file sources.",
+        ),
+    ] = None  # type: ignore[assignment]
+    root: Optional[str] = None
+    login: Optional[str] = None
+    password: Optional[str] = None
+    temp_path: Optional[str] = None
+    use_temp_files: bool = True  # Default to True to avoid memory issues with large files.
+
+    @field_validator("url")
+    @classmethod
+    def validate_url_required(cls, v):
+        if v is None or v == "":
+            raise ValueError("url is required for WebDAV file source")
+        return v
 
 
-class WebDavFilesSource(PyFilesystem2FilesSource):
+class WebDavFilesSource(PyFilesystem2FilesSource[WebDavFileSourceTemplateConfiguration, WebDavFileSourceConfiguration]):
     plugin_type = "webdav"
     required_module = WebDAVFS
     required_package = "fs.webdavfs"
     allow_key_error_on_empty_directories = True
 
-    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
-        props = cast(WebDavFilesSourceProperties, self._serialization_props(user_context))
+    template_config_class = WebDavFileSourceTemplateConfiguration
+    resolved_config_class = WebDavFileSourceConfiguration
+
+    def _open_fs(self, context: FilesSourceRuntimeContext[WebDavFileSourceConfiguration]):
+        if WebDAVFS is None:
+            raise self.required_package_exception
+
+        config = context.config
         file_sources_config = self._file_sources_config
-        use_temp_files = props.pop("use_temp_files", None)
-        if use_temp_files is None and file_sources_config and file_sources_config.webdav_use_temp_files is not None:
+        use_temp_files = config.use_temp_files
+        if file_sources_config and file_sources_config.webdav_use_temp_files is not None:
             use_temp_files = file_sources_config.webdav_use_temp_files
-        if use_temp_files is None:
-            # Default to True to avoid memory issues with large files.
-            use_temp_files = True
 
         if use_temp_files:
-            temp_path = props.get("temp_path")
+            temp_path = config.temp_path
             if temp_path is None and file_sources_config and file_sources_config.tmp_dir:
                 temp_path = file_sources_config.tmp_dir
             if temp_path is None:
                 temp_path = tempfile.mkdtemp(prefix="webdav_")
-            props["temp_path"] = temp_path
-        props["use_temp_files"] = use_temp_files
-        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
-        handle = WebDAVFS(**{**props, **extra_props})
+            config.temp_path = temp_path
+        config.use_temp_files = use_temp_files
+
+        handle = WebDAVFS(
+            url=config.url,
+            root=config.root,
+            login=config.login,
+            password=config.password,
+            temp_path=config.temp_path,
+            use_temp_files=config.use_temp_files,
+        )
         return handle
 
 
diff --git a/lib/galaxy/files/templates/models.py b/lib/galaxy/files/templates/models.py
index 5e656642d00e..7f939b24523f 100644
--- a/lib/galaxy/files/templates/models.py
+++ b/lib/galaxy/files/templates/models.py
@@ -9,9 +9,7 @@
     Field,
     RootModel,
 )
-from typing_extensions import (
-    Literal,
-)
+from typing_extensions import Literal
 
 from galaxy.util.config_templates import (
     ConfiguredOAuth2Sources,
diff --git a/lib/galaxy/files/unittest_utils/__init__.py b/lib/galaxy/files/unittest_utils/__init__.py
index f211c39ebf22..2190329505b3 100644
--- a/lib/galaxy/files/unittest_utils/__init__.py
+++ b/lib/galaxy/files/unittest_utils/__init__.py
@@ -1,8 +1,6 @@
 import os
 import tempfile
-from typing import (
-    Optional,
-)
+from typing import Optional
 
 from galaxy.files import (
     ConfiguredFileSources,
@@ -25,7 +23,7 @@ def __init__(self, root: str):
             "type": "posix",
             "root": root,
         }
-        file_sources_config = FileSourcePluginsConfig({})
+        file_sources_config = FileSourcePluginsConfig()
         super().__init__(file_sources_config, {"test1": plugin}, root)
 
 
diff --git a/lib/galaxy/files/uris.py b/lib/galaxy/files/uris.py
index 8697836aefca..5e492c53e0cc 100644
--- a/lib/galaxy/files/uris.py
+++ b/lib/galaxy/files/uris.py
@@ -3,9 +3,7 @@
 import os
 import socket
 import tempfile
-from typing import (
-    Optional,
-)
+from typing import Optional
 from urllib.parse import urlparse
 
 from galaxy.exceptions import (
@@ -17,7 +15,7 @@
     ConfiguredFileSources,
     NoMatchingFileSource,
 )
-from galaxy.files.sources import FilesSourceOptions
+from galaxy.files.models import FilesSourceOptions
 from galaxy.util import (
     stream_to_open_named_file,
     unicodify,
diff --git a/lib/galaxy/files/validate/__init__.py b/lib/galaxy/files/validate/__init__.py
new file mode 100644
index 000000000000..e69de29bb2d1
diff --git a/lib/galaxy/files/validate/script.py b/lib/galaxy/files/validate/script.py
new file mode 100755
index 000000000000..5ca0ec0d14fe
--- /dev/null
+++ b/lib/galaxy/files/validate/script.py
@@ -0,0 +1,302 @@
+#!/usr/bin/env python3
+"""
+Script to validate file sources configuration by attempting to create instances
+of each configured file source.
+
+This script loads the currently configured file_sources_config_file YAML file
+and tries to instantiate each of the specified file sources in order to validate
+the configuration models.
+"""
+
+import argparse
+import os
+import sys
+import traceback
+from typing import (
+    Dict,
+    List,
+    Optional,
+)
+
+import yaml
+
+from galaxy.files.models import FileSourcePluginsConfig
+from galaxy.files.plugins import FileSourcePluginLoader
+from galaxy.files.sources import BaseFilesSource
+
+
+class Colors:
+    """ANSI color codes for terminal output"""
+
+    RED = "\033[91m"
+    GREEN = "\033[92m"
+    YELLOW = "\033[93m"
+    BLUE = "\033[94m"
+    MAGENTA = "\033[95m"
+    CYAN = "\033[96m"
+    WHITE = "\033[97m"
+    BOLD = "\033[1m"
+    UNDERLINE = "\033[4m"
+    RESET = "\033[0m"
+
+    @classmethod
+    def colorize(cls, text: str, color: str) -> str:
+        """Apply color to text if stdout is a TTY"""
+        if sys.stdout.isatty():
+            return f"{color}{text}{cls.RESET}"
+        return text
+
+    @classmethod
+    def success(cls, text: str) -> str:
+        return cls.colorize(text, cls.GREEN)
+
+    @classmethod
+    def error(cls, text: str) -> str:
+        return cls.colorize(text, cls.RED)
+
+    @classmethod
+    def warning(cls, text: str) -> str:
+        return cls.colorize(text, cls.YELLOW)
+
+    @classmethod
+    def info(cls, text: str) -> str:
+        return cls.colorize(text, cls.BLUE)
+
+    @classmethod
+    def bold(cls, text: str) -> str:
+        return cls.colorize(text, cls.BOLD)
+
+
+def parse_arguments():
+    parser = argparse.ArgumentParser(
+        description="Validate file sources configuration by instantiating each configured file source"
+    )
+    parser.add_argument(
+        "--config-file",
+        "-c",
+        help="Path to Galaxy configuration file (galaxy.yml). If not provided, will use standard discovery.",
+    )
+    parser.add_argument(
+        "--file-sources-config",
+        "-f",
+        help="Path to file sources configuration file. If not provided, will use the value from Galaxy config.",
+    )
+    parser.add_argument(
+        "--verbose",
+        "-v",
+        action="store_true",
+        help="Show verbose output including successful validations",
+    )
+    parser.add_argument(
+        "--fail-fast",
+        action="store_true",
+        help="Exit immediately on first validation error",
+    )
+    return parser.parse_args()
+
+
+def find_galaxy_config(config_file: Optional[str] = None) -> Optional[str]:
+    """Find the Galaxy configuration file"""
+    if config_file and os.path.exists(config_file):
+        return config_file
+
+    # Try standard locations
+    possible_configs = [
+        "../config/galaxy.yml",
+        "config/galaxy.yml",
+        "galaxy.yml",
+        "../config/galaxy.yml.sample",
+        "config/galaxy.yml.sample",
+    ]
+
+    for config_path in possible_configs:
+        if os.path.exists(config_path):
+            return config_path
+
+    return None
+
+
+def load_file_sources_config(config_path: str) -> List[Dict]:
+    """Load file sources configuration from YAML file"""
+    try:
+        with open(config_path) as f:
+            config = yaml.safe_load(f)
+
+        if not isinstance(config, list):
+            raise ValueError(f"File sources config should be a list, got {type(config)}")
+
+        return config
+    except Exception as e:
+        print(Colors.error(f"Error loading file sources config from {config_path}: {e}"))
+        sys.exit(1)
+
+
+def validate_file_source_config(
+    file_source_config: Dict,
+    plugin_loader: FileSourcePluginLoader,
+    file_sources_config: FileSourcePluginsConfig,
+    verbose: bool = False,
+) -> bool:
+    """
+    Validate a single file source configuration by trying to instantiate it.
+    Returns True if valid, False if invalid.
+    """
+    file_source_id = file_source_config.get("id", "unknown")
+    file_source_type = file_source_config.get("type", "unknown")
+
+    try:
+        # Get the plugin class for this file source type
+        plugin_class = plugin_loader.get_plugin_type_class(file_source_type)
+
+        if verbose:
+            print(f"  Validating file source '{file_source_id}' of type '{file_source_type}'...")
+
+        # Create a copy of config with extra kwargs
+        plugin_kwds = file_source_config.copy()
+        plugin_kwds.update({"file_sources_config": file_sources_config})
+
+        if verbose:
+            print("    Using configuration:")
+            yaml_str = yaml.dump(file_source_config, default_flow_style=False, sort_keys=False, indent=2)
+            indented_yaml = "\n".join(" " * 8 + line if line.strip() else "" for line in yaml_str.splitlines())
+            print(Colors.info(indented_yaml))
+
+        # Try to instantiate the file source using the same pattern as the plugin loader
+        # Check if this plugin uses the configurable plugin pattern
+        configurable_instance = None
+        try:
+            if hasattr(plugin_class, "build_template_config"):
+                configurable_instance = plugin_class
+        except TypeError:
+            pass
+
+        if configurable_instance:
+            # Use the template config pattern
+            plugin_template_config = configurable_instance.build_template_config(**plugin_kwds)
+            file_source = configurable_instance(template_config=plugin_template_config)
+        else:
+            # Use direct instantiation
+            file_source = plugin_class(**plugin_kwds)
+
+        # Verify it's a valid file source
+        if not isinstance(file_source, BaseFilesSource):
+            raise ValueError("Plugin did not return a valid BaseFilesSource instance")
+
+        if verbose:
+            print(
+                f"    {Colors.success(' Valid')}: File source '{Colors.bold(file_source_id)}' ({file_source_type}) configured successfully"
+            )
+
+        return True
+
+    except KeyError as e:
+        print(
+            f"    {Colors.error(' Error')}: Unknown file source type '{Colors.bold(file_source_type)}' for file source '{Colors.bold(file_source_id)}': {e}"
+        )
+        return False
+    except Exception as e:
+        print(
+            f"    {Colors.error(' Error')}: Failed to validate file source '{Colors.bold(file_source_id)}' ({file_source_type}): {e}"
+        )
+        if verbose:
+            traceback.print_exc()
+        return False
+
+
+def main():
+    args = parse_arguments()
+
+    print(Colors.bold("Galaxy File Sources Configuration Validator"))
+    print(Colors.bold("=" * 45))
+
+    # Find Galaxy configuration file
+    galaxy_config_path = find_galaxy_config(args.config_file)
+    if not galaxy_config_path:
+        print(Colors.error("Error: Could not find Galaxy configuration file"))
+        print("Try specifying it with --config-file option")
+        sys.exit(1)
+
+    print(f"Using Galaxy config: {Colors.info(galaxy_config_path)}")
+
+    # Load Galaxy configuration to get file sources config path
+    if args.file_sources_config:
+        file_sources_config_path = args.file_sources_config
+    else:
+        # Try to determine from Galaxy config
+        try:
+            # Simple approach: look for file_sources_config_file in galaxy.yml
+            with open(galaxy_config_path) as f:
+                galaxy_config = yaml.safe_load(f)
+
+            galaxy_section = galaxy_config.get("galaxy", {})
+            file_sources_config_file = galaxy_section.get("file_sources_config_file", "file_sources_conf.yml")
+
+            # Resolve relative to config directory
+            config_dir = os.path.dirname(galaxy_config_path)
+            file_sources_config_path = os.path.join(config_dir, file_sources_config_file)
+
+        except Exception as e:
+            print(Colors.error(f"Error reading Galaxy config: {e}"))
+            # Fall back to default
+            file_sources_config_path = "config/file_sources_conf.yml"
+
+    if not os.path.exists(file_sources_config_path):
+        print(Colors.error(f"Error: File sources config file not found: {file_sources_config_path}"))
+        sys.exit(1)
+
+    print(f"Using file sources config: {Colors.info(file_sources_config_path)}")
+    print()
+
+    # Load file sources configuration
+    file_sources_list = load_file_sources_config(file_sources_config_path)
+    print(f"Found {Colors.bold(str(len(file_sources_list)))} file source(s) to validate")
+
+    if not file_sources_list:
+        print(Colors.warning("No file sources found in configuration"))
+        sys.exit(0)
+
+    # Create plugin loader and file sources config
+    plugin_loader = FileSourcePluginLoader()
+    file_sources_config = FileSourcePluginsConfig()
+
+    # Validate each file source
+    total_count = len(file_sources_list)
+    valid_count = 0
+    error_count = 0
+
+    print()
+    for i, file_source_config in enumerate(file_sources_list, 1):
+        if args.verbose:
+            print(f"[{Colors.bold(f'{i}/{total_count}')}] Validating file source configuration...")
+
+        is_valid = validate_file_source_config(
+            file_source_config, plugin_loader, file_sources_config, verbose=args.verbose
+        )
+
+        if is_valid:
+            valid_count += 1
+        else:
+            error_count += 1
+            if args.fail_fast:
+                print(f"\n{Colors.warning('Failing fast due to --fail-fast option')}")
+                sys.exit(1)
+        if args.verbose:
+            print()
+
+    # Print summary
+    print()
+    print(Colors.bold("Validation Summary:"))
+    print(f"  Total file sources: {Colors.bold(str(total_count))}")
+    print(f"  Valid: {Colors.success(str(valid_count))}")
+    print(f"  Errors: {Colors.error(str(error_count))}")
+
+    if error_count > 0:
+        print(f"\n{Colors.error(' Validation failed')}: {error_count} file source(s) have configuration errors")
+        sys.exit(1)
+    else:
+        print(f"\n{Colors.success(' All file sources validated successfully!')}")
+        sys.exit(0)
+
+
+if __name__ == "__main__":
+    main()
diff --git a/lib/galaxy/managers/file_source_instances.py b/lib/galaxy/managers/file_source_instances.py
index bccef23f71e2..9de1b43556d0 100644
--- a/lib/galaxy/managers/file_source_instances.py
+++ b/lib/galaxy/managers/file_source_instances.py
@@ -34,7 +34,6 @@
 from galaxy.files.sources import (
     BaseFilesSource,
     file_source_type_is_browsable,
-    FilesSourceProperties,
     PluginKind,
     SupportsBrowsing,
 )
@@ -559,7 +558,7 @@ def _user_file_source(self, uri: str) -> Optional[UserFileSource]:
         user_object_store: UserFileSource = self._sa_session.query(UserFileSource).filter(index_filter).one()
         return user_object_store
 
-    def _file_source_properties_from_uri(self, uri: str) -> Optional[FilesSourceProperties]:
+    def _file_source_properties_from_uri(self, uri: str) -> Optional[dict[str, Any]]:
         user_file_source = self._user_file_source(uri)
         if not user_file_source:
             return None
@@ -567,7 +566,7 @@ def _file_source_properties_from_uri(self, uri: str) -> Optional[FilesSourceProp
             return None
         return self._file_source_properties(user_file_source)
 
-    def _file_source_properties(self, user_file_source: UserFileSource) -> FilesSourceProperties:
+    def _file_source_properties(self, user_file_source: UserFileSource) -> dict[str, Any]:
         secrets = recover_secrets(user_file_source, self._app_vault, self._app_config)
         environment = prepare_environment(user_file_source, self._app_vault, self._app_config)
         template_server_configuration = self.template_server_configuration(
@@ -605,20 +604,20 @@ def find_best_match(self, url: str) -> Optional[FileSourceScore]:
         file_source = self._file_source(files_source_properties)
         return FileSourceScore(file_source, len(url))
 
-    def _file_source(self, files_source_properties: FilesSourceProperties) -> BaseFilesSource:
-        plugin_source = plugin_source_from_dict([cast(dict[str, Any], files_source_properties)])
+    def _file_source(self, files_source_properties: dict[str, Any]) -> BaseFilesSource:
+        plugin_source = plugin_source_from_dict([files_source_properties])
         file_source = self._plugin_loader.load_plugins(
             plugin_source,
             self._file_sources_config,
         )[0]
         return file_source
 
-    def _all_user_file_source_properties(self, user_context: FileSourcesUserContext) -> list[FilesSourceProperties]:
+    def _all_user_file_source_properties(self, user_context: FileSourcesUserContext) -> list[dict[str, Any]]:
         username_filter = User.__table__.c.username == user_context.username
         user: Optional[User] = self._sa_session.query(User).filter(username_filter).one_or_none()
         if user is None:
             return []
-        all_file_source_properties: list[FilesSourceProperties] = []
+        all_file_source_properties: list[dict[str, Any]] = []
         for user_file_source in user.file_sources:
             if user_file_source.hidden:
                 continue
@@ -662,7 +661,7 @@ def user_file_sources_to_dicts(
         browsable_only: Optional[bool] = False,
         include_kind: Optional[set[PluginKind]] = None,
         exclude_kind: Optional[set[PluginKind]] = None,
-    ) -> list[FilesSourceProperties]:
+    ) -> list[dict[str, Any]]:
         """Write out user file sources as list of config dictionaries."""
         if user_context.anonymous:
             return []
@@ -691,8 +690,8 @@ def configuration_to_file_source_properties(
     label: str,
     doc: Optional[str],
     id: str,
-) -> FilesSourceProperties:
-    file_source_properties = cast(FilesSourceProperties, file_source_configuration.model_dump())
+) -> dict[str, Any]:
+    file_source_properties = file_source_configuration.model_dump()
     file_source_properties["label"] = label
     file_source_properties["doc"] = doc
     file_source_properties["id"] = id
diff --git a/lib/galaxy/managers/remote_files.py b/lib/galaxy/managers/remote_files.py
index c9e4aa210953..f02a24edaad0 100644
--- a/lib/galaxy/managers/remote_files.py
+++ b/lib/galaxy/managers/remote_files.py
@@ -1,9 +1,6 @@
 import hashlib
 import logging
-from operator import itemgetter
-from typing import (
-    Optional,
-)
+from typing import Optional
 
 from galaxy import exceptions
 from galaxy.files import (
@@ -11,10 +8,8 @@
     FileSourcePath,
     ProvidesFileSourcesUserContext,
 )
-from galaxy.files.sources import (
-    FilesSourceOptions,
-    PluginKind,
-)
+from galaxy.files.models import FilesSourceOptions
+from galaxy.files.sources import PluginKind
 from galaxy.managers.context import ProvidesUserContext
 from galaxy.schema.remote_files import (
     AnyRemoteFilesListResponse,
@@ -25,10 +20,7 @@
     RemoteFilesTarget,
 )
 from galaxy.structured_app import MinimalManagerApp
-from galaxy.util import (
-    jstree,
-    smart_str,
-)
+from galaxy.util import jstree
 
 log = logging.getLogger(__name__)
 
@@ -48,7 +40,7 @@ def index(
         format: Optional[RemoteFilesFormat],
         recursive: Optional[bool],
         disable: Optional[RemoteFilesDisableMode],
-        writeable: Optional[bool] = False,
+        write_intent: Optional[bool] = False,
         limit: Optional[int] = None,
         offset: Optional[int] = None,
         query: Optional[str] = None,
@@ -89,7 +81,7 @@ def index(
         file_source = file_source_path.file_source
 
         opts = FilesSourceOptions()
-        opts.writeable = writeable or False
+        opts.write_intent = write_intent or False
         try:
             index, count = file_source.list(
                 file_source_path.path,
@@ -110,17 +102,17 @@ def index(
             raise exceptions.InternalServerError(message)
         if format == RemoteFilesFormat.flat:
             # rip out directories, ensure sorted by path
-            index = [i for i in index if i["class"] == "File"]
-            index = sorted(index, key=itemgetter("path"))
+            index = [i for i in index if i.class_ == "File"]
+            index = sorted(index, key=lambda x: x.path)
         elif format == RemoteFilesFormat.jstree:
             if disable is None:
                 disable = RemoteFilesDisableMode.folders
 
             jstree_paths = []
             for ent in index:
-                path = ent["path"]
-                path_hash = hashlib.sha1(smart_str(path)).hexdigest()
-                if ent["class"] == "Directory":
+                path = ent.path
+                path_hash = hashlib.sha1(path.encode()).hexdigest()
+                if ent.class_ == "Directory":
                     path_type = "folder"
                     disabled = True if disable == RemoteFilesDisableMode.folders else False
                 else:
@@ -172,7 +164,7 @@ def create_entry(self, user_ctx: ProvidesUserContext, entry_data: CreateEntryPay
         file_source_path = self._file_sources.get_file_source_path(target)
         file_source = file_source_path.file_source
         try:
-            result = file_source.create_entry(entry_data.dict(), user_context=user_file_source_context)
+            result = file_source.create_entry(entry_data, user_context=user_file_source_context)
         except exceptions.MessageException:
             log.warning(f"Problem creating entry {entry_data.name} in file source {entry_data.target}", exc_info=True)
             raise
@@ -181,7 +173,7 @@ def create_entry(self, user_ctx: ProvidesUserContext, entry_data: CreateEntryPay
             log.warning(message, exc_info=True)
             raise exceptions.InternalServerError(message)
         return CreatedEntryResponse(
-            name=result["name"],
-            uri=result["uri"],
-            external_link=result.get("external_link", None),
+            name=result.name,
+            uri=result.uri,
+            external_link=result.external_link,
         )
diff --git a/lib/galaxy/schema/__init__.py b/lib/galaxy/schema/__init__.py
index de8303ca6f91..7a1c2f7a20dc 100644
--- a/lib/galaxy/schema/__init__.py
+++ b/lib/galaxy/schema/__init__.py
@@ -1,21 +1,14 @@
-from collections.abc import Iterable
-from copy import deepcopy
 from datetime import datetime
 from enum import Enum
 from typing import (
-    Any,
-    Callable,
     Optional,
-    TypeVar,
     Union,
 )
 
 from pydantic import (
     BaseModel,
-    create_model,
     Field,
 )
-from pydantic.fields import FieldInfo
 
 
 class BootstrapAdminUser(BaseModel):
@@ -116,43 +109,3 @@ class PdfDocumentType(str, Enum):
 class APIKeyModel(BaseModel):
     key: str = Field(..., title="Key", description="API key to interact with the Galaxy API")
     create_time: datetime = Field(..., title="Create Time", description="The time and date this API key was created.")
-
-
-T = TypeVar("T", bound="BaseModel")
-
-
-# TODO: This is a workaround to make all fields optional.
-#       It should be removed when Python/pydantic supports this feature natively.
-# https://github.com/pydantic/pydantic/issues/1673
-def partial_model(
-    include: Optional[list[str]] = None, exclude: Optional[list[str]] = None
-) -> Callable[[type[T]], type[T]]:
-    """Decorator to make all model fields optional"""
-
-    if exclude is None:
-        exclude = []
-
-    def decorator(model: type[T]) -> type[T]:
-        def make_optional(field: FieldInfo, default: Any = None) -> tuple[Any, FieldInfo]:
-            new = deepcopy(field)
-            new.default = default
-            new.annotation = Optional[field.annotation or Any]  # type:ignore[assignment]
-            return new.annotation, new
-
-        if include is None:
-            fields: Iterable[tuple[str, FieldInfo]] = model.model_fields.items()
-        else:
-            fields = ((k, v) for k, v in model.model_fields.items() if k in include)
-
-        return create_model(
-            model.__name__,
-            __base__=model,
-            __module__=model.__module__,
-            **{
-                field_name: make_optional(field_info)
-                for field_name, field_info in fields
-                if exclude is None or field_name not in exclude
-            },
-        )  # type:ignore[call-overload]
-
-    return decorator
diff --git a/lib/galaxy/schema/groups.py b/lib/galaxy/schema/groups.py
index e1acfd235fa4..867841bce927 100644
--- a/lib/galaxy/schema/groups.py
+++ b/lib/galaxy/schema/groups.py
@@ -1,6 +1,4 @@
-from typing import (
-    Optional,
-)
+from typing import Optional
 
 from pydantic import (
     Field,
@@ -8,7 +6,6 @@
 )
 from typing_extensions import Literal
 
-from galaxy.schema import partial_model
 from galaxy.schema.fields import (
     DecodedDatabaseIdField,
     EncodedDatabaseIdField,
@@ -18,6 +15,7 @@
     Model,
     WithModelClass,
 )
+from galaxy.util.config_templates import partial_model
 
 GROUP_MODEL_CLASS = Literal["Group"]
 
diff --git a/lib/galaxy/schema/schema.py b/lib/galaxy/schema/schema.py
index 6791febf38ce..3bfb0b4795cf 100644
--- a/lib/galaxy/schema/schema.py
+++ b/lib/galaxy/schema/schema.py
@@ -33,7 +33,6 @@
     TypedDict,
 )
 
-from galaxy.schema import partial_model
 from galaxy.schema.bco import XrefItem
 from galaxy.schema.fields import (
     DecodedDatabaseIdField,
@@ -49,6 +48,7 @@
     RelativeUrl,
 )
 from galaxy.tool_util_models.tool_source import FieldDict
+from galaxy.util.config_templates import partial_model
 from galaxy.util.hash_util import HashFunctionNameEnum
 from galaxy.util.sanitize_html import sanitize_html
 
diff --git a/lib/galaxy/util/config_templates.py b/lib/galaxy/util/config_templates.py
index f193d762d20e..12695d50cda9 100644
--- a/lib/galaxy/util/config_templates.py
+++ b/lib/galaxy/util/config_templates.py
@@ -5,8 +5,11 @@
 
 import logging
 import os
+from collections.abc import Iterable
+from copy import deepcopy
 from typing import (
     Any,
+    Callable,
     cast,
     Dict,
     List,
@@ -24,9 +27,11 @@
 from pydantic import (
     BaseModel,
     ConfigDict,
+    create_model,
     RootModel,
     ValidationError,
 )
+from pydantic.fields import FieldInfo
 from typing_extensions import (
     Literal,
     NotRequired,
@@ -608,3 +613,43 @@ def read_oauth2_info_from_configuration(
 # but injected dynamically. Currently only `oauth2_access_token`.
 class ImplicitConfigurationParameters(TypedDict):
     oauth2_access_token: NotRequired[str]
+
+
+M = TypeVar("M", bound="BaseModel")
+
+
+# TODO: This is a workaround to make all fields optional.
+#       It should be removed when Python/pydantic supports this feature natively.
+# https://github.com/pydantic/pydantic/issues/1673
+def partial_model(
+    include: Optional[List[str]] = None, exclude: Optional[List[str]] = None
+) -> Callable[[Type[M]], Type[M]]:
+    """Decorator to make all model fields optional"""
+
+    if exclude is None:
+        exclude = []
+
+    def decorator(model: Type[M]) -> Type[M]:
+        def make_optional(field: FieldInfo, default: Any = None) -> tuple[Any, FieldInfo]:
+            new = deepcopy(field)
+            new.default = default
+            new.annotation = Optional[field.annotation or Any]  # type:ignore[assignment]
+            return new.annotation, new
+
+        if include is None:
+            fields: Iterable[tuple[str, FieldInfo]] = model.model_fields.items()
+        else:
+            fields = ((k, v) for k, v in model.model_fields.items() if k in include)
+
+        return create_model(
+            model.__name__,
+            __base__=model,
+            __module__=model.__module__,
+            **{
+                field_name: make_optional(field_info)
+                for field_name, field_info in fields
+                if exclude is None or field_name not in exclude
+            },
+        )  # type:ignore[call-overload]
+
+    return decorator
diff --git a/lib/galaxy/util/plugin_config.py b/lib/galaxy/util/plugin_config.py
index 2c1327dbca68..83546d20a8ae 100644
--- a/lib/galaxy/util/plugin_config.py
+++ b/lib/galaxy/util/plugin_config.py
@@ -92,13 +92,32 @@ def __load_plugins_from_element(
             template = "Failed to find plugin of type [%s] in available plugin types %s"
             message = template % (plugin_type, str(plugins_dict.keys()))
             raise Exception(message)
-
-        plugin = plugin_klazz(**plugin_kwds)
+        plugin = __create_plugin_instance(plugin_klazz, plugin_kwds)
         plugins.append(plugin)
 
     return plugins
 
 
+def __as_configurable_plugin_instance(obj: Any) -> Optional[Type]:
+    """Check if the class implements the configurable plugin pattern."""
+    try:
+        if isinstance(obj, type) and hasattr(obj, "build_template_config"):
+            return obj
+    except TypeError:
+        pass
+    return None
+
+
+def __create_plugin_instance(plugin_class: Type[T], plugin_kwds: Dict[str, Any]) -> T:
+    """Create an instance of the plugin class with the provided keyword arguments."""
+    configurable_instance = __as_configurable_plugin_instance(plugin_class)
+    if configurable_instance:
+        plugin_template_config = configurable_instance.build_template_config(**plugin_kwds)
+        return configurable_instance(template_config=plugin_template_config)
+    else:
+        return plugin_class(**plugin_kwds)
+
+
 def __load_plugins_from_dicts(
     plugins_dict: Dict[str, Type[T]],
     configs: PluginConfigsT,
@@ -129,7 +148,8 @@ def __load_plugins_from_dicts(
         if extra_kwds:
             plugin_kwds = plugin_kwds.copy()
             plugin_kwds.update(extra_kwds)
-        plugin = plugins_dict[plugin_type](**plugin_kwds)
+        plugin_class = plugins_dict[plugin_type]
+        plugin = __create_plugin_instance(plugin_class, plugin_kwds)
         plugins.append(plugin)
 
     return plugins
diff --git a/lib/galaxy/webapps/galaxy/api/remote_files.py b/lib/galaxy/webapps/galaxy/api/remote_files.py
index b8e8e22ebba0..75059d5cbdf6 100644
--- a/lib/galaxy/webapps/galaxy/api/remote_files.py
+++ b/lib/galaxy/webapps/galaxy/api/remote_files.py
@@ -66,8 +66,8 @@
     ),
 )
 
-WriteableQueryParam: Optional[bool] = Query(
-    title="Writeable",
+WriteIntentQueryParam: Optional[bool] = Query(
+    title="Write Intent",
     description=(
         "Whether the query is made with the intention of writing to the source."
         " If set to True, only entries that can be written to will be returned."
@@ -135,7 +135,10 @@ def index(
         format: Annotated[Optional[RemoteFilesFormat], FormatQueryParam] = RemoteFilesFormat.uri,
         recursive: Annotated[Optional[bool], RecursiveQueryParam] = None,
         disable: Annotated[Optional[RemoteFilesDisableMode], DisableModeQueryParam] = None,
-        writeable: Annotated[Optional[bool], WriteableQueryParam] = None,
+        writeable: Annotated[
+            Optional[bool], Query(description="Deprecated, please use `write_intent` instead.", deprecated=True)
+        ] = None,
+        write_intent: Annotated[Optional[bool], WriteIntentQueryParam] = None,
         limit: Annotated[Optional[int], LimitQueryParam] = None,
         offset: Annotated[Optional[int], OffsetQueryParam] = None,
         query: Annotated[Optional[str], SearchQueryParam] = None,
@@ -146,7 +149,7 @@ def index(
         The total count of files and directories is returned in the 'total_matches' header.
         """
         result, count = self.manager.index(
-            user_ctx, target, format, recursive, disable, writeable, limit, offset, query, sort_by
+            user_ctx, target, format, recursive, disable, write_intent or writeable, limit, offset, query, sort_by
         )
         response.headers["total_matches"] = str(count)
         return result
diff --git a/packages/files/setup.cfg b/packages/files/setup.cfg
index 9f5da027a1d5..98325cefc088 100644
--- a/packages/files/setup.cfg
+++ b/packages/files/setup.cfg
@@ -38,6 +38,10 @@ install_requires =
 packages = find:
 python_requires = >=3.9
 
+[options.entry_points]
+console_scripts =
+        galaxy-file-sources-conf-validation = galaxy.files.validate.script:main
+
 [options.packages.find]
 exclude =
     tests*
diff --git a/test/unit/data/test_dataset_materialization.py b/test/unit/data/test_dataset_materialization.py
index af02fc1fe3ce..3a2764b997bc 100644
--- a/test/unit/data/test_dataset_materialization.py
+++ b/test/unit/data/test_dataset_materialization.py
@@ -357,7 +357,7 @@ def test_deferred_hdas_basic_attached_file_sources(tmpdir):
     root.mkdir()
     content_path = root / "2.bed"
     content_path.write_text(CONTENTS_2_BED, encoding="utf-8")
-    file_sources = TestPosixConfiguredFileSources(root)
+    file_sources = TestPosixConfiguredFileSources(str(root))
     fixture_context = setup_fixture_context_with_history()
     store_dict = deferred_hda_model_store_dict(
         source_uri="gxfiles://test1/2.bed",
diff --git a/test/unit/files/_util.py b/test/unit/files/_util.py
index 0a4a77ee14ff..5fbb9e3ff4c8 100644
--- a/test/unit/files/_util.py
+++ b/test/unit/files/_util.py
@@ -10,6 +10,7 @@
     DictFileSourcesUserContext,
     OptionalUserContext,
 )
+from galaxy.files.models import AnyRemoteEntry
 from galaxy.files.plugins import FileSourcePluginsConfig
 
 TEST_USERNAME = "alice"
@@ -22,15 +23,15 @@ def serialize_and_recover(file_sources_o: ConfiguredFileSources, user_context: O
     return file_sources
 
 
-def find_file_a(dir_list):
+def find_file_a(dir_list: list[AnyRemoteEntry]) -> Optional[AnyRemoteEntry]:
     return find(dir_list, class_="File", name="a")
 
 
-def find(dir_list, class_=None, name=None):
+def find(dir_list: list[AnyRemoteEntry], class_=None, name=None) -> Optional[AnyRemoteEntry]:
     for ent in dir_list:
-        if class_ is not None and ent["class"] != class_:
+        if class_ is not None and ent.class_ != class_:
             continue
-        if name is not None and ent["name"] == name:
+        if name is not None and ent.name == name:
             return ent
 
     return None
diff --git a/test/unit/files/test_basespace.py b/test/unit/files/test_basespace.py
index ee79209e1969..179fc3855d40 100644
--- a/test/unit/files/test_basespace.py
+++ b/test/unit/files/test_basespace.py
@@ -34,4 +34,4 @@ def test_file_source():
     a_file = find(res, class_="File", name=os.path.basename(test_file))
     assert a_file
 
-    assert_realizes_as(file_sources, a_file["uri"], "a\n", user_context=user_context)
+    assert_realizes_as(file_sources, a_file.uri, "a\n", user_context=user_context)
diff --git a/test/unit/files/test_posix.py b/test/unit/files/test_posix.py
index 09e3b7a70dde..3bf3d5f560b8 100644
--- a/test/unit/files/test_posix.py
+++ b/test/unit/files/test_posix.py
@@ -1,8 +1,6 @@
 import os
 import tempfile
-from typing import (
-    Any,
-)
+from typing import Any
 
 import pytest
 
@@ -47,29 +45,32 @@ def test_posix():
     res = list_root(file_sources, "gxfiles://test1", recursive=False)
     file_a = find_file_a(res)
     assert file_a
-    assert file_a["uri"] == "gxfiles://test1/a"
-    assert file_a["name"] == "a"
+    assert file_a.uri == "gxfiles://test1/a"
+    assert file_a.name == "a"
 
     subdir1 = find(res, name="subdir1")
-    assert subdir1["class"] == "Directory"
-    assert subdir1["uri"] == "gxfiles://test1/subdir1"
+    assert subdir1
+    assert subdir1.class_ == "Directory"
+    assert subdir1.uri == "gxfiles://test1/subdir1"
 
     res = list_dir(file_sources, "gxfiles://test1/subdir1", recursive=False)
     subdir2 = find(res, name="subdir2")
     assert subdir2, res
-    assert subdir2["uri"] == "gxfiles://test1/subdir1/subdir2"
+    assert subdir2.uri == "gxfiles://test1/subdir1/subdir2"
 
     file_c = find(res, name="c")
     assert file_c, res
-    assert file_c["uri"] == "gxfiles://test1/subdir1/c"
+    assert file_c.uri == "gxfiles://test1/subdir1/c"
 
     res = list_root(file_sources, "gxfiles://test1", recursive=True)
     subdir1 = find(res, name="subdir1")
     subdir2 = find(res, name="subdir2")
-    assert subdir1["class"] == "Directory"
-    assert subdir1["uri"] == "gxfiles://test1/subdir1"
-    assert subdir2["uri"] == "gxfiles://test1/subdir1/subdir2"
-    assert subdir2["class"] == "Directory"
+    assert subdir1
+    assert subdir1.class_ == "Directory"
+    assert subdir1.uri == "gxfiles://test1/subdir1"
+    assert subdir2
+    assert subdir2.uri == "gxfiles://test1/subdir1/subdir2"
+    assert subdir2.class_ == "Directory"
 
 
 def test_posix_link_security():
@@ -158,6 +159,7 @@ def test_user_ftp_explicit_config():
         ftp_upload_purge=False,
     )
     plugin = {
+        "id": "_ftp",
         "type": "gxftp",
     }
     tmp, root = setup_root()
@@ -232,6 +234,7 @@ def test_import_dir_explicit_config():
         library_import_dir=root,
     )
     plugin = {
+        "id": "test-gximport",
         "type": "gximport",
     }
     file_sources = ConfiguredFileSources(file_sources_config, ConfiguredFileSourcesConf(conf_dict=[plugin]))
diff --git a/test/unit/files/test_temp.py b/test/unit/files/test_temp.py
index 2d6d56d12a0a..5a42142bb008 100644
--- a/test/unit/files/test_temp.py
+++ b/test/unit/files/test_temp.py
@@ -4,6 +4,7 @@
 
 from galaxy.exceptions import RequestParameterInvalidException
 from galaxy.files.plugins import FileSourcePluginsConfig
+from galaxy.files.sources import BaseFilesSource
 from galaxy.files.sources.temp import TempFilesSource
 from galaxy.files.unittest_utils import (
     setup_root,
@@ -30,7 +31,7 @@ def file_sources() -> TestConfiguredFileSources:
 
 
 @pytest.fixture(scope="session")
-def temp_file_source(file_sources: TestConfiguredFileSources) -> TempFilesSource:
+def temp_file_source(file_sources: TestConfiguredFileSources) -> BaseFilesSource:
     file_source_pair = file_sources.get_file_source_path(ROOT_URI)
     file_source = file_source_pair.file_source
     return file_source
@@ -45,17 +46,17 @@ def test_file_source(file_sources: TestConfiguredFileSources):
     assert_realizes_contains(file_sources, f"{ROOT_URI}/dir1/sub1/f", "f")
 
 
-def test_list(temp_file_source: TempFilesSource):
+def test_list(temp_file_source: BaseFilesSource):
     assert_list_names(temp_file_source, "/", recursive=False, expected_names=["a", "b", "c", "dir1"])
     assert_list_names(temp_file_source, "/dir1", recursive=False, expected_names=["d", "e", "sub1"])
 
 
-def test_list_recursive(temp_file_source: TempFilesSource):
+def test_list_recursive(temp_file_source: BaseFilesSource):
     expected_names = ["a", "b", "c", "dir1", "d", "e", "sub1", "f"]
     assert_list_names(temp_file_source, "/", recursive=True, expected_names=expected_names)
 
 
-def test_pagination(temp_file_source: TempFilesSource):
+def test_pagination(temp_file_source: BaseFilesSource):
     # Pagination is only supported for non-recursive listings.
     recursive = False
     root_lvl_entries, count = temp_file_source.list("/", recursive=recursive)
@@ -90,7 +91,7 @@ def test_pagination(temp_file_source: TempFilesSource):
     assert result[2] == root_lvl_entries[3]
 
 
-def test_search(temp_file_source: TempFilesSource):
+def test_search(temp_file_source: BaseFilesSource):
     # Search is only supported for non-recursive listings.
     recursive = False
     root_lvl_entries, count = temp_file_source.list("/", recursive=recursive)
@@ -100,24 +101,24 @@ def test_search(temp_file_source: TempFilesSource):
     result, count = temp_file_source.list("/", recursive=recursive, query="a")
     assert count == 1
     assert len(result) == 1
-    assert result[0]["name"] == "a"
+    assert result[0].name == "a"
 
     result, count = temp_file_source.list("/", recursive=recursive, query="b")
     assert count == 1
     assert len(result) == 1
-    assert result[0]["name"] == "b"
+    assert result[0].name == "b"
 
     result, count = temp_file_source.list("/", recursive=recursive, query="c")
     assert count == 1
     assert len(result) == 1
-    assert result[0]["name"] == "c"
+    assert result[0].name == "c"
 
     # Searching for 'd' at root level should return the directory 'dir1' but not the file 'd'
     # as it is not a direct child of the root.
     result, count = temp_file_source.list("/", recursive=recursive, query="d")
     assert count == 1
     assert len(result) == 1
-    assert result[0]["name"] == "dir1"
+    assert result[0].name == "dir1"
 
     # Searching for 'e' at root level should not return anything.
     result, count = temp_file_source.list("/", recursive=recursive, query="e")
@@ -127,10 +128,10 @@ def test_search(temp_file_source: TempFilesSource):
     result, count = temp_file_source.list("/dir1", recursive=recursive, query="e")
     assert count == 1
     assert len(result) == 1
-    assert result[0]["name"] == "e"
+    assert result[0].name == "e"
 
 
-def test_query_with_empty_string(temp_file_source: TempFilesSource):
+def test_query_with_empty_string(temp_file_source: BaseFilesSource):
     recursive = False
     root_lvl_entries, count = temp_file_source.list("/", recursive=recursive)
     assert count == 4
@@ -142,7 +143,7 @@ def test_query_with_empty_string(temp_file_source: TempFilesSource):
     assert result == root_lvl_entries
 
 
-def test_pagination_not_supported_raises(temp_file_source: TempFilesSource):
+def test_pagination_not_supported_raises(temp_file_source: BaseFilesSource):
     TempFilesSource.supports_pagination = False
     recursive = False
     with pytest.raises(RequestParameterInvalidException) as exc_info:
@@ -151,7 +152,7 @@ def test_pagination_not_supported_raises(temp_file_source: TempFilesSource):
     TempFilesSource.supports_pagination = True
 
 
-def test_pagination_parameters_non_negative(temp_file_source: TempFilesSource):
+def test_pagination_parameters_non_negative(temp_file_source: BaseFilesSource):
     recursive = False
     with pytest.raises(RequestParameterInvalidException) as exc_info:
         temp_file_source.list("/", recursive=recursive, limit=-1, offset=0)
@@ -166,7 +167,7 @@ def test_pagination_parameters_non_negative(temp_file_source: TempFilesSource):
     assert "Offset must be greater than or equal to 0" in str(exc_info.value)
 
 
-def test_search_not_supported_raises(temp_file_source: TempFilesSource):
+def test_search_not_supported_raises(temp_file_source: BaseFilesSource):
     TempFilesSource.supports_search = False
     recursive = False
     with pytest.raises(RequestParameterInvalidException) as exc_info:
@@ -175,14 +176,14 @@ def test_search_not_supported_raises(temp_file_source: TempFilesSource):
     TempFilesSource.supports_search = True
 
 
-def test_sorting_not_supported_raises(temp_file_source: TempFilesSource):
+def test_sorting_not_supported_raises(temp_file_source: BaseFilesSource):
     recursive = False
     with pytest.raises(RequestParameterInvalidException) as exc_info:
         temp_file_source.list("/", recursive=recursive, sort_by="name")
     assert "Server-side sorting is not supported by this file source" in str(exc_info.value)
 
 
-def _populate_test_scenario(file_source: TempFilesSource):
+def _populate_test_scenario(file_source: BaseFilesSource):
     """Create a directory structure in the file source."""
     user_context = user_context_fixture()
 
@@ -194,17 +195,17 @@ def _populate_test_scenario(file_source: TempFilesSource):
     _upload_to(file_source, "/dir1/sub1/f", content="f", user_context=user_context)
 
 
-def _upload_to(file_source: TempFilesSource, target_uri: str, content: str, user_context=None):
+def _upload_to(file_source: BaseFilesSource, target_uri: str, content: str, user_context=None):
     with tempfile.NamedTemporaryFile(mode="w") as f:
         f.write(content)
         f.flush()
         file_source.write_from(target_uri, f.name, user_context=user_context)
 
 
-def assert_list_names(file_source: TempFilesSource, uri: str, recursive: bool, expected_names: list[str]):
+def assert_list_names(file_source: BaseFilesSource, uri: str, recursive: bool, expected_names: list[str]):
     result, count = file_source.list(uri, recursive=recursive)
     assert count == len(expected_names)
-    assert sorted([entry["name"] for entry in result]) == sorted(expected_names)
+    assert sorted([entry.name for entry in result]) == sorted(expected_names)
     return result
 
 
diff --git a/test/unit/files/test_webdav.py b/test/unit/files/test_webdav.py
index 9294881ec247..c66df98435cd 100644
--- a/test/unit/files/test_webdav.py
+++ b/test/unit/files/test_webdav.py
@@ -5,6 +5,8 @@
 import pytest
 
 from galaxy.files.plugins import FileSourcePluginsConfig
+from galaxy.files.sources import BaseFilesSource
+from galaxy.files.sources.webdav import WebDavFilesSource
 from ._util import (
     configured_file_sources,
     find,
@@ -34,26 +36,27 @@ def test_file_source():
     res, _ = file_source.list("/", recursive=True)
     a_file = find_file_a(res)
     assert a_file
-    assert a_file["uri"] == "gxfiles://test1/a", a_file
+    assert a_file.uri == "gxfiles://test1/a", a_file
 
     res, _ = file_source.list("/", recursive=False)
     file_a = find_file_a(res)
     assert file_a
-    assert file_a["uri"] == "gxfiles://test1/a"
-    assert file_a["name"] == "a"
+    assert file_a.uri == "gxfiles://test1/a"
+    assert file_a.name == "a"
 
     subdir1 = find(res, name="subdir1")
-    assert subdir1["class"] == "Directory"
-    assert subdir1["uri"] == "gxfiles://test1/subdir1"
+    assert subdir1
+    assert subdir1.class_ == "Directory"
+    assert subdir1.uri == "gxfiles://test1/subdir1"
 
     res = list_dir(file_sources, "gxfiles://test1/subdir1", recursive=False)
     subdir2 = find(res, name="subdir2")
     assert subdir2, res
-    assert subdir2["uri"] == "gxfiles://test1/subdir1/subdir2"
+    assert subdir2.uri == "gxfiles://test1/subdir1/subdir2"
 
     file_c = find(res, name="c")
     assert file_c, res
-    assert file_c["uri"] == "gxfiles://test1/subdir1/c"
+    assert file_c.uri == "gxfiles://test1/subdir1/c"
 
 
 @skip_if_no_webdav
@@ -82,22 +85,28 @@ def test_serialization():
 @skip_if_no_webdav
 def test_config_options():
     file_sources = configured_file_sources(FILE_SOURCES_CONF)
-    fs = file_sources._file_sources[0]
-    user_context = user_context_fixture()
-    assert fs._open_fs(user_context).use_temp_files
+    fs = file_source_as_webdav(file_sources._file_sources[0])
+    assert fs.template_config.use_temp_files
+    assert fs._get_runtime_context().config.use_temp_files == fs.template_config.use_temp_files
 
     file_sources = configured_file_sources(FILE_SOURCES_CONF_NO_USE_TEMP_FILES)
-    fs = file_sources._file_sources[0]
-    user_context = user_context_fixture()
-    assert not fs._open_fs(user_context).use_temp_files
+    fs = file_source_as_webdav(file_sources._file_sources[0])
+    assert not fs.template_config.use_temp_files
+    assert fs._get_runtime_context().config.use_temp_files == fs.template_config.use_temp_files
 
     disable_default_use_temp = FileSourcePluginsConfig(
         webdav_use_temp_files=False,
     )
     file_sources = configured_file_sources(FILE_SOURCES_CONF, disable_default_use_temp)
-    fs = file_sources._file_sources[0]
-    user_context = user_context_fixture()
-    assert not fs._open_fs(user_context).use_temp_files
+    fs = file_source_as_webdav(file_sources._file_sources[0])
+    assert not fs.template_config.use_temp_files
+    assert fs._get_runtime_context().config.use_temp_files == fs.template_config.use_temp_files
+
+
+def file_source_as_webdav(file_source: BaseFilesSource) -> WebDavFilesSource:
+    if not isinstance(file_source, WebDavFilesSource):
+        raise TypeError(f"Expected WebDavFilesSource, got {type(file_source)}")
+    return file_source
 
 
 @skip_if_no_webdav
