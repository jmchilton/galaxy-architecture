diff --git a/client/src/schema/schema.ts b/client/src/schema/schema.ts
index ed9bc4fd31d5..df77c61c6352 100644
--- a/client/src/schema/schema.ts
+++ b/client/src/schema/schema.ts
@@ -520,7 +520,7 @@ export interface paths {
     };
     "/api/histories/{history_id}/contents/{id}": {
         /**
-         * Return detailed information about an HDA within a history.
+         * Return detailed information about an HDA within a history. ``/api/histories/{history_id}/contents/{type}s/{id}`` should be used instead.
          * @deprecated
          * @description Return detailed information about an `HDA` or `HDCA` within a history.
          *
@@ -528,7 +528,7 @@ export interface paths {
          */
         get: operations["history_content_api_histories__history_id__contents__id__get"];
         /**
-         * Updates the values for the history content item with the given ``ID``.
+         * Updates the values for the history content item with the given ``ID``. ``/api/histories/{history_id}/contents/{type}s/{id}`` should be used instead.
          * @deprecated
          * @description Updates the values for the history content item with the given ``ID``.
          */
@@ -3083,7 +3083,7 @@ export interface components {
              * @description The URI root used by this type of plugin.
              * @example gximport://
              */
-            uri_root: string;
+            uri_root?: string;
             /**
              * Writeable
              * @description Whether this files source plugin allows write access.
@@ -3097,6 +3097,7 @@ export interface components {
          * @default []
          * @example [
          *   {
+         *     "browsable": true,
          *     "doc": "Galaxy's library import directory",
          *     "id": "_import",
          *     "label": "Library Import Directory",
@@ -10605,7 +10606,7 @@ export interface operations {
     };
     history_content_api_histories__history_id__contents__id__get: {
         /**
-         * Return detailed information about an HDA within a history.
+         * Return detailed information about an HDA within a history. ``/api/histories/{history_id}/contents/{type}s/{id}`` should be used instead.
          * @deprecated
          * @description Return detailed information about an `HDA` or `HDCA` within a history.
          *
@@ -10658,7 +10659,7 @@ export interface operations {
     };
     update_api_histories__history_id__contents__id__put: {
         /**
-         * Updates the values for the history content item with the given ``ID``.
+         * Updates the values for the history content item with the given ``ID``. ``/api/histories/{history_id}/contents/{type}s/{id}`` should be used instead.
          * @deprecated
          * @description Updates the values for the history content item with the given ``ID``.
          */
@@ -13215,6 +13216,10 @@ export interface operations {
          * @description Display plugin information for each of the gxfiles:// URI targets available.
          */
         parameters?: {
+            /** @description Whether to return browsable filesources only. The default is `True`, which will omit filesourceslike `http` and `base64` that do not implement a list method. */
+            query?: {
+                browsable_only?: boolean;
+            };
             /** @description The user ID that will be used to effectively make this API call. Only admins and designated users can make API calls on behalf of other users. */
             header?: {
                 "run-as"?: string;
diff --git a/lib/galaxy/config/sample/file_sources_conf.yml.sample b/lib/galaxy/config/sample/file_sources_conf.yml.sample
index 3485d0fc0f16..8f5e7ee9521e 100644
--- a/lib/galaxy/config/sample/file_sources_conf.yml.sample
+++ b/lib/galaxy/config/sample/file_sources_conf.yml.sample
@@ -161,4 +161,3 @@
   doc: The Copernicus DEM is a Digital Surface Model (DSM) which represents the surface of the Earth including buildings, infrastructure and vegetation.
   bucket: copernicus-dem-30m
   anon: true
-
diff --git a/lib/galaxy/dependencies/__init__.py b/lib/galaxy/dependencies/__init__.py
index 89934921209d..79526e490304 100644
--- a/lib/galaxy/dependencies/__init__.py
+++ b/lib/galaxy/dependencies/__init__.py
@@ -245,10 +245,6 @@ def check_fs_dropboxfs(self):
     def check_fs_webdavfs(self):
         return "webdav" in self.file_sources
 
-    def check_fs_s3fs(self):
-        # pyfilesystem plugin access to s3
-        return "s3" in self.file_sources
-
     def check_fs_anvilfs(self):
         # pyfilesystem plugin access to terra on anvil
         return "anvil" in self.file_sources
@@ -256,10 +252,6 @@ def check_fs_anvilfs(self):
     def check_fs_sshfs(self):
         return "ssh" in self.file_sources
 
-    def check_s3fs(self):
-        # use s3fs directly (skipping pyfilesystem) for direct access to more options
-        return "s3fs" in self.file_sources
-
     def check_fs_googledrivefs(self):
         return "googledrive" in self.file_sources
 
diff --git a/lib/galaxy/dependencies/conditional-requirements.txt b/lib/galaxy/dependencies/conditional-requirements.txt
index 0c2e60d96b03..6a11581df6e5 100644
--- a/lib/galaxy/dependencies/conditional-requirements.txt
+++ b/lib/galaxy/dependencies/conditional-requirements.txt
@@ -19,8 +19,6 @@ total-perspective-vortex<3
 fs.webdavfs>=0.4.2  # type: webdav
 fs.dropboxfs  # type: dropbox
 fs.sshfs  # type: ssh
-fs-s3fs  # type: s3
-s3fs  # type: s3fs
 fs.anvilfs # type: anvil
 fs.googledrivefs # type: googledrive
 fs-gcsfs # type: googlecloudstorage
diff --git a/lib/galaxy/dependencies/pinned-requirements.txt b/lib/galaxy/dependencies/pinned-requirements.txt
index 31c26a48ff34..04d76098bb87 100644
--- a/lib/galaxy/dependencies/pinned-requirements.txt
+++ b/lib/galaxy/dependencies/pinned-requirements.txt
@@ -2,8 +2,10 @@
 
 a2wsgi==1.6.0 ; python_version >= "3.7" and python_version < "3.12"
 adal==1.2.7 ; python_version >= "3.7" and python_version < "3.12"
+aiobotocore==2.4.2 ; python_version >= "3.7" and python_version < "3.12"
 aiofiles==22.1.0 ; python_version >= "3.7" and python_version < "3.12"
 aiohttp==3.8.3 ; python_version >= "3.7" and python_version < "3.12"
+aioitertools==0.11.0 ; python_version >= "3.7" and python_version < "3.12"
 aiosignal==1.3.1 ; python_version >= "3.7" and python_version < "3.12"
 alembic-utils==0.7.8 ; python_version >= "3.7" and python_version < "3.12"
 alembic==1.9.2 ; python_version >= "3.7" and python_version < "3.12"
@@ -28,6 +30,7 @@ bioblend==1.0.0 ; python_version >= "3.7" and python_version < "3.12"
 bleach==6.0.0 ; python_version >= "3.7" and python_version < "3.12"
 boltons==21.0.0 ; python_version >= "3.7" and python_version < "3.12"
 boto==2.49.0 ; python_version >= "3.7" and python_version < "3.12"
+botocore==1.27.59 ; python_version >= "3.7" and python_version < "3.12"
 bx-python==0.9.0 ; python_version >= "3.7" and python_version < "3.12"
 cachecontrol==0.12.11 ; python_version >= "3.7" and python_version < "3.12"
 cachecontrol[filecache]==0.12.11 ; python_version >= "3.7" and python_version < "3.12"
@@ -65,6 +68,7 @@ fastapi==0.89.1 ; python_version >= "3.7" and python_version < "3.12"
 flupy==1.2.0 ; python_version >= "3.7" and python_version < "3.12"
 frozenlist==1.3.3 ; python_version >= "3.7" and python_version < "3.12"
 fs==2.4.16 ; python_version >= "3.7" and python_version < "3.12"
+fsspec==2023.1.0 ; python_version >= "3.7" and python_version < "3.12"
 funcsigs==1.0.2 ; python_version >= "3.7" and python_version < "3.12"
 future==0.18.3 ; python_version >= "3.7" and python_version < "3.12"
 galaxy-sequence-utils==1.1.5 ; python_version >= "3.7" and python_version < "3.12"
@@ -82,6 +86,7 @@ importlib-resources==5.10.2 ; python_version >= "3.7" and python_version < "3.12
 isa-rwval==0.10.10 ; python_version >= "3.7" and python_version < "3.12"
 isodate==0.6.1 ; python_version >= "3.7" and python_version < "3.12"
 jinja2==3.1.2 ; python_version >= "3.7" and python_version < "3.12"
+jmespath==1.0.1 ; python_version >= "3.7" and python_version < "3.12"
 jsonref==1.1.0 ; python_version >= "3.7" and python_version < "3.12"
 jsonschema==4.17.3 ; python_version >= "3.7" and python_version < "3.12"
 kombu==5.2.4 ; python_version >= "3.7" and python_version < "3.12"
@@ -158,6 +163,7 @@ routes==2.5.1 ; python_version >= "3.7" and python_version < "3.12"
 rsa==4.9 ; python_version >= "3.7" and python_version < "3.12"
 ruamel-yaml-clib==0.2.7 ; platform_python_implementation == "CPython" and python_version < "3.11" and python_version >= "3.7"
 ruamel-yaml==0.17.21 ; python_version < "3.12" and python_version >= "3.7"
+s3fs==2023.1.0 ; python_version >= "3.7" and python_version < "3.12"
 schema-salad==8.4.20230201194352 ; python_version >= "3.7" and python_version < "3.12"
 setuptools-scm==5.0.2 ; python_version >= "3.7" and python_version < "3.12"
 setuptools==67.1.0 ; python_version >= "3.7" and python_version < "3.12"
@@ -193,6 +199,7 @@ wcwidth==0.2.6 ; python_version >= "3.7" and python_version < "3.12"
 webencodings==0.5.1 ; python_version >= "3.7" and python_version < "3.12"
 webob==1.8.7 ; python_version >= "3.7" and python_version < "3.12"
 whoosh==2.7.4 ; python_version >= "3.7" and python_version < "3.12"
+wrapt==1.14.1 ; python_version >= "3.7" and python_version < "3.12"
 yacman==0.8.4 ; python_version >= "3.7" and python_version < "3.12"
 yarl==1.8.2 ; python_version >= "3.7" and python_version < "3.12"
 zipp==3.12.0 ; python_version >= "3.7" and python_version < "3.10"
diff --git a/lib/galaxy/files/__init__.py b/lib/galaxy/files/__init__.py
index f4b5c474d9fc..20dc7d78d1f5 100644
--- a/lib/galaxy/files/__init__.py
+++ b/lib/galaxy/files/__init__.py
@@ -19,6 +19,11 @@
 log = logging.getLogger(__name__)
 
 FileSourcePath = namedtuple("FileSourcePath", ["file_source", "path"])
+FileSourceScore = namedtuple("FileSourceScore", ["file_source", "score"])
+
+
+class NoMatchingFileSource(Exception):
+    pass
 
 
 class ConfiguredFileSources:
@@ -51,12 +56,17 @@ def _ensure_loaded(plugin_type):
                         return
                 stock_file_source_conf_dict.append({"type": plugin_type})
 
+            _ensure_loaded("http")
+            _ensure_loaded("base64")
+            _ensure_loaded("drs")
+
             if file_sources_config.ftp_upload_dir is not None:
                 _ensure_loaded("gxftp")
             if file_sources_config.library_import_dir is not None:
                 _ensure_loaded("gximport")
             if file_sources_config.user_library_import_dir is not None:
                 _ensure_loaded("gxuserimport")
+
             if stock_file_source_conf_dict:
                 stock_plugin_source = plugin_config.plugin_source_from_dict(stock_file_source_conf_dict)
                 # insert at begining instead of append so FTP and library import appear
@@ -87,24 +97,21 @@ def _parse_plugin_source(self, plugin_source):
             dict_to_list_key="id",
         )
 
+    def find_best_match(self, url: str):
+        """Returns the best matching file source for handling a particular url. Each filesource scores its own
+        ability to match a particular url, and the highest scorer with a score > 0 is selected."""
+        scores = [FileSourceScore(file_source, file_source.score_url_match(url)) for file_source in self._file_sources]
+        scores.sort(key=lambda f: f.score, reverse=True)
+        return next((fsscore.file_source for fsscore in scores if fsscore.score > 0), None)
+
     def get_file_source_path(self, uri):
         """Parse uri into a FileSource object and a path relative to its base."""
         if "://" not in uri:
             raise exceptions.RequestParameterInvalidException(f"Invalid uri [{uri}]")
-        scheme, rest = uri.split("://", 1)
-        if scheme not in self.get_schemes():
-            raise exceptions.RequestParameterInvalidException(f"Unsupported URI scheme [{scheme}]")
-
-        if scheme != "gxfiles":
-            # prefix unused
-            id_prefix = None
-            path = rest
-        else:
-            if "/" in rest:
-                id_prefix, path = rest.split("/", 1)
-            else:
-                id_prefix, path = rest, "/"
-        file_source = self.get_file_source(id_prefix, scheme)
+        file_source = self.find_best_match(uri)
+        if not file_source:
+            raise exceptions.RequestParameterInvalidException(f"Could not find handler for URI [{uri}]")
+        path = file_source.to_relative_path(uri)
         return FileSourcePath(file_source, path)
 
     def validate_uri_root(self, uri, user_context):
@@ -139,38 +146,26 @@ def validate_uri_root(self, uri, user_context):
                     "Your FTP directory does not exist, attempting to upload files to it may cause it to be created."
                 )
 
-    def get_file_source(self, id_prefix, scheme):
-        for file_source in self._file_sources:
-            # gxfiles uses prefix to find plugin, other scheme are assumed to have
-            # at most one file_source.
-            if scheme != file_source.get_scheme():
-                continue
-            prefix_match = scheme != "gxfiles" or file_source.get_prefix() == id_prefix
-            if prefix_match:
-                return file_source
-
     def looks_like_uri(self, path_or_uri):
         # is this string a URI this object understands how to realize
-        if path_or_uri.startswith("gx") and "://" in path_or_uri:
-            for scheme in self.get_schemes():
-                if path_or_uri.startswith(f"{scheme}://"):
-                    return True
-
-        return False
-
-    def get_schemes(self):
-        schemes = set()
-        for file_source in self._file_sources:
-            schemes.add(file_source.get_scheme())
-        return schemes
+        file_source = self.find_best_match(path_or_uri)
+        if file_source:
+            return True
+        else:
+            return False
 
     def plugins_to_dict(
-        self, for_serialization: bool = False, user_context: Optional["FileSourceDictifiable"] = None
+        self,
+        for_serialization: bool = False,
+        user_context: Optional["FileSourceDictifiable"] = None,
+        browsable_only: Optional[bool] = False,
     ) -> List[Dict[str, Any]]:
         rval = []
         for file_source in self._file_sources:
             if not file_source.user_has_access(user_context):
                 continue
+            if browsable_only and not file_source.get_browsable():
+                continue
             el = file_source.to_dict(for_serialization=for_serialization, user_context=user_context)
             rval.append(el)
         return rval
@@ -196,7 +191,7 @@ def from_app_config(config):
         )
 
     @staticmethod
-    def from_dict(as_dict):
+    def from_dict(as_dict, load_stock_plugins=False):
         if as_dict is not None:
             sources_as_dict = as_dict["file_sources"]
             config_as_dict = as_dict["config"]
@@ -204,7 +199,9 @@ def from_dict(as_dict):
         else:
             sources_as_dict = []
             file_sources_config = ConfiguredFileSourcesConfig()
-        return ConfiguredFileSources(file_sources_config, conf_dict=sources_as_dict)
+        return ConfiguredFileSources(
+            file_sources_config, conf_dict=sources_as_dict, load_stock_plugins=load_stock_plugins
+        )
 
 
 class NullConfiguredFileSources(ConfiguredFileSources):
@@ -276,14 +273,19 @@ def role_names(self) -> Set[str]:
         raise NotImplementedError
 
     @property
-    def group_names(sefl) -> Set[str]:
+    def group_names(self) -> Set[str]:
+        raise NotImplementedError
+
+    @property
+    def file_sources(self):
+        """Return other filesources available in the system, for chained filesource resolution"""
         raise NotImplementedError
 
 
 class ProvidesUserFileSourcesUserContext(FileSourceDictifiable):
     """Implement a FileSourcesUserContext from a Galaxy ProvidesUserContext (e.g. trans)."""
 
-    def __init__(self, trans):
+    def __init__(self, trans, **kwargs):
         self.trans = trans
 
     @property
@@ -334,6 +336,10 @@ def app_vault(self):
         vault = self.trans.app.vault
         return vault or defaultdict(lambda: None)
 
+    @property
+    def file_sources(self):
+        return self.trans.app.file_sources
+
 
 class DictFileSourcesUserContext(FileSourceDictifiable):
     def __init__(self, **kwd):
@@ -374,3 +380,7 @@ def user_vault(self):
     @property
     def app_vault(self):
         return self._kwd.get("app_vault")
+
+    @property
+    def file_sources(self):
+        return self._kwd.get("file_sources")
diff --git a/lib/galaxy/files/sources/__init__.py b/lib/galaxy/files/sources/__init__.py
index 5e079d7cf910..d369fcfe4293 100644
--- a/lib/galaxy/files/sources/__init__.py
+++ b/lib/galaxy/files/sources/__init__.py
@@ -2,8 +2,16 @@
 import os
 import time
 from typing import (
+    Any,
     ClassVar,
+    Optional,
     Set,
+    TYPE_CHECKING,
+)
+
+from typing_extensions import (
+    NotRequired,
+    TypedDict,
 )
 
 from galaxy.exceptions import (
@@ -19,41 +27,134 @@
 DEFAULT_SCHEME = "gxfiles"
 DEFAULT_WRITABLE = False
 
+if TYPE_CHECKING:
+    from galaxy.files import ConfiguredFileSourcesConfig
 
-class FilesSource(metaclass=abc.ABCMeta):
-    """ """
 
-    @abc.abstractmethod
-    def get_uri_root(self) -> str:
-        """Return a prefix for the root (e.g. gxfiles://prefix/)."""
+class FilesSourceProperties(TypedDict):
+    """Initial set of properties used to initialize a filesource.
 
-    @abc.abstractmethod
-    def get_scheme(self) -> str:
-        """Return a prefix for the root (e.g. the gxfiles in gxfiles://prefix/path)."""
+    Filesources can extend this typed dict to define any additional
+    filesource specific properties.
+    """
+
+    file_sources_config: NotRequired["ConfiguredFileSourcesConfig"]
+    id: NotRequired[str]
+    label: NotRequired[str]
+    doc: NotRequired[Optional[str]]
+    scheme: NotRequired[str]
+    writable: NotRequired[bool]
+    requires_roles: NotRequired[Optional[str]]
+    requires_groups: NotRequired[Optional[str]]
+    # API helper values
+    uri_root: NotRequired[str]
+    type: NotRequired[str]
+    browsable: NotRequired[bool]
+
+
+class FilesSourceOptions:
+    """Options to control behaviour of filesource operations, such as realize_to and write_from"""
+
+    # Property overrides for values initially configured through the constructor. For example
+    # the HTTPFilesSource passes in additional http_headers through these properties, which
+    # are merged with constructor defined http_headers. The interpretation of these properties
+    # are filesystem specific.
+    extra_props: Optional[FilesSourceProperties]
+
+
+class SingleFileSource(metaclass=abc.ABCMeta):
+    """
+    Represents a protocol handler for a single remote file that can be read by or written to by Galaxy.
+    A remote file source can typically handle a url like `https://galaxyproject.org/myfile.txt` or
+    `drs://myserver/123456`. The filesource abstraction allows programmatic control over the specific source
+    to access, injection of credentials and access control. Filesources are typically listed and configured
+    through `file_sources_conf.yml` or programmatically, as required.
+
+    Filesources can be contextualized with a `user_context`, which contains information related to the current
+    user attempting to access that filesource such as the username, preferences, roles etc., which can then
+    be used by the filesource to make authorization decisions or inject credentials.
+
+    Filesources are loaded through Galaxy's plugin system in `galaxy.util.plugin_config`.
+    """
 
     @abc.abstractmethod
-    def get_writable(self):
-        """Return a boolean indicating if this target is writable."""
+    def get_writable(self) -> bool:
+        """Return a boolean indicating whether this target is writable."""
 
     @abc.abstractmethod
     def user_has_access(self, user_context) -> bool:
-        """Return a boolean indicating if the user can access the FileSource."""
+        """Return a boolean indicating whether the user can access the FileSource."""
 
-    # TODO: off-by-default
     @abc.abstractmethod
-    def list(self, source_path="/", recursive=False, user_context=None):
-        """Return dictionary of 'Directory's and 'File's."""
+    def realize_to(
+        self, source_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        """Realize source path (relative to uri root) to local file system path.
+
+        :param source_path: url of the source file to copy from. e.g. `https://galaxyproject.org/myfile.txt`
+        :type source_path: str
+        :param native_path: local path to write to. e.g. `/tmp/myfile.txt`
+        :type native_path: str
+        :param user_context: A user context , defaults to None
+        :type user_context: FileSourceDictifiable, optional
+        :param opts: A set of options to exercise additional control over the realize_to method. Filesource specific, defaults to None
+        :type opts: Optional[FilesSourceOptions], optional
+        """
 
     @abc.abstractmethod
-    def realize_to(self, source_path, native_path, user_context=None):
-        """Realize source path (relative to uri root) to local file system path."""
+    def write_from(
+        self, target_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        """Write file at native path to target_path (relative to uri root).
+
+        :param target_path: url of the target file to write to within the filesource. e.g. `gxfiles://myftp1/myfile.txt`
+        :type target_path: str
+        :param native_path: The local file to read. e.g. `/tmp/myfile.txt`
+        :type native_path: str
+        :param user_context: A user context , defaults to None
+        :type user_context: _type_, optional
+        :param opts: A set of options to exercise additional control over the write_from method. Filesource specific, defaults to None
+        :type opts: Optional[FilesSourceOptions], optional
+        """
 
     @abc.abstractmethod
-    def write_from(self, target_path, native_path, user_context=None):
-        """Write file at native path to target_path (relative to uri root)."""
+    def score_url_match(self, url: str) -> int:
+        """Return how well a given url matches this filesource. A score greater than zero indicates that
+        this filesource is capable of processing the given url.
+
+        Scoring is based on the following rules:
+        a. The maximum score will be the length of the url.
+        b. The minimum score will be the length of the scheme if the filesource can handle the file.
+        c. The score will be zero if the filesource cannot handle the file.
+
+        For example, given the following file source config:
+        - type: webdav
+          id: cloudstor
+          url: "https://cloudstor.aarnet.edu.au"
+          root: "/plus/remote.php/webdav"
+        - type: http
+          id: generic_http
+
+        the generic http handler may return a score of 8 for the url
+        https://cloudstor.aarnet.edu.au/plus/remote.php/webdav/myfolder/myfile.txt,
+        as it can handle only the scheme part of the url. A webdav handler may return a score of
+        55 for the same url, as both the webdav url and root combined are a specific match.
+
+        :param url: The url to score for a match against this filesource.
+        :type url: str
+        :return: A score based on the aforementioned rules.
+        :rtype: int
+        """
 
     @abc.abstractmethod
-    def to_dict(self, for_serialization=False, user_context=None):
+    def to_relative_path(self, url: str) -> str:
+        """Convert this url to a filesource relative path. For example, given the url
+        `gxfiles://mysource1/myfile.txt` it will return `/myfile.txt`. Protocols directly understood
+        by the handler need not be relativized. For example, the url `s3://bucket/myfile.txt` can be
+        returned unchanged."""
+
+    @abc.abstractmethod
+    def to_dict(self, for_serialization=False, user_context=None) -> FilesSourceProperties:
         """Return a dictified representation of this FileSource instance.
 
         If ``user_context`` is supplied, properties should be written so user
@@ -61,16 +162,49 @@ def to_dict(self, for_serialization=False, user_context=None):
         """
 
 
+class SupportsBrowsing(metaclass=abc.ABCMeta):
+    """An interface indicating that this filesource is browsable.
+
+    Browsable filesources will typically have a root uri from which to start browsing.
+    e.g. In an s3 bucket, the root uri may be gxfiles://bucket1/
+
+    They will also have a list method to list files in a specific path within the filesource.
+    """
+
+    @abc.abstractmethod
+    def get_uri_root(self) -> str:
+        """Return a prefix for the root (e.g. gxfiles://prefix/)."""
+
+    @abc.abstractmethod
+    def list(self, path="/", recursive=False, user_context=None, opts: Optional[FilesSourceOptions] = None) -> dict:
+        """Return dictionary of 'Directory's and 'File's."""
+
+
+class FilesSource(SingleFileSource, SupportsBrowsing):
+    """Represents a combined interface for single or browsable filesources.
+    The `get_browsable` method can be used to determine whether the filesource is browsable and
+    implements the `SupportsBrowsing` interface.
+    """
+
+    @abc.abstractmethod
+    def get_browsable(self) -> bool:
+        """Return true if the filesource implements the SupportsBrowsing interface."""
+
+
 class BaseFilesSource(FilesSource):
     plugin_type: ClassVar[str]
 
-    def get_prefix(self):
+    def get_browsable(self) -> bool:
+        # Check whether the list method has been overridden
+        return type(self).list != BaseFilesSource.list or type(self)._list != BaseFilesSource._list
+
+    def get_prefix(self) -> Optional[str]:
         return self.id
 
-    def get_scheme(self):
+    def get_scheme(self) -> str:
         return "gxfiles"
 
-    def get_writable(self):
+    def get_writable(self) -> bool:
         return self.writable
 
     def user_has_access(self, user_context) -> bool:
@@ -86,7 +220,7 @@ def user_has_access(self, user_context) -> bool:
     def user_context_required(self) -> bool:
         return self.requires_roles is not None or self.requires_groups is not None
 
-    def get_uri_root(self):
+    def get_uri_root(self) -> str:
         prefix = self.get_prefix()
         scheme = self.get_scheme()
         root = f"{scheme}://"
@@ -94,11 +228,18 @@ def get_uri_root(self):
             root = uri_join(root, prefix)
         return root
 
-    def uri_from_path(self, path):
+    def to_relative_path(self, url: str) -> str:
+        return url.replace(self.get_uri_root(), "") or "/"
+
+    def score_url_match(self, url: str) -> int:
+        root = self.get_uri_root()
+        return len(root) if root in url else 0
+
+    def uri_from_path(self, path) -> str:
         uri_root = self.get_uri_root()
         return uri_join(uri_root, path)
 
-    def _parse_common_config_opts(self, kwd: dict):
+    def _parse_common_config_opts(self, kwd: FilesSourceProperties):
         self._file_sources_config = kwd.pop("file_sources_config")
         self.id = kwd.pop("id")
         self.label = kwd.pop("label", None) or self.id
@@ -111,19 +252,22 @@ def _parse_common_config_opts(self, kwd: dict):
         # If coming from to_dict, strip API helper values
         kwd.pop("uri_root", None)
         kwd.pop("type", None)
+        kwd.pop("browsable", None)
         return kwd
 
-    def to_dict(self, for_serialization=False, user_context=None):
-        rval = {
+    def to_dict(self, for_serialization=False, user_context=None) -> FilesSourceProperties:
+        rval: FilesSourceProperties = {
             "id": self.id,
             "type": self.plugin_type,
-            "uri_root": self.get_uri_root(),
             "label": self.label,
             "doc": self.doc,
             "writable": self.writable,
+            "browsable": self.get_browsable(),
             "requires_roles": self.requires_roles,
             "requires_groups": self.requires_groups,
         }
+        if self.get_browsable():
+            rval["uri_root"] = self.get_uri_root()
         if for_serialization:
             rval.update(self._serialization_props(user_context=user_context))
         return rval
@@ -137,36 +281,34 @@ def to_dict_time(self, ctime):
             return ctime.strftime("%m/%d/%Y %I:%M:%S %p")
 
     @abc.abstractmethod
-    def _serialization_props(self, user_context=None):
+    def _serialization_props(self, user_context=None) -> FilesSourceProperties:
         """Serialize properties needed to recover plugin configuration.
-
         Used in to_dict method if for_serialization is True.
         """
 
-    def list(self, path="/", recursive=False, user_context=None):
+    def list(self, path="/", recursive=False, user_context=None, opts: Optional[FilesSourceOptions] = None):
         self._check_user_access(user_context)
-        return self._list(path, recursive, user_context)
+        return self._list(path, recursive, user_context, opts)
 
-    @abc.abstractmethod
-    def _list(self, path="/", recursive=False, user_context=None):
+    def _list(self, path="/", recursive=False, user_context=None, opts: Optional[FilesSourceOptions] = None):
         pass
 
-    def write_from(self, target_path, native_path, user_context=None):
+    def write_from(self, target_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         if not self.get_writable():
             raise Exception("Cannot write to a non-writable file source.")
         self._check_user_access(user_context)
-        self._write_from(target_path, native_path, user_context=user_context)
+        self._write_from(target_path, native_path, user_context=user_context, opts=opts)
 
     @abc.abstractmethod
-    def _write_from(self, target_path, native_path, user_context=None):
+    def _write_from(self, target_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         pass
 
-    def realize_to(self, source_path, native_path, user_context=None):
+    def realize_to(self, source_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         self._check_user_access(user_context)
-        self._realize_to(source_path, native_path, user_context)
+        self._realize_to(source_path, native_path, user_context, opts=opts)
 
     @abc.abstractmethod
-    def _realize_to(self, source_path, native_path, user_context=None):
+    def _realize_to(self, source_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         pass
 
     def _check_user_access(self, user_context):
@@ -178,7 +320,7 @@ def _check_user_access(self, user_context):
         if user_context is not None and not self.user_has_access(user_context):
             raise ItemAccessibilityException(f"User {user_context.username} has no access to file source.")
 
-    def _evaluate_prop(self, prop_val, user_context):
+    def _evaluate_prop(self, prop_val: Any, user_context):
         rval = prop_val
         if isinstance(prop_val, str) and "$" in prop_val:
             template_context = dict(
@@ -187,6 +329,10 @@ def _evaluate_prop(self, prop_val, user_context):
                 config=self._file_sources_config,
             )
             rval = fill_template(prop_val, context=template_context, futurized=True)
+        elif isinstance(prop_val, dict):
+            rval = {key: self._evaluate_prop(childprop, user_context) for key, childprop in prop_val.items()}
+        elif isinstance(prop_val, list):
+            rval = [self._evaluate_prop(childprop, user_context) for childprop in prop_val]
 
         return rval
 
diff --git a/lib/galaxy/files/sources/_pyfilesystem2.py b/lib/galaxy/files/sources/_pyfilesystem2.py
index 45162f86c5ff..3336f72a2165 100644
--- a/lib/galaxy/files/sources/_pyfilesystem2.py
+++ b/lib/galaxy/files/sources/_pyfilesystem2.py
@@ -13,8 +13,13 @@
 
 import fs
 from fs.base import FS
+from typing_extensions import Unpack
 
-from . import BaseFilesSource
+from . import (
+    BaseFilesSource,
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 
 log = logging.getLogger(__name__)
 
@@ -25,20 +30,20 @@ class PyFilesystem2FilesSource(BaseFilesSource):
     required_module: ClassVar[Optional[Type[FS]]]
     required_package: ClassVar[str]
 
-    def __init__(self, **kwd):
+    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
         if self.required_module is None:
             raise Exception(PACKAGE_MESSAGE % self.required_package)
         props = self._parse_common_config_opts(kwd)
         self._props = props
 
     @abc.abstractmethod
-    def _open_fs(self, user_context=None):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         """Subclasses must instantiate a PyFilesystem2 handle for this file system."""
 
-    def _list(self, path="/", recursive=False, user_context=None):
+    def _list(self, path="/", recursive=False, user_context=None, opts: Optional[FilesSourceOptions] = None):
         """Return dictionary of 'Directory's and 'File's."""
 
-        with self._open_fs(user_context=user_context) as h:
+        with self._open_fs(user_context=user_context, opts=opts) as h:
             if recursive:
                 res: List[Dict[str, Any]] = []
                 for p, dirs, files in h.walk(path):
@@ -51,13 +56,13 @@ def _list(self, path="/", recursive=False, user_context=None):
                 to_dict = functools.partial(self._resource_info_to_dict, path)
                 return list(map(to_dict, res))
 
-    def _realize_to(self, source_path, native_path, user_context=None):
+    def _realize_to(self, source_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         with open(native_path, "wb") as write_file:
-            self._open_fs(user_context=user_context).download(source_path, write_file)
+            self._open_fs(user_context=user_context, opts=opts).download(source_path, write_file)
 
-    def _write_from(self, target_path, native_path, user_context=None):
+    def _write_from(self, target_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         with open(native_path, "rb") as read_file:
-            openfs = self._open_fs(user_context=user_context)
+            openfs = self._open_fs(user_context=user_context, opts=opts)
             dirname = fs.path.dirname(target_path)
             if not openfs.isdir(dirname):
                 openfs.makedirs(dirname)
diff --git a/lib/galaxy/files/sources/anvil.py b/lib/galaxy/files/sources/anvil.py
index aaaf31f134b5..a98c58001919 100644
--- a/lib/galaxy/files/sources/anvil.py
+++ b/lib/galaxy/files/sources/anvil.py
@@ -2,6 +2,15 @@
     from anvilfs.anvilfs import AnVILFS
 except ImportError:
     AnVILFS = None
+from typing import (
+    Optional,
+    Union,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -10,9 +19,10 @@ class AnVILFilesSource(PyFilesystem2FilesSource):
     required_module = AnVILFS
     required_package = "fs.anvilfs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
-        handle = AnVILFS(**props)
+        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
+        handle = AnVILFS(**{**props, **extra_props})
         return handle
 
 
diff --git a/lib/galaxy/files/sources/base64.py b/lib/galaxy/files/sources/base64.py
new file mode 100644
index 000000000000..1c7f16ab1985
--- /dev/null
+++ b/lib/galaxy/files/sources/base64.py
@@ -0,0 +1,55 @@
+import base64
+import logging
+from typing import Optional
+
+from typing_extensions import Unpack
+
+from . import (
+    BaseFilesSource,
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
+
+log = logging.getLogger(__name__)
+
+
+class Base64FilesSource(BaseFilesSource):
+    plugin_type = "base64"
+
+    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
+        kwds: FilesSourceProperties = dict(
+            id="_base64",
+            label="Base64 encoded string",
+            doc="Base64 string handler",
+            writable=False,
+        )
+        kwds.update(kwd)
+        props = self._parse_common_config_opts(kwds)
+        self._props = props
+
+    def _realize_to(
+        self, source_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        with open(native_path, "wb") as temp:
+            temp.write(base64.b64decode(source_path[len("base64://") :]))
+            temp.flush()
+
+    def _write_from(
+        self, target_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        raise NotImplementedError()
+
+    def score_url_match(self, url: str):
+        if url.startswith("base64://"):
+            return len("base64://")
+        else:
+            return 0
+
+    def _serialization_props(self, user_context=None):
+        effective_props = {}
+        for key, val in self._props.items():
+            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
+        return effective_props
+
+
+__all__ = ("Base64FilesSource",)
diff --git a/lib/galaxy/files/sources/basespace.py b/lib/galaxy/files/sources/basespace.py
index 3ed27bdb2278..3229955857d6 100644
--- a/lib/galaxy/files/sources/basespace.py
+++ b/lib/galaxy/files/sources/basespace.py
@@ -3,6 +3,15 @@
 except ImportError:
     BASESPACEFS = None
 
+from typing import (
+    Optional,
+    Union,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -11,9 +20,10 @@ class BaseSpaceFilesSource(PyFilesystem2FilesSource):
     required_module = BASESPACEFS
     required_package = "fs-basespace"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
-        handle = BASESPACEFS(**props)
+        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
+        handle = BASESPACEFS(**{**props, **extra_props})
         return handle
 
 
diff --git a/lib/galaxy/files/sources/dropbox.py b/lib/galaxy/files/sources/dropbox.py
index 6fa2a3b9d3c4..6ef847aca95b 100644
--- a/lib/galaxy/files/sources/dropbox.py
+++ b/lib/galaxy/files/sources/dropbox.py
@@ -3,6 +3,15 @@
 except ImportError:
     DropboxFS = None
 
+from typing import (
+    Optional,
+    Union,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -11,9 +20,10 @@ class DropboxFilesSource(PyFilesystem2FilesSource):
     required_module = DropboxFS
     required_package = "fs.dropboxfs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
-        handle = DropboxFS(**props)
+        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
+        handle = DropboxFS(**{**props, **extra_props})
         return handle
 
 
diff --git a/lib/galaxy/files/sources/drs.py b/lib/galaxy/files/sources/drs.py
new file mode 100644
index 000000000000..f281e677566b
--- /dev/null
+++ b/lib/galaxy/files/sources/drs.py
@@ -0,0 +1,51 @@
+import logging
+from typing import Optional
+
+from typing_extensions import Unpack
+
+from galaxy.util.drs import fetch_drs_to_file
+from . import (
+    BaseFilesSource,
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
+
+log = logging.getLogger(__name__)
+
+
+class DRSFilesSource(BaseFilesSource):
+    plugin_type = "drs"
+
+    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
+        kwds: FilesSourceProperties = dict(
+            id="_drs",
+            label="DRS file",
+            doc="DRS file handler",
+            writable=False,
+        )
+        kwds.update(kwd)
+        props = self._parse_common_config_opts(kwds)
+        self._props = props
+
+    def _realize_to(self, source_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
+        props = self._serialization_props(user_context)
+        headers = props.pop("http_headers", {}) or {}
+        fetch_drs_to_file(source_path, native_path, user_context, headers=headers)
+
+    def _write_from(self, target_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
+        raise NotImplementedError()
+
+    def score_url_match(self, url: str):
+        if url.startswith("drs://"):
+            return len("drs://")
+        else:
+            return 0
+
+    def _serialization_props(self, user_context=None):
+        effective_props = {}
+        for key, val in self._props.items():
+            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
+        return effective_props
+
+
+__all__ = ("DRSFilesSource",)
diff --git a/lib/galaxy/files/sources/ftp.py b/lib/galaxy/files/sources/ftp.py
index eebf0fdd8527..f2877f648392 100644
--- a/lib/galaxy/files/sources/ftp.py
+++ b/lib/galaxy/files/sources/ftp.py
@@ -1,20 +1,104 @@
+import urllib.parse
+
 try:
     from fs.ftpfs import FTPFS
 except ImportError:
     FTPFS = None  # type: ignore[misc,assignment]
 
+from typing import (
+    cast,
+    Optional,
+    Tuple,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
+class FTPFilesSourceProperties(FilesSourceProperties, total=False):
+    host: str
+    port: int
+    user: str
+    passwd: str
+
+
 class FtpFilesSource(PyFilesystem2FilesSource):
     plugin_type = "ftp"
     required_module = FTPFS
     required_package = "fs.ftpfs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
-        handle = FTPFS(**props)
+        extra_props: FTPFilesSourceProperties = cast(FTPFilesSourceProperties, opts.extra_props or {} if opts else {})
+        handle = FTPFS(**{**props, **extra_props})
         return handle
 
+    def _realize_to(
+        self, source_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        extra_props: FTPFilesSourceProperties
+        if opts and opts.extra_props:
+            extra_props = cast(FTPFilesSourceProperties, opts.extra_props)
+        else:
+            opts = FilesSourceOptions()
+            extra_props = {}
+        path, opts.extra_props = self._get_props_and_rel_path(extra_props, source_path)
+        super()._realize_to(path, native_path, user_context=user_context, opts=opts)
+
+    def _write_from(
+        self, target_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        extra_props: FTPFilesSourceProperties
+        if opts and opts.extra_props:
+            extra_props = cast(FTPFilesSourceProperties, opts.extra_props)
+        else:
+            opts = FilesSourceOptions()
+            extra_props = {}
+        path, opts.extra_props = self._get_props_and_rel_path(extra_props, target_path)
+        super()._write_from(path, native_path, user_context=user_context, opts=opts)
+
+    def _get_props_and_rel_path(
+        self, extra_props: FTPFilesSourceProperties, url: str
+    ) -> Tuple[str, FTPFilesSourceProperties]:
+        host = self._props.get("host")
+        port = self._props.get("port")
+        user = self._props.get("user")
+        passwd = self._props.get("passwd")
+        rel_path = url
+        if url.startswith(f"ftp://{host or ''}"):
+            props = self._extract_url_props(url)
+            extra_props["host"] = host or props["host"]
+            extra_props["port"] = port or props["port"]
+            extra_props["user"] = user or props["user"]
+            extra_props["passwd"] = passwd or props["passwd"]
+            rel_path = props["path"] or url
+        return rel_path, extra_props
+
+    def _extract_url_props(self, url: str):
+        result = urllib.parse.urlparse(url)
+        return {
+            "host": result.hostname,
+            "port": result.port or 21,
+            "user": result.username,
+            "passwd": result.password,
+            "path": result.path,
+        }
+
+    def score_url_match(self, url: str):
+        host = self._props.get("host")
+        port = self._props.get("port")
+        if host and port and url.startswith(f"ftp://{host}:{port}"):
+            return len(f"ftp://{host}:{port}")
+        # For security, we need to ensure that a partial match doesn't work e.g. ftp://{host}something/myfiles
+        elif host and (url.startswith(f"ftp://{host}/") or url == f"ftp://{host}"):
+            return len(f"ftp://{host}")
+        elif not host and url.startswith("ftp://"):
+            return len("ftp://")
+        else:
+            return super().score_url_match(url)
+
 
 __all__ = ("FtpFilesSource",)
diff --git a/lib/galaxy/files/sources/galaxy.py b/lib/galaxy/files/sources/galaxy.py
index 0f344906156a..ebbf9d303e4c 100644
--- a/lib/galaxy/files/sources/galaxy.py
+++ b/lib/galaxy/files/sources/galaxy.py
@@ -1,81 +1,79 @@
 """Static Galaxy file sources - ftp and libraries."""
 
-from .posix import PosixFilesSource
+from typing import (
+    cast,
+    Optional,
+)
+
+from typing_extensions import Unpack
+
+from .posix import (
+    PosixFilesSource,
+    PosixFilesSourceProperties,
+)
 
 
 class UserFtpFilesSource(PosixFilesSource):
     plugin_type = "gxftp"
 
-    def __init__(self, label="FTP Directory", doc="Galaxy User's FTP Directory", root="${user.ftp_dir}", **kwd):
-        posix_kwds = dict(
+    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
+        posix_kwds: PosixFilesSourceProperties = dict(
             id="_ftp",
-            root=root,
-            label=label,
-            doc=doc,
+            root="${user.ftp_dir}",
+            label="FTP Directory",
+            doc="Galaxy User's FTP Directory",
             writable=True,
         )
         posix_kwds.update(kwd)
         if "delete_on_realize" not in posix_kwds:
             file_sources_config = kwd["file_sources_config"]
-            posix_kwds["delete_on_realize"] = file_sources_config.ftp_upload_purge
+            posix_kwds["delete_on_realize"] = cast(bool, file_sources_config.ftp_upload_purge)
         super().__init__(**posix_kwds)
 
-    def get_prefix(self):
+    def get_prefix(self) -> Optional[str]:
         return None
 
-    def get_scheme(self):
+    def get_scheme(self) -> str:
         return "gxftp"
 
 
 class LibraryImportFilesSource(PosixFilesSource):
     plugin_type = "gximport"
 
-    def __init__(
-        self,
-        label="Library Import Directory",
-        doc="Galaxy's library import directory",
-        root="${config.library_import_dir}",
-        **kwd,
-    ):
-        posix_kwds = dict(
+    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
+        posix_kwds: PosixFilesSourceProperties = dict(
             id="_import",
-            root=root,
-            label=label,
-            doc=doc,
+            root="${config.library_import_dir}",
+            label="Library Import Directory",
+            doc="Galaxy's library import directory",
         )
         posix_kwds.update(kwd)
         super().__init__(**posix_kwds)
 
-    def get_prefix(self):
+    def get_prefix(self) -> Optional[str]:
         return None
 
-    def get_scheme(self):
+    def get_scheme(self) -> str:
         return "gximport"
 
 
 class UserLibraryImportFilesSource(PosixFilesSource):
     plugin_type = "gxuserimport"
 
-    def __init__(
-        self,
-        label="Library User Import Directory",
-        doc="Galaxy's user library import directory",
-        root="${config.user_library_import_dir}/${user.email}",
-        **kwd,
-    ):
-        posix_kwds = dict(
+    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
+        posix_kwds: PosixFilesSourceProperties = dict(
             id="_userimport",
-            root=root,
-            label=label,
-            doc=doc,
+            root="${config.user_library_import_dir}/${user.email}",
+            label="Library User Import Directory",
+            doc="Galaxy's user library import directory",
         )
         posix_kwds.update(kwd)
         super().__init__(**posix_kwds)
 
-    def get_prefix(self):
+    def get_prefix(self) -> Optional[str]:
         return None
 
-    def get_scheme(self):
+    def get_scheme(self) -> str:
         return "gxuserimport"
 
 
diff --git a/lib/galaxy/files/sources/googlecloudstorage.py b/lib/galaxy/files/sources/googlecloudstorage.py
index bbe64e6a9832..9eaaa0c79adf 100644
--- a/lib/galaxy/files/sources/googlecloudstorage.py
+++ b/lib/galaxy/files/sources/googlecloudstorage.py
@@ -5,16 +5,35 @@
 except ImportError:
     GCSFS = None
 
+from typing import (
+    cast,
+    Optional,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
+class GoogleCloudStorageFilesSourceProperties(FilesSourceProperties, total=False):
+    bucket_name: str
+    root_path: str
+    project: str
+    anonymous: bool
+
+
 class GoogleCloudStorageFilesSource(PyFilesystem2FilesSource):
     plugin_type = "googlecloudstorage"
     required_module = GCSFS
     required_package = "fs-gcsfs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
+        extra_props: GoogleCloudStorageFilesSourceProperties = cast(
+            GoogleCloudStorageFilesSourceProperties, opts.extra_props or {} if opts else {}
+        )
         bucket_name = props.pop("bucket_name", None)
         root_path = props.pop("root_path", None)
         project = props.pop("project", None)
@@ -23,7 +42,7 @@ def _open_fs(self, user_context):
             args["client"] = Client.create_anonymous_client()
         elif props.get("token"):
             args["client"] = Client(project=project, credentials=Credentials(**props))
-        handle = GCSFS(bucket_name, root_path=root_path, retry=0, **args)
+        handle = GCSFS(bucket_name, root_path=root_path, retry=0, **{**args, **extra_props})
         return handle
 
 
diff --git a/lib/galaxy/files/sources/googledrive.py b/lib/galaxy/files/sources/googledrive.py
index f0a4bb7249a4..c5e777d6b78d 100644
--- a/lib/galaxy/files/sources/googledrive.py
+++ b/lib/galaxy/files/sources/googledrive.py
@@ -4,6 +4,9 @@
 except ImportError:
     GoogleDriveFS = None
 
+from typing import Optional
+
+from . import FilesSourceOptions
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -12,7 +15,7 @@ class GoogleDriveFilesSource(PyFilesystem2FilesSource):
     required_module = GoogleDriveFS
     required_package = "fs.googledrivefs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
         credentials = Credentials(**props)
         handle = GoogleDriveFS(credentials)
diff --git a/lib/galaxy/files/sources/http.py b/lib/galaxy/files/sources/http.py
new file mode 100644
index 000000000000..83ecd3b0b9e2
--- /dev/null
+++ b/lib/galaxy/files/sources/http.py
@@ -0,0 +1,84 @@
+import logging
+import re
+import urllib.request
+from typing import (
+    cast,
+    Dict,
+    Optional,
+)
+
+from typing_extensions import Unpack
+
+from galaxy.util import (
+    DEFAULT_SOCKET_TIMEOUT,
+    get_charset_from_http_headers,
+    stream_to_open_named_file,
+)
+from . import (
+    BaseFilesSource,
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
+
+log = logging.getLogger(__name__)
+
+
+class HTTPFilesSourceProperties(FilesSourceProperties, total=False):
+    url_regex: str
+    http_headers: Dict[str, str]
+
+
+class HTTPFilesSource(BaseFilesSource):
+    plugin_type = "http"
+
+    def __init__(self, **kwd: Unpack[FilesSourceProperties]):
+        kwds: FilesSourceProperties = dict(
+            id="_http",
+            label="HTTP File",
+            doc="Default HTTP file handler",
+            writable=False,
+        )
+        kwds.update(kwd)
+        props: HTTPFilesSourceProperties = cast(HTTPFilesSourceProperties, self._parse_common_config_opts(kwds))
+        self._url_regex_str = props.pop("url_regex", r"^https?://|^ftp://")
+        assert self._url_regex_str
+        self._url_regex = re.compile(self._url_regex_str)
+        self._props = props
+
+    def _realize_to(
+        self, source_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        props = self._serialization_props(user_context)
+        extra_props: HTTPFilesSourceProperties = cast(HTTPFilesSourceProperties, opts.extra_props or {} if opts else {})
+        headers = props.pop("http_headers", {}) or {}
+        headers.update(extra_props.get("http_headers") or {})
+
+        req = urllib.request.Request(source_path, headers=headers)
+
+        with urllib.request.urlopen(req, timeout=DEFAULT_SOCKET_TIMEOUT) as page:
+            f = open(native_path, "wb")  # fd will be .close()ed in stream_to_open_named_file
+            return stream_to_open_named_file(
+                page, f.fileno(), native_path, source_encoding=get_charset_from_http_headers(page.headers)
+            )
+
+    def _write_from(
+        self, target_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
+        raise NotImplementedError()
+
+    def _serialization_props(self, user_context=None) -> HTTPFilesSourceProperties:
+        effective_props = {}
+        for key, val in self._props.items():
+            effective_props[key] = self._evaluate_prop(val, user_context=user_context)
+        effective_props["url_regex"] = self._url_regex_str
+        return cast(HTTPFilesSourceProperties, effective_props)
+
+    def score_url_match(self, url: str):
+        match = self._url_regex.match(url)
+        if match:
+            return match.span()[1]
+        else:
+            return 0
+
+
+__all__ = ("HTTPFilesSource",)
diff --git a/lib/galaxy/files/sources/onedata.py b/lib/galaxy/files/sources/onedata.py
index 59b98765f3d3..78f04230d5e0 100644
--- a/lib/galaxy/files/sources/onedata.py
+++ b/lib/galaxy/files/sources/onedata.py
@@ -3,6 +3,15 @@
 except ImportError:
     OnedataFS = None
 
+from typing import (
+    Optional,
+    Union,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -11,9 +20,10 @@ class OneDataFilesSource(PyFilesystem2FilesSource):
     required_module = OnedataFS
     required_package = "fs-onedatafs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
-        handle = OnedataFS(**props)
+        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
+        handle = OnedataFS(**{**props, **extra_props})
         return handle
 
 
diff --git a/lib/galaxy/files/sources/posix.py b/lib/galaxy/files/sources/posix.py
index 9d529809e7a9..7edebfe6ffbc 100644
--- a/lib/galaxy/files/sources/posix.py
+++ b/lib/galaxy/files/sources/posix.py
@@ -5,21 +5,35 @@
     Any,
     Dict,
     List,
+    Optional,
 )
 
+from typing_extensions import Unpack
+
 from galaxy import exceptions
 from galaxy.util.path import (
     safe_contains,
     safe_path,
     safe_walk,
 )
-from . import BaseFilesSource
+from . import (
+    BaseFilesSource,
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 
 DEFAULT_ENFORCE_SYMLINK_SECURITY = True
 DEFAULT_DELETE_ON_REALIZE = False
 DEFAULT_ALLOW_SUBDIR_CREATION = True
 
 
+class PosixFilesSourceProperties(FilesSourceProperties, total=False):
+    root: str
+    enforce_symlink_security: bool
+    delete_on_realize: bool
+    allow_subdir_creation: bool
+
+
 class PosixFilesSource(BaseFilesSource):
     plugin_type = "posix"
 
@@ -30,14 +44,14 @@ class PosixFilesSource(BaseFilesSource):
     #    handle = OSFS(**self._props)
     #    return handle
 
-    def __init__(self, **kwd):
+    def __init__(self, **kwd: Unpack[PosixFilesSourceProperties]):
         props = self._parse_common_config_opts(kwd)
         self.root = props["root"]
         self.enforce_symlink_security = props.get("enforce_symlink_security", DEFAULT_ENFORCE_SYMLINK_SECURITY)
         self.delete_on_realize = props.get("delete_on_realize", DEFAULT_DELETE_ON_REALIZE)
         self.allow_subdir_creation = props.get("allow_subdir_creation", DEFAULT_ALLOW_SUBDIR_CREATION)
 
-    def _list(self, path="/", recursive=True, user_context=None):
+    def _list(self, path="/", recursive=True, user_context=None, opts: Optional[FilesSourceOptions] = None):
         dir_path = self._to_native_path(path, user_context=user_context)
         if not self._safe_directory(dir_path):
             raise exceptions.ObjectNotFound(f"The specified directory does not exist [{dir_path}].")
@@ -55,7 +69,9 @@ def _list(self, path="/", recursive=True, user_context=None):
             to_dict = functools.partial(self._resource_info_to_dict, path, user_context=user_context)
             return list(map(to_dict, res))
 
-    def _realize_to(self, source_path, native_path, user_context=None):
+    def _realize_to(
+        self, source_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
         effective_root = self._effective_root(user_context)
         source_native_path = self._to_native_path(source_path, user_context=user_context)
         if self.enforce_symlink_security:
@@ -70,7 +86,9 @@ def _realize_to(self, source_path, native_path, user_context=None):
         else:
             shutil.move(source_native_path, native_path)
 
-    def _write_from(self, target_path, native_path, user_context=None):
+    def _write_from(
+        self, target_path: str, native_path: str, user_context=None, opts: Optional[FilesSourceOptions] = None
+    ):
         effective_root = self._effective_root(user_context)
         target_native_path = self._to_native_path(target_path, user_context=user_context)
         if self.enforce_symlink_security:
@@ -89,7 +107,7 @@ def _write_from(self, target_path, native_path, user_context=None):
 
         shutil.copyfile(native_path, target_native_path)
 
-    def _to_native_path(self, source_path, user_context=None):
+    def _to_native_path(self, source_path: str, user_context=None):
         source_path = os.path.normpath(source_path)
         if source_path.startswith("/"):
             source_path = source_path[1:]
@@ -98,7 +116,7 @@ def _to_native_path(self, source_path, user_context=None):
     def _effective_root(self, user_context=None):
         return self._evaluate_prop(self.root, user_context=user_context)
 
-    def _resource_info_to_dict(self, dir, name, user_context=None):
+    def _resource_info_to_dict(self, dir: str, name: str, user_context=None):
         rel_path = os.path.normpath(os.path.join(dir, name))
         full_path = self._to_native_path(rel_path, user_context=user_context)
         uri = self.uri_from_path(rel_path)
@@ -126,7 +144,7 @@ def _safe_directory(self, directory):
             return False
         return True
 
-    def _serialization_props(self, user_context=None):
+    def _serialization_props(self, user_context=None) -> PosixFilesSourceProperties:
         return {
             # abspath needed because will be used by external Python from
             # a job working directory
diff --git a/lib/galaxy/files/sources/s3.py b/lib/galaxy/files/sources/s3.py
deleted file mode 100644
index 4e5652f07120..000000000000
--- a/lib/galaxy/files/sources/s3.py
+++ /dev/null
@@ -1,20 +0,0 @@
-try:
-    from fs_s3fs import S3FS
-except ImportError:
-    S3FS = None
-
-from ._pyfilesystem2 import PyFilesystem2FilesSource
-
-
-class S3FilesSource(PyFilesystem2FilesSource):
-    plugin_type = "s3"
-    required_module = S3FS
-    required_package = "fs-s3fs"
-
-    def _open_fs(self, user_context):
-        props = self._serialization_props(user_context)
-        handle = S3FS(**props)
-        return handle
-
-
-__all__ = ("S3FilesSource",)
diff --git a/lib/galaxy/files/sources/s3fs.py b/lib/galaxy/files/sources/s3fs.py
index 8353b4d8d51c..6234391409c8 100644
--- a/lib/galaxy/files/sources/s3fs.py
+++ b/lib/galaxy/files/sources/s3fs.py
@@ -3,8 +3,17 @@
 import os
 from typing import (
     Any,
+    cast,
     Dict,
     List,
+    Optional,
+)
+
+from typing_extensions import Unpack
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
 )
 
 try:
@@ -20,20 +29,29 @@
 log = logging.getLogger(__name__)
 
 
+class S3FsFilesSourceProperties(FilesSourceProperties, total=False):
+    bucket: str
+    endpoint_url: int
+    user: str
+    passwd: str
+    client_kwargs: dict  # internally computed. Should not be specified in config file
+
+
 class S3FsFilesSource(BaseFilesSource):
     plugin_type = "s3fs"
 
-    def __init__(self, **kwd):
+    def __init__(self, **kwd: Unpack[S3FsFilesSourceProperties]):
         if s3fs is None:
             raise Exception("Package s3fs unavailable but required for this file source plugin.")
-        props = self._parse_common_config_opts(kwd)
+        props: S3FsFilesSourceProperties = cast(S3FsFilesSourceProperties, self._parse_common_config_opts(kwd))
         self._bucket = props.pop("bucket", "")
         self._endpoint_url = props.pop("endpoint_url", None)
-        assert self._endpoint_url or self._bucket
         self._props = props
+        if self._endpoint_url:
+            self._props.update({"client_kwargs": {"endpoint_url": self._endpoint_url}})
 
-    def _list(self, path="/", recursive=True, user_context=None):
-        fs = self._open_fs(user_context=user_context)
+    def _list(self, path="/", recursive=True, user_context=None, opts: Optional[FilesSourceOptions] = None):
+        fs = self._open_fs(user_context=user_context, opts=opts)
         if recursive:
             res: List[Dict[str, Any]] = []
             bucket_path = self._bucket_path(path)
@@ -48,25 +66,26 @@ def _list(self, path="/", recursive=True, user_context=None):
             to_dict = functools.partial(self._resource_info_to_dict, path)
             return list(map(to_dict, res))
 
-    def _realize_to(self, source_path, native_path, user_context=None):
+    def _realize_to(self, source_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         bucket_path = self._bucket_path(source_path)
-        self._open_fs(user_context=user_context).download(bucket_path, native_path)
+        self._open_fs(user_context=user_context, opts=opts).download(bucket_path, native_path)
 
-    def _write_from(self, target_path, native_path, user_context=None):
+    def _write_from(self, target_path, native_path, user_context=None, opts: Optional[FilesSourceOptions] = None):
         raise NotImplementedError()
 
-    def _bucket_path(self, path):
-        if not path.startswith("/"):
+    def _bucket_path(self, path: str):
+        if path.startswith("s3://"):
+            return path.replace("s3://", "")
+        elif not path.startswith("/"):
             path = f"/{path}"
         return f"{self._bucket}{path}"
 
-    def _open_fs(self, user_context=None):
-        if self._endpoint_url:
-            self._props.update({"client_kwargs": {"endpoint_url": self._endpoint_url}})
-        fs = s3fs.S3FileSystem(**self._props)
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
+        extra_props = opts.extra_props or {} if opts else {}
+        fs = s3fs.S3FileSystem(**{**self._props, **extra_props})
         return fs
 
-    def _resource_info_to_dict(self, dir_path, resource_info):
+    def _resource_info_to_dict(self, dir_path: str, resource_info):
         name = os.path.basename(resource_info["name"])
         path = os.path.join(dir_path, name)
         uri = self.uri_from_path(path)
@@ -90,5 +109,14 @@ def _serialization_props(self, user_context=None):
         effective_props["bucket"] = self._bucket
         return effective_props
 
+    def score_url_match(self, url: str):
+        # For security, we need to ensure that a partial match doesn't work. e.g. s3://{bucket}something/myfiles
+        if self._bucket and (url.startswith(f"s3://{self._bucket}/") or url == f"s3://{self._bucket}"):
+            return len(f"s3://{self._bucket}")
+        elif not self._bucket and url.startswith("s3://"):
+            return len("s3://")
+        else:
+            return super().score_url_match(url)
+
 
 __all__ = ("S3FsFilesSource",)
diff --git a/lib/galaxy/files/sources/ssh.py b/lib/galaxy/files/sources/ssh.py
index 15114ce687f5..7be93f7ec2c4 100644
--- a/lib/galaxy/files/sources/ssh.py
+++ b/lib/galaxy/files/sources/ssh.py
@@ -3,6 +3,15 @@
 except ImportError:
     SSHFS = None
 
+from typing import (
+    Optional,
+    Union,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -11,10 +20,11 @@ class SshFilesSource(PyFilesystem2FilesSource):
     required_module = SSHFS
     required_package = "fs.sshfs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
+        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
         path = props.pop("path")
-        handle = SSHFS(**props)
+        handle = SSHFS(**{**props, **extra_props})
         if path:
             handle = handle.opendir(path)
         return handle
diff --git a/lib/galaxy/files/sources/webdav.py b/lib/galaxy/files/sources/webdav.py
index 4a9a3f0e7cee..86ed55483e70 100644
--- a/lib/galaxy/files/sources/webdav.py
+++ b/lib/galaxy/files/sources/webdav.py
@@ -3,6 +3,15 @@
 except ImportError:
     WebDAVFS = None
 
+from typing import (
+    Optional,
+    Union,
+)
+
+from . import (
+    FilesSourceOptions,
+    FilesSourceProperties,
+)
 from ._pyfilesystem2 import PyFilesystem2FilesSource
 
 
@@ -11,9 +20,10 @@ class WebDavFilesSource(PyFilesystem2FilesSource):
     required_module = WebDAVFS
     required_package = "fs.webdavfs"
 
-    def _open_fs(self, user_context):
+    def _open_fs(self, user_context=None, opts: Optional[FilesSourceOptions] = None):
         props = self._serialization_props(user_context)
-        handle = WebDAVFS(**props)
+        extra_props: Union[FilesSourceProperties, dict] = opts.extra_props or {} if opts else {}
+        handle = WebDAVFS(**{**props, **extra_props})
         return handle
 
 
diff --git a/lib/galaxy/files/uris.py b/lib/galaxy/files/uris.py
index d7172d15dcda..4fbfdb41de09 100644
--- a/lib/galaxy/files/uris.py
+++ b/lib/galaxy/files/uris.py
@@ -1,14 +1,11 @@
-import base64
 import ipaddress
 import logging
 import os
 import socket
 import tempfile
-import urllib.request
 from typing import (
     List,
     Optional,
-    TYPE_CHECKING,
     Union,
 )
 from urllib.parse import urlparse
@@ -17,17 +14,15 @@
     AdminRequiredException,
     ConfigDoesNotAllowException,
 )
+from galaxy.files import (
+    ConfiguredFileSources,
+    NoMatchingFileSource,
+)
+from galaxy.files.sources import FilesSourceOptions
 from galaxy.util import (
-    DEFAULT_SOCKET_TIMEOUT,
-    get_charset_from_http_headers,
     stream_to_open_named_file,
     unicodify,
 )
-from galaxy.util.drs import fetch_drs_to_file
-
-if TYPE_CHECKING:
-    from galaxy.files import ConfiguredFileSources
-
 
 log = logging.getLogger(__name__)
 
@@ -44,33 +39,25 @@ def stream_url_to_str(
 
 
 def stream_url_to_file(
-    path: str,
+    url: str,
     file_sources: Optional["ConfiguredFileSources"] = None,
     prefix: str = "gx_file_stream",
     dir: Optional[str] = None,
     user_context=None,
+    target_path: Optional[str] = None,
+    file_source_opts: Optional[FilesSourceOptions] = None,
 ) -> str:
-    temp_name: str
-    if file_sources and file_sources.looks_like_uri(path):
-        file_source_path = file_sources.get_file_source_path(path)
-        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False, dir=dir) as temp:
-            temp_name = temp.name
-        file_source_path.file_source.realize_to(file_source_path.path, temp_name, user_context=user_context)
-    elif path.startswith("drs://"):
-        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False) as temp:
-            temp_name = temp.name
-            fetch_drs_to_file(path, temp_name)
-    elif path.startswith("base64://"):
-        with tempfile.NamedTemporaryFile(prefix=prefix, delete=False, dir=dir) as temp:
-            temp_name = temp.name
-            temp.write(base64.b64decode(path[len("base64://") :]))
-            temp.flush()
+    if file_sources is None:
+        file_sources = ConfiguredFileSources.from_dict(None, load_stock_plugins=True)
+    file_source, rel_path = file_sources.get_file_source_path(url)
+    if file_source:
+        if not target_path:
+            with tempfile.NamedTemporaryFile(prefix=prefix, delete=False, dir=dir) as temp:
+                target_path = temp.name
+        file_source.realize_to(rel_path, target_path, user_context=user_context, opts=file_source_opts)
+        return target_path
     else:
-        page = urllib.request.urlopen(path, timeout=DEFAULT_SOCKET_TIMEOUT)  # page will be .close()ed in stream_to_file
-        temp_name = stream_to_file(
-            page, prefix=prefix, source_encoding=get_charset_from_http_headers(page.headers), dir=dir
-        )
-    return temp_name
+        raise NoMatchingFileSource(f"Could not find a matching handler for: {url}")
 
 
 def stream_to_file(stream, suffix="", prefix="", dir=None, text=False, **kwd):
@@ -80,8 +67,8 @@ def stream_to_file(stream, suffix="", prefix="", dir=None, text=False, **kwd):
 
 
 IpAddressT = Union[ipaddress.IPv4Address, ipaddress.IPv6Address]
-IpNetwrokT = Union[ipaddress.IPv4Network, ipaddress.IPv6Network]
-IpAllowedListEntryT = Union[IpAddressT, IpNetwrokT]
+IpNetworkT = Union[ipaddress.IPv4Network, ipaddress.IPv6Network]
+IpAllowedListEntryT = Union[IpAddressT, IpNetworkT]
 
 
 def validate_uri_access(uri: str, is_admin: bool, ip_allowlist: List[IpAllowedListEntryT]) -> None:
diff --git a/lib/galaxy/job_execution/setup.py b/lib/galaxy/job_execution/setup.py
index fe3c8f535735..cfa281a329c2 100644
--- a/lib/galaxy/job_execution/setup.py
+++ b/lib/galaxy/job_execution/setup.py
@@ -92,11 +92,11 @@ def __init__(
         is_task: bool = False,
     ):
         user_context_instance: Union[ProvidesUserFileSourcesUserContext, DictFileSourcesUserContext]
+        self.file_sources_dict = file_sources_dict
         if isinstance(user_context, dict):
-            user_context_instance = DictFileSourcesUserContext(**user_context)
+            user_context_instance = DictFileSourcesUserContext(**user_context, file_sources=self.file_sources)
         else:
             user_context_instance = user_context
-        self.file_sources_dict = file_sources_dict
         self.user_context = user_context_instance
         self.sa_session = sa_session
         self.job = job
diff --git a/lib/galaxy/managers/remote_files.py b/lib/galaxy/managers/remote_files.py
index e23d5a642cf5..0fc51cfdf44f 100644
--- a/lib/galaxy/managers/remote_files.py
+++ b/lib/galaxy/managers/remote_files.py
@@ -118,10 +118,14 @@ def index(
 
         return index
 
-    def get_files_source_plugins(self, user_context: ProvidesUserContext) -> FilesSourcePluginList:
+    def get_files_source_plugins(
+        self, user_context: ProvidesUserContext, browsable_only: Optional[bool] = True
+    ) -> FilesSourcePluginList:
         """Display plugin information for each of the gxfiles:// URI targets available."""
         user_file_source_context = ProvidesUserFileSourcesUserContext(user_context)
-        plugins = self._file_sources.plugins_to_dict(user_context=user_file_source_context)
+        plugins = self._file_sources.plugins_to_dict(
+            user_context=user_file_source_context, browsable_only=True if browsable_only is None else browsable_only
+        )
         return FilesSourcePluginList.construct(__root__=plugins)
 
     @property
diff --git a/lib/galaxy/schema/remote_files.py b/lib/galaxy/schema/remote_files.py
index ad083bc923f8..6c5379333148 100644
--- a/lib/galaxy/schema/remote_files.py
+++ b/lib/galaxy/schema/remote_files.py
@@ -42,8 +42,8 @@ class FilesSourcePlugin(Model):
         description="The type of the plugin.",
         example="gximport",
     )
-    uri_root: str = Field(
-        ...,  # This field is required
+    uri_root: Optional[str] = Field(
+        None,
         title="URI root",
         description="The URI root used by this type of plugin.",
         example="gximport://",
@@ -96,6 +96,7 @@ class FilesSourcePluginList(Model):
                 "label": "Library Import Directory",
                 "doc": "Galaxy's library import directory",
                 "writable": False,
+                "browsable": True,
             }
         ],
     )
diff --git a/lib/galaxy/util/drs.py b/lib/galaxy/util/drs.py
index bde667747881..b2df75850cea 100644
--- a/lib/galaxy/util/drs.py
+++ b/lib/galaxy/util/drs.py
@@ -2,15 +2,18 @@
 from os import PathLike
 from typing import (
     Optional,
+    Tuple,
     Union,
 )
 
 import requests
 
-from galaxy.util import (
-    CHUNK_SIZE,
-    DEFAULT_SOCKET_TIMEOUT,
-)
+from galaxy import exceptions
+from galaxy.files import FileSourceDictifiable
+from galaxy.files.sources import FilesSourceOptions
+from galaxy.files.sources.http import HTTPFilesSourceProperties
+from galaxy.files.uris import stream_url_to_file
+from galaxy.util import DEFAULT_SOCKET_TIMEOUT
 
 TargetPathT = Union[str, PathLike]
 
@@ -31,8 +34,8 @@ class RetryOptions:
     override_retry_after: Optional[float] = None
 
 
-def retry_and_get(get_url: str, retry_options: RetryOptions) -> requests.Response:
-    response = requests.get(get_url, timeout=DEFAULT_SOCKET_TIMEOUT)
+def retry_and_get(get_url: str, retry_options: RetryOptions, headers: Optional[dict] = None) -> requests.Response:
+    response = requests.get(get_url, timeout=DEFAULT_SOCKET_TIMEOUT, headers=headers)
     response.raise_for_status()
     if response.status_code == 202:
         if retry_options.retry_times == 0:
@@ -47,8 +50,34 @@ def retry_and_get(get_url: str, retry_options: RetryOptions) -> requests.Respons
         return response
 
 
+def _get_access_info(obj_url: str, access_method: dict, headers=None) -> Tuple[str, dict]:
+    try:
+        access_url = access_method["access_url"]
+    except KeyError:
+        access_id = access_method["access_id"]
+        access_get_url = f"{obj_url}/access/{access_id}"
+        access_response = requests.get(access_get_url, timeout=DEFAULT_SOCKET_TIMEOUT, headers=headers)
+        access_response.raise_for_status()
+        access_response_object = access_response.json()
+        access_url = access_response_object
+
+    url = access_url["url"]
+    headers_list = access_url.get("headers") or []
+    headers_as_dict = {}
+    for header_str in headers_list:
+        key, value = header_str.split(": ", 1)
+        headers_as_dict[key] = value
+
+    return url, headers_as_dict
+
+
 def fetch_drs_to_file(
-    drs_uri: str, target_path: TargetPathT, force_http=False, retry_options: Optional[RetryOptions] = None
+    drs_uri: str,
+    target_path: TargetPathT,
+    user_context: FileSourceDictifiable,
+    force_http=False,
+    retry_options: Optional[RetryOptions] = None,
+    headers: Optional[dict] = None,
 ):
     """Fetch contents of drs:// URI to a target path."""
     if not drs_uri.startswith("drs://"):
@@ -63,43 +92,35 @@ def fetch_drs_to_file(
     if force_http:
         scheme = "http"
     get_url = f"{scheme}://{netspec}/ga4gh/drs/v1/objects/{object_id}"
-    response = retry_and_get(get_url, retry_options or RetryOptions())
+    response = retry_and_get(get_url, retry_options or RetryOptions(), headers=headers)
     response.raise_for_status()
     response_object = response.json()
-    if "access_methods" not in response_object:
-        raise ValueError(f"No access methods found in DRS response for {drs_uri}")
-    access_methods = response_object["access_methods"]
+    access_methods = response_object.get("access_methods", [])
     if len(access_methods) == 0:
         raise ValueError(f"No access methods found in DRS response for {drs_uri}")
 
-    filtered_access_methods = [m for m in access_methods if m["type"].startswith("http")]
-    if len(filtered_access_methods) == 0:
+    downloaded = False
+    for access_method in access_methods:
+        access_url, access_headers = _get_access_info(get_url, access_method, headers=headers)
+        opts = FilesSourceOptions()
+        if access_method["type"] == "https":
+            extra_props: HTTPFilesSourceProperties = {"http_headers": access_headers or {}}
+            opts.extra_props = extra_props
+        else:
+            opts.extra_props = {}
+        try:
+            stream_url_to_file(
+                access_url,
+                target_path=str(target_path),
+                file_sources=user_context.file_sources,
+                user_context=user_context,
+                file_source_opts=opts,
+            )
+            downloaded = True
+            break
+        except exceptions.RequestParameterInvalidException:
+            continue
+
+    if not downloaded:
         unimplemented_access_types = [m["type"] for m in access_methods]
         raise _not_implemented(drs_uri, f"that is fetched via unimplemented types ({unimplemented_access_types})")
-
-    access_method = filtered_access_methods[0]
-    try:
-        access_url = access_method["access_url"]
-    except KeyError:
-        access_id = access_method["access_id"]
-        access_get_url = f"{get_url}/access/{access_id}"
-        access_response = requests.get(access_get_url, timeout=DEFAULT_SOCKET_TIMEOUT)
-        access_response.raise_for_status()
-        access_response_object = access_response.json()
-        access_url = access_response_object
-
-    url = access_url["url"]
-    headers_list = access_url.get("headers") or []
-    headers_as_dict = {}
-    for header_str in headers_list:
-        key, value = header_str.split(": ", 1)
-        headers_as_dict[key] = value
-
-    download_response = requests.get(url, headers=headers_as_dict, stream=True, timeout=DEFAULT_SOCKET_TIMEOUT)
-    download_response.raise_for_status()
-    with open(target_path, "wb") as f:
-        for chunk in download_response.iter_content(chunk_size=CHUNK_SIZE):
-            # If you have chunk encoded response uncomment if
-            # and set chunk_size parameter to None.
-            # if chunk:
-            f.write(chunk)
diff --git a/lib/galaxy/webapps/galaxy/api/remote_files.py b/lib/galaxy/webapps/galaxy/api/remote_files.py
index 779503998ffb..c84d8855bcb3 100644
--- a/lib/galaxy/webapps/galaxy/api/remote_files.py
+++ b/lib/galaxy/webapps/galaxy/api/remote_files.py
@@ -63,6 +63,15 @@
     ),
 )
 
+BrowsableQueryParam: Optional[bool] = Query(
+    default=True,
+    title="Browsable filesources only",
+    description=(
+        "Whether to return browsable filesources only. The default is `True`, which will omit filesources"
+        "like `http` and `base64` that do not implement a list method."
+    ),
+)
+
 
 @router.cbv
 class FastAPIRemoteFiles:
@@ -97,6 +106,7 @@ async def index(
     async def plugins(
         self,
         user_ctx: ProvidesUserContext = DependsOnTrans,
+        browsable_only: Optional[bool] = BrowsableQueryParam,
     ) -> FilesSourcePluginList:
         """Display plugin information for each of the gxfiles:// URI targets available."""
-        return self.manager.get_files_source_plugins(user_ctx)
+        return self.manager.get_files_source_plugins(user_ctx, browsable_only)
diff --git a/lib/galaxy_test/api/test_drs.py b/lib/galaxy_test/api/test_drs.py
index f942c7761e21..c221e486a03c 100644
--- a/lib/galaxy_test/api/test_drs.py
+++ b/lib/galaxy_test/api/test_drs.py
@@ -11,6 +11,11 @@
 
 import requests
 
+from galaxy.files import (
+    ConfiguredFileSources,
+    ConfiguredFileSourcesConfig,
+    DictFileSourcesUserContext,
+)
 from galaxy.util.drs import (
     fetch_drs_to_file,
     RetryOptions,
@@ -26,6 +31,18 @@
 CHECKSUM_TEST_SLEEP_TIME = 3.0
 
 
+def user_context_fixture():
+    file_sources_config = ConfiguredFileSourcesConfig()
+    file_sources = ConfiguredFileSources(file_sources_config, load_stock_plugins=True)
+    user_context = DictFileSourcesUserContext(
+        preferences={
+            "oidc|bearer_token": "IBearTokens",
+        },
+        file_sources=file_sources,
+    )
+    return user_context
+
+
 class TestDrsApi(ApiTestCase):
     dataset_populator: DatasetPopulator
 
@@ -95,7 +112,13 @@ def test_public_data_access_util_code(self):
         with tempfile.NamedTemporaryFile(prefix="gxtest_drs") as tf:
             retry_options = RetryOptions()
             retry_options.override_retry_after = CHECKSUM_TEST_SLEEP_TIME
-            fetch_drs_to_file(drs_uri, tf.name, force_http=force_http, retry_options=retry_options)
+            fetch_drs_to_file(
+                drs_uri,
+                tf.name,
+                user_context=user_context_fixture(),
+                force_http=force_http,
+                retry_options=retry_options,
+            )
             with open(tf.name) as f:
                 assert CONTENT == f.read()
 
diff --git a/packages/files/test-requirements.txt b/packages/files/test-requirements.txt
index f00855512c0b..000f71070273 100644
--- a/packages/files/test-requirements.txt
+++ b/packages/files/test-requirements.txt
@@ -1,2 +1,3 @@
 pytest
 fs-gcsfs
+s3fs
diff --git a/packages/package.Makefile b/packages/package.Makefile
index dff6b157a687..d44f4d7aac7f 100644
--- a/packages/package.Makefile
+++ b/packages/package.Makefile
@@ -93,4 +93,4 @@ push-release:
 release: release-local push-release
 
 mypy:
-	mypy .
+	mypy . --enable-incomplete-feature=Unpack
diff --git a/pyproject.toml b/pyproject.toml
index 0f669a3bda84..e38167adbdd1 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -111,6 +111,7 @@ uvloop = "*"
 WebOb = "*"
 Whoosh = "*"
 zipstream-new = "*"
+s3fs = "^2023.1.0"
 
 [tool.poetry.group.dev.dependencies]
 ase = ">=3.18.1"
diff --git a/test/integration/test_materialize_dataset_instance_tasks.py b/test/integration/test_materialize_dataset_instance_tasks.py
index c756078857af..7bd606cd390a 100644
--- a/test/integration/test_materialize_dataset_instance_tasks.py
+++ b/test/integration/test_materialize_dataset_instance_tasks.py
@@ -131,7 +131,7 @@ def test_materialize_library_dataset(self, history_id: str):
 
     @pytest.mark.require_new_history
     def test_upload_vs_materialize_simplest_upload(self, history_id: str):
-        item = {"src": "url", "url": "gxfiles://testdatafiles//simple_line_no_newline.txt", "ext": "txt"}
+        item = {"src": "url", "url": "gxfiles://testdatafiles/simple_line_no_newline.txt", "ext": "txt"}
         output = self.dataset_populator.fetch_hda(history_id, item)
         uploaded_details = self.dataset_populator.get_history_dataset_details(
             history_id, dataset=output, assert_ok=True
@@ -148,7 +148,7 @@ def test_upload_vs_materialize_simplest_upload(self, history_id: str):
     def test_upload_vs_materialize_to_posix_lines(self, history_id: str):
         item = {
             "src": "url",
-            "url": "gxfiles://testdatafiles//simple_line_no_newline.txt",
+            "url": "gxfiles://testdatafiles/simple_line_no_newline.txt",
             "ext": "txt",
             "to_posix_lines": True,
         }
@@ -174,7 +174,7 @@ def test_upload_vs_materialize_to_posix_lines(self, history_id: str):
     def test_upload_vs_materialize_space_to_tab(self, history_id: str):
         item = {
             "src": "url",
-            "url": "gxfiles://testdatafiles//simple_line_no_newline.txt",
+            "url": "gxfiles://testdatafiles/simple_line_no_newline.txt",
             "ext": "txt",
             "space_to_tab": True,
         }
@@ -200,7 +200,7 @@ def test_upload_vs_materialize_space_to_tab(self, history_id: str):
     def test_upload_vs_materialize_to_posix_and_space_to_tab(self, history_id: str):
         item = {
             "src": "url",
-            "url": "gxfiles://testdatafiles//simple_line_no_newline.txt",
+            "url": "gxfiles://testdatafiles/simple_line_no_newline.txt",
             "ext": "txt",
             "space_to_tab": True,
             "to_posix_lines": True,
diff --git a/test/unit/files/_util.py b/test/unit/files/_util.py
index 2b96536f097a..9d3277395da9 100644
--- a/test/unit/files/_util.py
+++ b/test/unit/files/_util.py
@@ -48,7 +48,7 @@ def list_dir(file_sources, uri, recursive, user_context=None):
     return res
 
 
-def user_context_fixture(user_ftp_dir=None, role_names=None, group_names=None, is_admin=False):
+def user_context_fixture(user_ftp_dir=None, role_names=None, group_names=None, is_admin=False, file_sources=None):
     user_context = DictFileSourcesUserContext(
         username=TEST_USERNAME,
         email=TEST_EMAIL,
@@ -71,10 +71,12 @@ def user_context_fixture(user_ftp_dir=None, role_names=None, group_names=None, i
             "basespace|client_id": os.environ.get("GALAXY_TEST_ONEDATA_CLIENT_ID"),
             "basespace|client_secret": os.environ.get("GALAXY_TEST_ONEDATA_CLIENT_SECRET"),
             "basespace|access_token": os.environ.get("GALAXY_TEST_ONEDATA_ACCESS_TOKEN"),
+            "oidc|bearer_token": "IBearTokens",
         },
         role_names=role_names or set(),
         group_names=group_names or set(),
         is_admin=is_admin,
+        file_sources=file_sources,
     )
     return user_context
 
diff --git a/test/unit/files/base64_file_sources_conf.yml b/test/unit/files/base64_file_sources_conf.yml
new file mode 100644
index 000000000000..f4e27c735dc4
--- /dev/null
+++ b/test/unit/files/base64_file_sources_conf.yml
@@ -0,0 +1,3 @@
+- type: base64
+  id: test1
+  doc: Test base64 string decoding
diff --git a/test/unit/files/drs_file_sources_conf.yml b/test/unit/files/drs_file_sources_conf.yml
new file mode 100644
index 000000000000..72c563b58bcb
--- /dev/null
+++ b/test/unit/files/drs_file_sources_conf.yml
@@ -0,0 +1,10 @@
+- type: drs
+  id: test1
+  doc: Test drs repository filesource
+  http_headers:
+    Authorization: "Bearer ${user.preferences['oidc|bearer_token']}"
+
+- type: s3fs
+  id: test2
+  doc: Generic s3 handler
+  anon: true
diff --git a/test/unit/files/ftp_file_sources_conf.yml b/test/unit/files/ftp_file_sources_conf.yml
new file mode 100644
index 000000000000..1a5c447766c1
--- /dev/null
+++ b/test/unit/files/ftp_file_sources_conf.yml
@@ -0,0 +1,8 @@
+- type: ftp
+  id: test1
+  doc: A specific ftp url handler
+  host: "ftp.gnu.org"
+
+- type: ftp
+  id: test2
+  doc: A default ftp handler, built-in and not needed other than for tests
diff --git a/test/unit/files/http_file_sources_conf.yml b/test/unit/files/http_file_sources_conf.yml
new file mode 100644
index 000000000000..4b49c78e8a5b
--- /dev/null
+++ b/test/unit/files/http_file_sources_conf.yml
@@ -0,0 +1,17 @@
+- type: http
+  id: test1
+  doc: A specific http url handler
+  url_regex: "^https?://www.usegalaxy.org/"
+  http_headers:
+    Authorization: "Bearer ${user.preferences['oidc|bearer_token']}"
+
+- type: http
+  id: test2
+  doc: Another specific http url handler
+  url_regex: "^https?://www.galaxyproject.org/"
+  http_headers:
+    Another_header: "found"
+
+- type: http
+  id: test3
+  doc: A default http handler, built-in and not needed other than for tests
diff --git a/test/unit/files/s3_file_sources_conf.yml b/test/unit/files/s3_file_sources_conf.yml
index 087d6cf85d5a..419a7fa8808c 100644
--- a/test/unit/files/s3_file_sources_conf.yml
+++ b/test/unit/files/s3_file_sources_conf.yml
@@ -3,3 +3,8 @@
   doc: Test anonymous access to an AWS S3 bucket.
   bucket: genomeark
   anon: true
+
+- type: s3fs
+  id: test2
+  doc: Test generic access to an AWS S3 bucket.
+  anon: true
diff --git a/test/unit/files/test_base64.py b/test/unit/files/test_base64.py
new file mode 100644
index 000000000000..22f7c2774443
--- /dev/null
+++ b/test/unit/files/test_base64.py
@@ -0,0 +1,25 @@
+import base64
+import os
+
+from galaxy import util
+from ._util import (
+    assert_realizes_as,
+    configured_file_sources,
+    user_context_fixture,
+)
+
+SCRIPT_DIRECTORY = os.path.abspath(os.path.dirname(__file__))
+FILE_SOURCES_CONF = os.path.join(SCRIPT_DIRECTORY, "base64_file_sources_conf.yml")
+
+
+def test_file_source():
+    ORIGINAL_STRING = "I'm a b64 encoded string"
+    test_url = f"base64://{util.unicodify(base64.b64encode(util.smart_str(ORIGINAL_STRING)))}"
+    user_context = user_context_fixture()
+    file_sources = configured_file_sources(FILE_SOURCES_CONF)
+    file_source_pair = file_sources.get_file_source_path(test_url)
+
+    assert file_source_pair.path == test_url
+    assert file_source_pair.file_source.id == "test1"
+
+    assert_realizes_as(file_sources, test_url, ORIGINAL_STRING, user_context=user_context)
diff --git a/test/unit/files/test_drs.py b/test/unit/files/test_drs.py
new file mode 100644
index 000000000000..10c5ba89eeb8
--- /dev/null
+++ b/test/unit/files/test_drs.py
@@ -0,0 +1,104 @@
+import io
+import json
+import os
+import urllib
+from typing import Any
+from unittest import mock
+
+import responses
+
+from ._util import (
+    assert_realizes_as,
+    assert_realizes_contains,
+    configured_file_sources,
+    user_context_fixture,
+)
+
+SCRIPT_DIRECTORY = os.path.abspath(os.path.dirname(__file__))
+FILE_SOURCES_CONF = os.path.join(SCRIPT_DIRECTORY, "drs_file_sources_conf.yml")
+
+
+@responses.activate
+def test_file_source_drs_http():
+    def drs_repo_handler(request):
+        assert request.headers["Authorization"] == "Bearer IBearTokens"
+        data = {
+            "id": "314159",
+            "name": "hello-314159",
+            "access_methods": [
+                {
+                    "type": "https",
+                    "access_url": {
+                        "url": "https://my.respository.org/myfile.txt",
+                        "headers": ["Authorization: Basic Z2E0Z2g6ZHJz"],
+                    },
+                    "access_id": "1234",
+                }
+            ],
+        }
+        return (200, {}, json.dumps(data))
+
+    responses.add_callback(
+        responses.GET,
+        "https://drs.example.org/ga4gh/drs/v1/objects/314159",
+        callback=drs_repo_handler,
+        content_type="application/json",
+    )
+
+    def check_specific_header(request, **kwargs):
+        assert request.full_url == "https://my.respository.org/myfile.txt"
+        assert request.headers["Authorization"] == "Basic Z2E0Z2g6ZHJz"
+        response: Any = io.StringIO("hello drs world")
+        response.headers = {}
+        return response
+
+    with mock.patch.object(urllib.request, "urlopen", new=check_specific_header):
+        test_url = "drs://drs.example.org/314159"
+        user_context = user_context_fixture()
+        file_sources = configured_file_sources(FILE_SOURCES_CONF)
+        file_source_pair = file_sources.get_file_source_path(test_url)
+
+        assert file_source_pair.path == test_url
+        assert file_source_pair.file_source.id == "test1"
+
+        assert_realizes_as(file_sources, test_url, "hello drs world", user_context=user_context)
+
+
+@responses.activate
+def test_file_source_drs_s3():
+    def drs_repo_handler(request):
+        assert request.headers["Authorization"] == "Bearer IBearTokens"
+        data = {
+            "id": "314160",
+            "name": "hello-314160",
+            "access_methods": [
+                {
+                    "type": "s3",
+                    "access_url": {
+                        "url": "s3://ga4gh-demo-data/phenopackets/Cao-2018-TGFBR2-Patient_4.json",
+                    },
+                    "access_id": "1234",
+                    "region": "us-east-1",
+                }
+            ],
+        }
+        return (200, {}, json.dumps(data))
+
+    responses.add_callback(
+        responses.GET,
+        "https://drs.example.org/ga4gh/drs/v1/objects/314160",
+        callback=drs_repo_handler,
+        content_type="application/json",
+    )
+
+    test_url = "drs://drs.example.org/314160"
+    file_sources = configured_file_sources(FILE_SOURCES_CONF)
+    user_context = user_context_fixture(file_sources=file_sources)
+    file_source_pair = file_sources.get_file_source_path(test_url)
+
+    assert file_source_pair.path == test_url
+    assert file_source_pair.file_source.id == "test1"
+
+    assert_realizes_contains(
+        file_sources, test_url, "PMID:30101859-Cao-2018-TGFBR2-Patient_4", user_context=user_context
+    )
diff --git a/test/unit/files/test_ftp.py b/test/unit/files/test_ftp.py
new file mode 100644
index 000000000000..55b63944dc01
--- /dev/null
+++ b/test/unit/files/test_ftp.py
@@ -0,0 +1,44 @@
+import os
+
+from ._util import (
+    assert_realizes_contains,
+    configured_file_sources,
+    user_context_fixture,
+)
+
+SCRIPT_DIRECTORY = os.path.abspath(os.path.dirname(__file__))
+FILE_SOURCES_CONF = os.path.join(SCRIPT_DIRECTORY, "ftp_file_sources_conf.yml")
+
+
+def test_file_source_ftp_specific():
+    test_url = "ftp://ftp.gnu.org/README"
+    user_context = user_context_fixture()
+    file_sources = configured_file_sources(FILE_SOURCES_CONF)
+    file_source_pair = file_sources.get_file_source_path(test_url)
+
+    assert file_source_pair.path == test_url
+    assert file_source_pair.file_source.id == "test1"
+
+    assert_realizes_contains(
+        file_sources,
+        test_url,
+        "This is ftp.gnu.org, the FTP server of the the GNU project.",
+        user_context=user_context,
+    )
+
+
+def test_file_source_ftp_generic():
+    test_url = "ftp://ftp.slackware.com/welcome.msg"
+    user_context = user_context_fixture()
+    file_sources = configured_file_sources(FILE_SOURCES_CONF)
+    file_source_pair = file_sources.get_file_source_path(test_url)
+
+    assert file_source_pair.path == test_url
+    assert file_source_pair.file_source.id == "test2"
+
+    assert_realizes_contains(
+        file_sources,
+        test_url,
+        "Oregon State University",
+        user_context=user_context,
+    )
diff --git a/test/unit/files/test_http.py b/test/unit/files/test_http.py
new file mode 100644
index 000000000000..a904134b80bf
--- /dev/null
+++ b/test/unit/files/test_http.py
@@ -0,0 +1,96 @@
+import io
+import os
+import urllib
+from typing import Any
+from unittest import mock
+
+from ._util import (
+    assert_realizes_as,
+    assert_realizes_contains,
+    configured_file_sources,
+    user_context_fixture,
+)
+
+SCRIPT_DIRECTORY = os.path.abspath(os.path.dirname(__file__))
+FILE_SOURCES_CONF = os.path.join(SCRIPT_DIRECTORY, "http_file_sources_conf.yml")
+
+
+def test_file_source_http_specific():
+    def check_specific_header(request, **kwargs):
+        assert request.headers["Authorization"] == "Bearer IBearTokens"
+        response: Any = io.StringIO("hello specific world")
+        response.headers = {}
+        return response
+
+    with mock.patch.object(urllib.request, "urlopen", new=check_specific_header):
+        test_url = "https://www.usegalaxy.org/myfile.txt"
+        user_context = user_context_fixture()
+        file_sources = configured_file_sources(FILE_SOURCES_CONF)
+        file_source_pair = file_sources.get_file_source_path(test_url)
+
+        assert file_source_pair.path == test_url
+        assert file_source_pair.file_source.id == "test1"
+
+        assert_realizes_as(file_sources, test_url, "hello specific world", user_context=user_context)
+
+
+def test_file_source_another_http_specific():
+    def check_another_header(request, **kwargs):
+        assert request.headers["Another_header"] == "found"
+        response: Any = io.StringIO("hello another world")
+        response.headers = {}
+        return response
+
+    with mock.patch.object(urllib.request, "urlopen", new=check_another_header):
+        test_url = "http://www.galaxyproject.org/anotherfile.txt"
+        user_context = user_context_fixture()
+        file_sources = configured_file_sources(FILE_SOURCES_CONF)
+        file_source_pair = file_sources.get_file_source_path(test_url)
+
+        assert file_source_pair.path == test_url
+        assert file_source_pair.file_source.id == "test2"
+
+        assert_realizes_as(file_sources, test_url, "hello another world", user_context=user_context)
+
+
+def test_file_source_http_generic():
+    def check_generic_headers(request, **kwargs):
+        assert not request.headers
+        response: Any = io.StringIO("hello generic world")
+        response.headers = {}
+        return response
+
+    with mock.patch.object(urllib.request, "urlopen", new=check_generic_headers):
+        test_url = "https://www.elsewhere.org/myfile.txt"
+        user_context = user_context_fixture()
+        file_sources = configured_file_sources(FILE_SOURCES_CONF)
+        file_source_pair = file_sources.get_file_source_path(test_url)
+
+        assert file_source_pair.path == test_url
+        assert file_source_pair.file_source.id == "test3"
+
+        assert_realizes_as(file_sources, test_url, "hello generic world", user_context=user_context)
+
+
+def test_file_source_ftp_url():
+    def check_generic_headers(request, **kwargs):
+        assert not request.headers
+        response: Any = io.StringIO("This is ftp.gnu.org, the FTP server of the the GNU project.")
+        response.headers = {}
+        return response
+
+    with mock.patch.object(urllib.request, "urlopen", new=check_generic_headers):
+        test_url = "ftp://ftp.gnu.org/README"
+        user_context = user_context_fixture()
+        file_sources = configured_file_sources(FILE_SOURCES_CONF)
+        file_source_pair = file_sources.get_file_source_path(test_url)
+
+        assert file_source_pair.path == test_url
+        assert file_source_pair.file_source.id == "test3"
+
+        assert_realizes_contains(
+            file_sources,
+            test_url,
+            "This is ftp.gnu.org, the FTP server of the the GNU project.",
+            user_context=user_context,
+        )
diff --git a/test/unit/files/test_s3.py b/test/unit/files/test_s3.py
index 1ce5a887eff9..e728261195e0 100644
--- a/test/unit/files/test_s3.py
+++ b/test/unit/files/test_s3.py
@@ -2,7 +2,12 @@
 
 import pytest
 
-from ._util import assert_simple_file_realize
+from ._util import (
+    assert_realizes_contains,
+    assert_simple_file_realize,
+    configured_file_sources,
+    user_context_fixture,
+)
 
 pytest.importorskip("s3fs")
 
@@ -18,3 +23,29 @@ def test_file_source():
         contents="DATA USE POLICIES",
         contains=True,
     )
+
+
+def test_file_source_generic():
+    file_url = "s3://ga4gh-demo-data/phenopackets/Cao-2018-TGFBR2-Patient_4.json"
+    user_context = user_context_fixture()
+    file_sources = configured_file_sources(FILE_SOURCES_CONF)
+    file_source_pair = file_sources.get_file_source_path(file_url)
+
+    assert file_source_pair.path == file_url
+    assert file_source_pair.file_source.id == "test2"
+
+    assert_realizes_contains(
+        file_sources, file_url, "PMID:30101859-Cao-2018-TGFBR2-Patient_4", user_context=user_context
+    )
+
+
+def test_file_source_specific():
+    file_url = "s3://genomeark/data_use_policies.txt"
+    user_context = user_context_fixture()
+    file_sources = configured_file_sources(FILE_SOURCES_CONF)
+    file_source_pair = file_sources.get_file_source_path(file_url)
+
+    assert file_source_pair.path == file_url
+    assert file_source_pair.file_source.id == "test1"
+
+    assert_realizes_contains(file_sources, file_url, "DATA USE POLICIES", user_context=user_context)
diff --git a/tox.ini b/tox.ini
index 46b8b4f41340..b2005f41f483 100644
--- a/tox.ini
+++ b/tox.ini
@@ -16,7 +16,7 @@ commands =
     reports_startup: bash .ci/first_startup.sh reports
     mulled,unit: bash run_tests.sh -u
     # start with test here but obviously someday all of it...
-    mypy: mypy test lib
+    mypy: mypy test lib --enable-incomplete-feature=Unpack
     test_galaxy_packages: make generate-cwl-conformance-tests
     test_galaxy_packages: bash packages/test.sh
 allowlist_externals =
